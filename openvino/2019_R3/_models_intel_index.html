<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Overview of OpenVINO&trade; Toolkit Pre-Trained Models - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Overview of OpenVINO&trade; Toolkit Pre-Trained Models </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>OpenVINO&trade; toolkit provides a set of pre-trained models that you can use for learning and demo purposes or for developing deep learning software. Most recent version is available in the <a href="https://github.com/opencv/open_model_zoo">repo on Github</a>.</p>
<p>The models can be downloaded via Model Downloader (<code>&lt;OPENVINO_INSTALL_DIR&gt;/deployment_tools/open_model_zoo/tools/downloader</code>). They can also be downloaded manually from <a href="https://download.01.org/opencv">01.org</a>.</p>
<h2>Object Detection Models</h2>
<p>Several detection models can be used to detect a set of the most popular objects - for example, faces, people, vehicles. Most of the networks are SSD-based and provide reasonable accuracy/performance trade-offs. Networks that detect the same types of objects (for example, <code>face-detection-adas-0001</code> and <code>face-detection-retail-0004</code>) provide a choice for higher accuracy/wider applicability at the cost of slower performance, so you can expect a "bigger" network to detect objects of the same type better.</p>
<table class="doxtable">
<tr>
<th>Model Name </th><th>Complexity (GFLOPs) </th><th>Size (Mp) </th><th>Face </th><th>Person </th><th>Vehicle </th><th>Bike </th><th>License plate  </th></tr>
<tr>
<td><a class="el" href="_models_intel_face_detection_adas_0001_description_face_detection_adas_0001.html">face-detection-adas-0001</a> </td><td>2.835 </td><td>1.053 </td><td>X </td><td></td><td></td><td></td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_face_detection_adas_binary_0001_description_face_detection_adas_binary_0001.html">face-detection-adas-binary-0001</a> </td><td>0.819 </td><td>1.053 </td><td>X </td><td></td><td></td><td></td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_face_detection_retail_0004_description_face_detection_retail_0004.html">face-detection-retail-0004</a> </td><td>1.067 </td><td>0.588 </td><td>X </td><td></td><td></td><td></td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_face_detection_retail_0005_description_face_detection_retail_0005.html">face-detection-retail-0005</a> </td><td>0.982 </td><td>1.021 </td><td>X </td><td></td><td></td><td></td><td></td></tr>
<tr>
<td><a class="el" href="person-detection-retail-0002.html">person-detection-retail-0002</a> </td><td>12.427 </td><td>3.244 </td><td></td><td>X </td><td></td><td></td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_person_detection_retail_0013_description_person_detection_retail_0013.html">person-detection-retail-0013</a> </td><td>2.300 </td><td>0.723 </td><td></td><td>X </td><td></td><td></td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_person_detection_action_recognition_0005_description_person_detection_action_recognition_0005.html">person-detection-action-recognition-0005</a> </td><td>7.140 </td><td>1.951 </td><td></td><td>X </td><td></td><td></td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_person_detection_action_recognition_0006_description_person_detection_action_recognition_0006.html">person-detection-action-recognition-0006</a> </td><td>8.225 </td><td>2.001 </td><td></td><td>X </td><td></td><td></td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_person_detection_action_recognition_teacher_0002_description_person_detection_action_recognition_teacher_0002.html">person-detection-action-recognition-teacher-0002</a> </td><td>7.140 </td><td>1.951 </td><td></td><td>X </td><td></td><td></td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_person_detection_raisinghand_recognition_0001_description_person_detection_raisinghand_recognition_0001.html">person-detection-raisinghand-recognition-0001</a> </td><td>7.138 </td><td>1.951 </td><td></td><td>X </td><td></td><td></td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_pedestrian_detection_adas_0002_description_pedestrian_detection_adas_0002.html">pedestrian-detection-adas-0002</a> </td><td>2.836 </td><td>1.165 </td><td></td><td>X </td><td></td><td></td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_pedestrian_detection_adas_binary_0001_description_pedestrian_detection_adas_binary_0001.html">pedestrian-detection-adas-binary-0001</a> </td><td>0.945 </td><td>1.165 </td><td></td><td>X </td><td></td><td></td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_pedestrian_and_vehicle_detector_adas_0001_description_pedestrian_and_vehicle_detector_adas_0001.html">pedestrian-and-vehicle-detector-adas-0001</a> </td><td>3.974 </td><td>1.650 </td><td></td><td>X </td><td>X </td><td></td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_vehicle_detection_adas_0002_description_vehicle_detection_adas_0002.html">vehicle-detection-adas-0002</a> </td><td>2.798 </td><td>1.079 </td><td></td><td></td><td>X </td><td></td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_vehicle_detection_adas_binary_0001_description_vehicle_detection_adas_binary_0001.html">vehicle-detection-adas-binary-0001</a> </td><td>0.942 </td><td>1.079 </td><td></td><td></td><td>X </td><td></td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_person_vehicle_bike_detection_crossroad_0078_description_person_vehicle_bike_detection_crossroad_0078.html">person-vehicle-bike-detection-crossroad-0078</a> </td><td>3.964 </td><td>1.178 </td><td></td><td>X </td><td>X </td><td>X </td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_person_vehicle_bike_detection_crossroad_1016_description_person_vehicle_bike_detection_crossroad_1016.html">person-vehicle-bike-detection-crossroad-1016</a> </td><td>3.560 </td><td>2.887 </td><td></td><td>X </td><td>X </td><td>X </td><td></td></tr>
<tr>
<td><a class="el" href="_models_intel_vehicle_license_plate_detection_barrier_0106_description_vehicle_license_plate_detection_barrier_0106.html">vehicle-license-plate-detection-barrier-0106</a> </td><td>0.349 </td><td>0.634 </td><td></td><td></td><td>X </td><td></td><td>X </td></tr>
</table>
<h2>Object Recognition Models</h2>
<p>Object recognition models are used for classification, regression, and character recognition. Use these networks after a respective detector (for example, Age/Gender recognition after Face Detection).</p>
<table class="doxtable">
<tr>
<th>Model Name </th><th>Complexity (GFLOPs) </th><th>Size (Mp)  </th></tr>
<tr>
<td><a class="el" href="_models_intel_age_gender_recognition_retail_0013_description_age_gender_recognition_retail_0013.html">age-gender-recognition-retail-0013</a> </td><td>0.094 </td><td>2.138 </td></tr>
<tr>
<td><a class="el" href="_models_intel_head_pose_estimation_adas_0001_description_head_pose_estimation_adas_0001.html">head-pose-estimation-adas-0001</a> </td><td>0.105 </td><td>1.911 </td></tr>
<tr>
<td><a class="el" href="_models_intel_license_plate_recognition_barrier_0001_description_license_plate_recognition_barrier_0001.html">license-plate-recognition-barrier-0001</a> </td><td>0.328 </td><td>1.218 </td></tr>
<tr>
<td><a class="el" href="_models_intel_vehicle_attributes_recognition_barrier_0039_description_vehicle_attributes_recognition_barrier_0039.html">vehicle-attributes-recognition-barrier-0039</a> </td><td>0.126 </td><td>0.626 </td></tr>
<tr>
<td><a class="el" href="_models_intel_emotions_recognition_retail_0003_description_emotions_recognition_retail_0003.html">emotions-recognition-retail-0003</a> </td><td>0.126 </td><td>2.483 </td></tr>
<tr>
<td><a class="el" href="_models_intel_landmarks_regression_retail_0009_description_landmarks_regression_retail_0009.html">landmarks-regression-retail-0009</a> </td><td>0.021 </td><td>0.191 </td></tr>
<tr>
<td><a class="el" href="_models_intel_facial_landmarks_35_adas_0002_description_facial_landmarks_35_adas_0002.html">facial-landmarks-35-adas-0002</a> </td><td>0.042 </td><td>4.595 </td></tr>
<tr>
<td><a class="el" href="_models_intel_person_attributes_recognition_crossroad_0230_description_person_attributes_recognition_crossroad_0230.html">person-attributes-recognition-crossroad-0230</a> </td><td>0.174 </td><td>0.735 </td></tr>
<tr>
<td><a class="el" href="_models_intel_gaze_estimation_adas_0002_description_gaze_estimation_adas_0002.html">gaze-estimation-adas-0002</a> </td><td>0.139 </td><td>1.882 </td></tr>
</table>
<h2>Reidentification Models</h2>
<p>Precise tracking of objects in a video is a common application of Computer Vision (for example, for people counting). It is often complicated by a set of events that can be described as a "relatively long absence of an object". For example, it can be caused by occlusion or out-of-frame movement. In such cases, it is better to recognize the object as "seen before" regardless of its current position in an image or the amount of time passed since last known position.</p>
<p>The following networks can be used in such scenarios. They take an image of a person and evaluate an embedding - a vector in high-dimensional space that represents an appearance of this person. This vector can be used for further evaluation: images that correspond to the same person will have embedding vectors that are "close" by L2 metric (Euclidean distance).</p>
<p>There are multiple models that provide various trade-offs between performance and accuracy (expect a bigger model to perform better).</p>
<table class="doxtable">
<tr>
<th>Model Name </th><th>Complexity (GFLOPs) </th><th>Size (Mp) </th><th>Pairwise accuracy  </th></tr>
<tr>
<td><a class="el" href="_models_intel_person_reidentification_retail_0031_description_person_reidentification_retail_0031.html">person-reidentification-retail-0031</a> </td><td>0.028 </td><td>0.280 </td><td>92.11% </td></tr>
<tr>
<td><a class="el" href="_models_intel_person_reidentification_retail_0079_description_person_reidentification_retail_0079.html">person-reidentification-retail-0079</a> </td><td>0.124 </td><td>0.820 </td><td>92.93% </td></tr>
<tr>
<td><a class="el" href="_models_intel_person_reidentification_retail_0076_description_person_reidentification_retail_0076.html">person-reidentification-retail-0076</a> </td><td>0.594 </td><td>0.820 </td><td>93.35% </td></tr>
<tr>
<td><a class="el" href="_models_intel_face_reidentification_retail_0095_description_face_reidentification_retail_0095.html">face-reidentification-retail-0095</a> </td><td>0.588 </td><td>1.107 </td><td>99.33% </td></tr>
</table>
<h2>Semantic Segmentation Models</h2>
<p>Semantic segmentation is an extension of object detection problem. Instead of returning bounding boxes, semantic segmentation models return a "painted" version of the input image, where the "color" of each pixel represents a certain class. These networks are much bigger than respective object detection networks, but they provide a better (pixel-level) localization of objects and they can detect areas with complex shape (for example, free space on the road).</p>
<table class="doxtable">
<tr>
<th>Model Name </th><th>Complexity (GFLOPs) </th><th>Size (Mp)  </th></tr>
<tr>
<td><a class="el" href="_models_intel_road_segmentation_adas_0001_description_road_segmentation_adas_0001.html">road-segmentation-adas-0001</a> </td><td>4.770 </td><td>0.184 </td></tr>
<tr>
<td><a class="el" href="_models_intel_semantic_segmentation_adas_0001_description_semantic_segmentation_adas_0001.html">semantic-segmentation-adas-0001</a> </td><td>58.572 </td><td>6.686 </td></tr>
</table>
<h2>Instance Segmentation Models</h2>
<p>Instance segmentation is an extension of object detection and semantic segmentation problems. Instead of predicting a bounding box around each object instance instance segmentation model outputs pixel-wise masks for all instances.</p>
<table class="doxtable">
<tr>
<th>Model Name </th><th>Complexity (GFLOPs) </th><th>Size (Mp)  </th></tr>
<tr>
<td><a class="el" href="_models_intel_instance_segmentation_security_0050_description_instance_segmentation_security_0050.html">instance-segmentation-security-0050</a> </td><td>46.602 </td><td>30.448 </td></tr>
<tr>
<td><a class="el" href="_models_intel_instance_segmentation_security_0083_description_instance_segmentation_security_0083.html">instance-segmentation-security-0083</a> </td><td>365.626 </td><td>143.444 </td></tr>
<tr>
<td><a class="el" href="_models_intel_instance_segmentation_security_0010_description_instance_segmentation_security_0010.html">instance-segmentation-security-0010</a> </td><td>899.568 </td><td>174.568 </td></tr>
</table>
<h2>Human Pose Estimation Models</h2>
<p>Human pose estimation task is to predict a pose: body skeleton, which consists of keypoints and connections between them, for every person in an input image or video. Keypoints are body joints, i.e. ears, eyes, nose, shoulders, knees, etc. There are two major groups of such metods: top-down and bottom-up. The first detects persons in a given frame, crops or rescales detections, then runs pose estimation network for every detection. These methods are very accurate. The second finds all keypoints in a given frame, then groups them by person instances, thus faster than previous, because network runs once.</p>
<table class="doxtable">
<tr>
<th>Model Name </th><th>Complexity (GFLOPs) </th><th>Size (Mp)  </th></tr>
<tr>
<td><a class="el" href="_models_intel_human_pose_estimation_0001_description_human_pose_estimation_0001.html">human-pose-estimation-0001</a> </td><td>15.435 </td><td>4.099 </td></tr>
</table>
<h2>Image Processing</h2>
<p>Deep Learning models find their application in various image processing tasks to increase the quality of the output.</p>
<table class="doxtable">
<tr>
<th>Model Name </th><th>Complexity (GFLOPs) </th><th>Size (Mp)  </th></tr>
<tr>
<td><a class="el" href="_models_intel_single_image_super_resolution_1032_description_single_image_super_resolution_1032.html">single-image-super-resolution-1032</a> </td><td>11.654 </td><td>0.030 </td></tr>
<tr>
<td><a class="el" href="_models_intel_single_image_super_resolution_1033_description_single_image_super_resolution_1033.html">single-image-super-resolution-1033</a> </td><td>16.062 </td><td>0.030 </td></tr>
<tr>
<td><a class="el" href="_models_intel_text_image_super_resolution_0001_description_text_image_super_resolution_0001.html">text-image-super-resolution-0001</a> </td><td>1.379 </td><td>0.003 </td></tr>
</table>
<h2>Text Detection</h2>
<p>Deep Learning models for text detection in various applications.</p>
<table class="doxtable">
<tr>
<th>Model Name </th><th>Complexity (GFLOPs) </th><th>Size (Mp)  </th></tr>
<tr>
<td><a class="el" href="_models_intel_text_detection_0003_description_text_detection_0003.html">text-detection-0003</a> </td><td>51.256 </td><td>6.747 </td></tr>
<tr>
<td><a class="el" href="_models_intel_text_detection_0004_description_text_detection_0004.html">text-detection-0004</a> </td><td>23.305 </td><td>4.328 </td></tr>
</table>
<h2>Text Recognition</h2>
<p>Deep Learning models for text recognition in various applications.</p>
<table class="doxtable">
<tr>
<th>Model Name </th><th>Complexity (GFLOPs) </th><th>Size (Mp)  </th></tr>
<tr>
<td><a class="el" href="_models_intel_text_recognition_0012_description_text_recognition_0012.html">text-recognition-0012</a> </td><td>1.485 </td><td>5.568 </td></tr>
<tr>
<td><a class="el" href="_models_intel_handwritten_score_recognition_0003_description_handwritten_score_recognition_0003.html">handwritten-score-recognition-0003</a> </td><td>0.792 </td><td>5.555 </td></tr>
</table>
<h2>Action Recognition Models</h2>
<p>Action Recognition models predict action that is being performed on a short video clip (tensor formed by stacking sampled frames from input video). Some models (for example <code>driver-action-recognition-adas-0002</code> may use precomputed high-level spatial or spatio-temporal) features (embeddings) from individual clip fragments and then aggregate them in a temporal model to predict a vector with classification scores. Models that compute embeddings are called <em>encoder</em>, while models that predict an actual labels are called <em>decoder</em>.</p>
<table class="doxtable">
<tr>
<th>Model Name </th><th>Complexity (GFLOPs) </th><th>Size (Mp)  </th></tr>
<tr>
<td><a class="el" href="_models_intel_driver_action_recognition_adas_0002_encoder_description_driver_action_recognition_adas_0002_encoder.html">driver-action-recognition-adas-0002-encoder</a> </td><td>0.676 </td><td>2.863 </td></tr>
<tr>
<td><a class="el" href="_models_intel_driver_action_recognition_adas_0002_decoder_description_driver_action_recognition_adas_0002_decoder.html">driver-action-recognition-adas-0002-decoder</a> </td><td>0.147 </td><td>4.205 </td></tr>
<tr>
<td><a class="el" href="_models_intel_action_recognition_0001_encoder_description_action_recognition_0001_encoder.html">action-recognition-0001-encoder</a> </td><td>7.340 </td><td>21.276 </td></tr>
<tr>
<td><a class="el" href="_models_intel_action_recognition_0001_decoder_description_action_recognition_0001_decoder.html">action-recognition-0001-decoder</a> </td><td>0.147 </td><td>4.405 </td></tr>
</table>
<h2>Image Retrieval</h2>
<p>Deep Learning models for image retrieval (ranking 'gallery' images according to their similarity to some 'probe' image).</p>
<table class="doxtable">
<tr>
<th>Model Name </th><th>Complexity (GFLOPs) </th><th>Size (Mp)  </th></tr>
<tr>
<td><a class="el" href="_models_intel_image_retrieval_0001_description_image_retrieval_0001.html">image-retrieval-0001</a> </td><td>0.613 </td><td>2.535 </td></tr>
</table>
<h2>Compressed models</h2>
<p>Deep Learning compressed models</p>
<table class="doxtable">
<tr>
<th>Model Name </th><th>Complexity (GFLOPs) </th><th>Size (Mp)  </th></tr>
<tr>
<td><a class="el" href="_models_intel_resnet50_binary_0001_description_resnet50_binary_0001.html">resnet50-binary-0001</a> </td><td>1.002 </td><td>7.446 </td></tr>
<tr>
<td><a class="el" href="_models_intel_resnet_50_int8_tf_0001_description_resnet_50_int8_tf_0001.html">resnet-50-int8-tf-0001</a> </td><td>6.996 </td><td>25.530 </td></tr>
<tr>
<td><a class="el" href="_models_intel_resnet_50_int8_sparse_v1_tf_0001_description_resnet_50_int8_sparse_v1_tf_0001.html">resnet-50-int8-sparse-v1-tf-0001</a> </td><td>6.996 </td><td>25.530 </td></tr>
<tr>
<td><a class="el" href="_models_intel_resnet_50_int8_sparse_v2_tf_0001_description_resnet_50_int8_sparse_v2_tf_0001.html">resnet-50-int8-sparse-v2-tf-0001</a> </td><td>6.996 </td><td>25.530 </td></tr>
<tr>
<td><a class="el" href="_models_intel_inceptionv3_int8_tf_0001_description_inceptionv3_int8_tf_0001.html">inceptionv3-int8-tf-0001</a> </td><td>11.469 </td><td>23.819 </td></tr>
<tr>
<td><a class="el" href="_models_intel_inceptionv3_int8_sparse_v1_tf_0001_description_inceptionv3_int8_sparse_v1_tf_0001.html">inceptionv3-int8-sparse-v1-tf-0001</a> </td><td>11.469 </td><td>23.819 </td></tr>
<tr>
<td><a class="el" href="_models_intel_inceptionv3_int8_sparse_v2_tf_0001_description_inceptionv3_int8_sparse_v2_tf_0001.html">inceptionv3-int8-sparse-v2-tf-0001</a> </td><td>11.469 </td><td>23.819 </td></tr>
<tr>
<td><a class="el" href="_models_intel_mobilenetv2_int8_tf_0001_description_mobilenetv2_int8_tf_0001.html">mobilenetv2-int8-tf-0001</a> </td><td>0.615 </td><td>3.489 </td></tr>
<tr>
<td><a class="el" href="_models_intel_mobilenetv2_int8_sparse_v1_tf_0001_description_mobilenetv2_int8_sparse_v1_tf_0001.html">mobilenetv2-int8-sparse-v1-tf-0001</a> </td><td>0.615 </td><td>3.489 </td></tr>
<tr>
<td><a class="el" href="_models_intel_mobilenetv2_int8_sparse_v2_tf_0001_description_mobilenetv2_int8_sparse_v2_tf_0001.html">mobilenetv2-int8-sparse-v2-tf-0001</a> </td><td>0.615 </td><td>3.489 </td></tr>
</table>
<h2>Legal Information</h2>
<p>[*] Other names and brands may be claimed as the property of others. </p>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>