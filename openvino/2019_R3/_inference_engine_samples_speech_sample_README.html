<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Automatic Speech Recognition C++ Sample - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Automatic Speech Recognition C++ Sample </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This topic shows how to run the speech sample application, which demonstrates acoustic model inference based on Kaldi* neural networks and speech feature vectors.</p>
<h2>How It Works</h2>
<p>Upon the start-up, the application reads command line parameters and loads a Kaldi-trained neural network along with Kaldi ARK speech feature vector file to the Inference Engine plugin. It then performs inference on all speech utterances stored in the input ARK file. Context-windowed speech frames are processed in batches of 1-8 frames according to the <code>-bs</code> parameter. Batching across utterances is not supported by this sample. When inference is done, the application creates an output ARK file. If the <code>-r</code> option is given, error statistics are provided for each speech utterance as shown above.</p>
<h3>GNA-specific details</h3>
<h4>Quantization</h4>
<p>If the GNA device is selected (for example, using the <code>-d</code> GNA flag), the GNA Inference Engine plugin quantizes the model and input feature vector sequence to integer representation before performing inference. Several parameters control neural network quantization. The <code>-q</code> flag determines the quantization mode. Three modes are supported: static, dynamic, and user-defined. In static quantization mode, the first utterance in the input ARK file is scanned for dynamic range. The scale factor (floating point scalar multiplier) required to scale the maximum input value of the first utterance to 16384 (15 bits) is used for all subsequent inputs. The neural network is quantized to accomodate the scaled input dynamic range. In user-defined quantization mode, the user may specify a scale factor via the <code>-sf</code> flag that will be used for static quantization. In dynamic quantization mode, the scale factor for each input batch is computed just before inference on that batch. The input and network are (re)quantized on-the-fly using an efficient procedure.</p>
<p>The <code>-qb</code> flag provides a hint to the GNA plugin regarding the preferred target weight resolution for all layers. For example, when <code>-qb 8</code> is specified, the plugin will use 8-bit weights wherever possible in the network. Note that it is not always possible to use 8-bit weights due to GNA hardware limitations. For example, convolutional layers always use 16-bit weights (GNA harware verison 1 and 2). This limitation will be removed in GNA hardware version 3 and higher.</p>
<h4>Execution Modes</h4>
<p>Several execution modes are supported via the <code>-d</code> flag. If the device is set to <code>CPU</code> mode, then all calculation will be performed on CPU device using CPU Plugin. If the device is set to <code>GNA_AUTO</code>, then the GNA hardware is used if available and the driver is installed. Otherwise, the GNA device is emulated in fast-but-not-bit-exact mode. If the device is set to <code>GNA_HW</code>, then the GNA hardware is used if available and the driver is installed. Otherwise, an error will occur. If the device is set to <code>GNA_SW</code>, the GNA device is emulated in fast-but-not-bit-exact mode. Finally, if the device is set to <code>GNA_SW_EXACT</code>, the GNA device is emulated in bit-exact mode.</p>
<h4>Loading and Saving Models</h4>
<p>The GNA plugin supports loading and saving of the GNA-optimized model (non-IR) via the <code>-rg</code> and <code>-wg</code> flags. Thereby, it is possible to avoid the cost of full model quantization at run time. The GNA plugin also supports export of firmware-compatible embedded model images for the IntelÂ® Speech Enabling Developer Kit and Amazon Alexa* Premium Far-Field Voice Development Kit via the <code>-we</code> flag (save only).</p>
<p>In addition to performing inference directly from a GNA model file, these options make it possible to:</p><ul>
<li>Convert from IR format to GNA format model file (<code>-m</code>, <code>-wg</code>)</li>
<li>Convert from IR format to embedded format model file (<code>-m</code>, <code>-we</code>)</li>
<li>Convert from GNA format to embedded format model file (<code>-rg</code>, <code>-we</code>)</li>
</ul>
<h2>Running</h2>
<p>Running the application with the <code>-h</code> option yields the following usage message:</p>
<div class="fragment"><div class="line">$ ./speech_sample -h</div><div class="line">InferenceEngine:</div><div class="line">    API version ............ &lt;version&gt;</div><div class="line">    Build .................. &lt;number&gt;</div><div class="line"></div><div class="line">speech_sample [OPTION]</div><div class="line">Options:</div><div class="line"></div><div class="line">    -h                      Print a usage message.</div><div class="line">    -i &quot;&lt;path&gt;&quot;             Required. Paths to an .ark files. Example of usage: &lt;file1.ark,file2.ark&gt; or &lt;file.ark&gt;.</div><div class="line">    -m &quot;&lt;path&gt;&quot;             Required. Path to an .xml file with a trained model (required if -rg is missing).</div><div class="line">    -o &quot;&lt;path&gt;&quot;             Optional. Output file name (default name is &quot;scores.ark&quot;).</div><div class="line">    -l &quot;&lt;absolute_path&gt;&quot;    Required for CPU custom layers. Absolute path to a shared library with the kernel implementations.</div><div class="line">    -d &quot;&lt;device&gt;&quot;           Optional. Specify a target device to infer on. CPU, GPU, GNA_AUTO, GNA_HW, GNA_SW, GNA_SW_EXACT and HETERO with combination of GNA</div><div class="line">     as the primary device and CPU as a secondary (e.g. HETERO:GNA,CPU) are supported. The list of available devices is shown below. The sample will look for a suitable plugin for device specified.</div><div class="line">    -p                      Optional. Plugin name. For example, GPU. If this parameter is set, the sample will look for this plugin only</div><div class="line">    -pc                     Optional. Enables performance report</div><div class="line">    -q &quot;&lt;mode&gt;&quot;             Optional. Input quantization mode:  &quot;static&quot; (default), &quot;dynamic&quot;, or &quot;user&quot; (use with -sf).</div><div class="line">    -qb &quot;&lt;integer&gt;&quot;         Optional. Weight bits for quantization:  8 or 16 (default)</div><div class="line">    -sf &quot;&lt;double&gt;&quot;          Optional. Input scale factor for quantization (use with -q user).</div><div class="line">    -bs &quot;&lt;integer&gt;&quot;         Optional. Batch size 1-8 (default 1)</div><div class="line">    -r &quot;&lt;path&gt;&quot;             Optional. Read reference score .ark file and compare scores.</div><div class="line">    -rg &quot;&lt;path&gt;&quot;            Optional. Read GNA model from file using path/filename provided (required if -m is missing).</div><div class="line">    -wg &quot;&lt;path&gt;&quot;            Optional. Write GNA model to file using path/filename provided.</div><div class="line">    -we &quot;&lt;path&gt;&quot;            Optional. Write GNA embedded model to file using path/filename provided.</div><div class="line">    -nthreads &quot;&lt;integer&gt;&quot;   Optional. Number of threads to use for concurrent async inference requests on the GNA.</div><div class="line">    -cw_l &quot;&lt;integer&gt;&quot;       Optional. Number of frames for left context windows (default is 0). Works only with context window networks.</div><div class="line">                            If you use the cw_l or cw_r flag, then batch size and nthreads arguments are ignored.</div><div class="line">    -cw_r &quot;&lt;integer&gt;&quot;       Optional. Number of frames for right context windows (default is 0). Works only with context window networks.</div><div class="line">                            If you use the cw_r or cw_l flag, then batch size and nthreads arguments are ignored.</div></div><!-- fragment --><p>Running the application with the empty list of options yields the usage message given above and an error message.</p>
<h3>Model Preparation</h3>
<p>You can use the following model optimizer command to convert a Kaldi nnet1 or nnet2 neural network to Intel IR format:</p>
<div class="fragment"><div class="line">$ python3 mo.py --framework kaldi --input_model wsj_dnn5b_smbr.nnet --counts wsj_dnn5b_smbr.counts --remove_output_softmax</div></div><!-- fragment --><p>Assuming that the model optimizer (<code>mo.py</code>), Kaldi-trained neural network, <code>wsj_dnn5b_smbr.nnet</code>, and Kaldi class counts file, <code>wsj_dnn5b_smbr.counts</code>, are in the working directory this produces the Intel IR network consisting of <code>wsj_dnn5b_smbr.xml</code> and <code>wsj_dnn5b_smbr.bin</code>.</p>
<p>The following pre-trained models are available:</p>
<ul>
<li>wsj_dnn5b_smbr</li>
<li>rm_lstm4f</li>
<li>rm_cnn4a_smbr</li>
</ul>
<p>All of them can be downloaded from <a href="https://download.01.org/openvinotoolkit/models_contrib/speech/kaldi">https://download.01.org/openvinotoolkit/models_contrib/speech/kaldi</a> or using the OpenVINO <a href="https://github.com/opencv/open_model_zoo/tree/2018/model_downloader">Model Downloader</a> .</p>
<h3>Speech Inference</h3>
<p>Once the IR is created, you can use the following command to do inference on Intel^&reg; Processors with the GNA co-processor (or emulation library):</p>
<div class="fragment"><div class="line">$ ./speech_sample -d GNA_AUTO -bs 2 -i wsj_dnn5b_smbr_dev93_10.ark -m wsj_dnn5b_smbr_fp32.xml -o scores.ark -r wsj_dnn5b_smbr_dev93_scores_10.ark</div></div><!-- fragment --><p>Here, the floating point Kaldi-generated reference neural network scores (<code>wsj_dnn5b_smbr_dev93_scores_10.ark</code>) corresponding to the input feature file (<code>wsj_dnn5b_smbr_dev93_10.ark</code>) are assumed to be available for comparison.</p>
<blockquote class="doxtable">
<p><b>NOTE</b>: Before running the sample with a trained model, make sure the model is converted to the Inference Engine format (*.xml + *.bin) using the <a class="el" href="_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html">Model Optimizer tool</a>. </p>
</blockquote>
<h2>Sample Output</h2>
<p>The acoustic log likelihood sequences for all utterances are stored in the Kaldi ARK file, <code>scores.ark</code>. If the <code>-r</code> option is used, a report on the statistical score error is generated for each utterance such as the following:</p>
<div class="fragment"><div class="line">Utterance 0: 4k0c0301</div><div class="line">   Average inference time per frame: 6.26867 ms</div><div class="line">         max error: 0.0667191</div><div class="line">         avg error: 0.00473641</div><div class="line">     avg rms error: 0.00602212</div><div class="line">       stdev error: 0.00393488</div></div><!-- fragment --><h2>Use of Sample in Kaldi* Speech Recognition Pipeline</h2>
<p>The Wall Street Journal DNN model used in this example was prepared using the Kaldi s5 recipe and the Kaldi Nnet (nnet1) framework. It is possible to recognize speech by substituting the <code>speech_sample</code> for Kaldi's nnet-forward command. Since the speech_sample does not yet use pipes, it is necessary to use temporary files for speaker- transformed feature vectors and scores when running the Kaldi speech recognition pipeline. The following operations assume that feature extraction was already performed according to the <code>s5</code> recipe and that the working directory within the Kaldi source tree is <code>egs/wsj/s5</code>.</p><ol type="1">
<li>Prepare a speaker-transformed feature set given the feature transform specified in <code>final.feature_transform</code> and the feature files specified in <code>feats.scp</code>: <div class="fragment"><div class="line">nnet-forward --use-gpu=no final.feature_transform &quot;ark,s,cs:copy-feats scp:feats.scp ark:- |&quot; ark:feat.ark</div></div><!-- fragment --></li>
<li>Score the feature set using the <code>speech_sample</code>: <div class="fragment"><div class="line">./speech_sample -d GNA_AUTO -bs 8 -i feat.ark -m wsj_dnn5b_smbr_fp32.xml -o scores.ark</div></div><!-- fragment --></li>
<li>Run the Kaldi decoder to produce n-best text hypotheses and select most likely text given the WFST (<code>HCLG.fst</code>), vocabulary (<code>words.txt</code>), and TID/PID mapping (<code>final.mdl</code>): <div class="fragment"><div class="line">latgen-faster-mapped --max-active=7000 --max-mem=50000000 --beam=13.0 --lattice-beam=6.0 --acoustic-scale=0.0833 --allow-partial=true --word-symbol-table=words.txt final.mdl HCLG.fst ark:scores.ark ark:-| lattice-scale --inv-acoustic-scale=13 ark:- ark:- | lattice-best-path --word-symbol-table=words.txt ark:- ark,t:-  &gt; out.txt &amp;</div></div><!-- fragment --></li>
<li>Run the word error rate tool to check accuracy given the vocabulary (<code>words.txt</code>) and reference transcript (<code>test_filt.txt</code>): <div class="fragment"><div class="line">cat out.txt | utils/int2sym.pl -f 2- words.txt | sed s:&lt;UNK&gt;::g | compute-wer --text --mode=present ark:test_filt.txt ark,p:-</div></div><!-- fragment --></li>
</ol>
<h2>See Also</h2>
<ul>
<li><a class="el" href="_docs_IE_DG_Samples_Overview.html">Using Inference Engine Samples</a></li>
<li><a class="el" href="_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html">Model Optimizer</a></li>
<li><a href="https://github.com/opencv/open_model_zoo/tree/2018/model_downloader">Model Downloader</a> </li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>