<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Inference Engine Kernels Extensibility - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Inference Engine Kernels Extensibility </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The Inference Engine workflow involves the creation of custom kernels and either custom or existing layers.</p>
<p>A <em>Layer</em> is a convolutional neural network (CNN) building block implemented in the training framework, for example, <code>Convolution</code> in Caffe*. A <em>Kernel</em> is defined as the corresponding implementation in Inference Engine.</p>
<p>Please refer to the <a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Customize_Model_Optimizer.html">Custom Layers in the Model Optimizer</a> section for the details of how a mapping between framework layers and Inference Engine kernels is registered.</p>
<p>In short, you can plug your own kernel implementations into the Inference Engine and map them to the layers in the original framework.</p>
<p>The rest of the section covers custom <em>kernels</em> and how do you integrate them into the Inference Engine.</p>
<h2>Example of Custom Kernels Support in the Samples</h2>
<p>Every sample uses the Inference Engine API to load custom kernels depending on the device type. Specifically, for the CPU, it is a shared library that exports certain interface that registers the kernels. For GPU or MYRIAD, it is an <code>.xml</code> file that lists the kernels along with parameters that the kernels accept and how these map to the specific Intermediate Representation (IR) values.</p>
<h2>Example Custom Kernels</h2>
<p>You can find the examples of CPU-targeted kernels in the <code>&lt;INSTALL_DIR&gt;/deployment_tools/inference_engine/src/extension</code> directory. You can also use as an example global GPU kernels delivered with the OpenVINO toolkit.</p>
<p>Several GPU-targeted kernels are also added to the binaries upon samples compilation so that the samples application can easy load them. Refer to the <code>cldnn_global_custom_kernels</code> folder in GPU plugin installation directory.</p>
<h3>How to Implement Custom GPU Layers</h3>
<p>The GPU codepath abstracts many details about OpenCL&trade;. You need to provide the kernel code in the OpenCL C and the configuration file that connects the kernel and its parameters to the parameters of the layer.</p>
<p>There are two options of using custom layer configuration file:</p>
<ul>
<li>Include a section with your kernels into global automatically-loaded <code>cldnn_global_custom_kernels/cldnn_global_custom_kernels.xml</code> file (hosted in the <code>&lt;INSTALL_DIR&gt;/deployment_tools/inference_engine/bin/intel64/{Debug/Release}</code> folder)</li>
<li>Call the <code><a class="el" href="classInferenceEngine_1_1Core.html#a268e2d24595061e9d804460cc6ca9ad3" title="Sets configuration for device, acceptable keys can be found in ie_plugin_config.hpp. ">InferenceEngine::Core::SetConfig()</a></code> method from your application with the <code><a class="el" href="namespaceInferenceEngine_1_1PluginConfigParams.html#a8b8c53f571862c6c394c67f3a66e7db2" title="This key directs the plugin to load a configuration file. The value should be a file name with the pl...">InferenceEngine::PluginConfigParams::KEY_CONFIG_FILE</a></code> key and the configuration file name as a value before loading the network that uses custom layers to the plugin: <div class="fragment"><div class="line"><a class="code" href="classInferenceEngine_1_1Core.html">InferenceEngine::Core</a> core;</div><div class="line"><span class="comment">// Load GPU Extensions</span></div><div class="line">core.<a class="code" href="classInferenceEngine_1_1Core.html#a268e2d24595061e9d804460cc6ca9ad3">SetConfig</a>({ { <a class="code" href="namespaceInferenceEngine_1_1PluginConfigParams.html#a8b8c53f571862c6c394c67f3a66e7db2">InferenceEngine::PluginConfigParams::KEY_CONFIG_FILE</a>, <span class="stringliteral">&quot;&lt;path_to_the_xml_file&gt;&quot;</span> } }, <span class="stringliteral">&quot;GPU&quot;</span>);</div></div><!-- fragment --></li>
</ul>
<p>All Inference Engine samples (except trivial <code>hello_classification</code>) feature a dedicated command-line option <code>-c</code> to load custom kernels. For example, to load custom layers for the classification sample: </p><div class="fragment"><div class="line">$ ./classification_sample -m &lt;path_to_model&gt;/bvlc_alexnet_fp16.xml -i ./validation_set/daily/227x227/apron.bmp -d GPU</div><div class="line"> -c &lt;absolute_path_to_config&gt;/custom_layer_example.xml</div></div><!-- fragment --><h4>Configuration File Format <a class="anchor" id="config-file-format"></a></h4>
<p>The configuration file is expected to follow the <code>.xml</code> file structure with a node of type <code>CustomLayer</code> for every custom layer you provide.</p>
<p>The following definitions will use the notations:</p>
<ul>
<li>(0/1) Can have 0 or 1 instances of this node/attribute</li>
<li>(1) Must have 1 instance of this node/attribute</li>
<li>(0+) Can have any number of instances of this node/attribute</li>
<li>(1+) Can have 1 or more instances of this node/attribute</li>
</ul>
<p><b>CustomLayer Node and Sub-node Structure</b></p>
<p><code>CustomLayer</code> node contains the entire configuration for a single custom layer.</p>
<table class="doxtable">
<tr>
<th>Attribute Name </th><th># </th><th>Description  </th></tr>
<tr>
<td><code>name</code> </td><td>(1) </td><td>The name of the layer type to be used. This name should be identical to the type used in the IR. </td></tr>
<tr>
<td><code>type</code> </td><td>(1) </td><td>Must be <code>SimpleGPU</code> </td></tr>
<tr>
<td><code>version</code> </td><td>(1) </td><td>Must be <code>1</code> </td></tr>
</table>
<p><b>Sub-nodes</b>: <code>Kernel</code> (1), <code>Buffers</code> (1), <code>CompilerOptions</code> (0+), <code>WorkSizes</code> (0/1)</p>
<p><b>Kernel Node and Sub-node Structure</b></p>
<p><code>Kernel</code> node contains all kernel source code configuration. No kernel node structure exists.</p>
<p><b>Sub-nodes</b>: <code>Source</code> (1+), <code>Define</code> (0+)</p>
<p><b>Source Node and Sub-node Structure</b></p>
<p><code>Source</code> node points to a single OpenCL source file.</p>
<table class="doxtable">
<tr>
<th>Attribute Name </th><th># </th><th>Description  </th></tr>
<tr>
<td><code>filename</code> </td><td>(1) </td><td>Name of the file containing OpenCL source code. Notice that path is relative to your executable. Multiple source nodes will have their sources concatenated in order. </td></tr>
</table>
<p><b>Sub-nodes</b>: None</p>
<p><b>Define Node and Sub-node Structure</b></p>
<p><code>Define</code> node configures a single <code>#&zwj;define</code> instruction to be added to the sources during compilation (JIT).</p>
<table class="doxtable">
<tr>
<th>Attribute Name </th><th># </th><th>Description  </th></tr>
<tr>
<td><code>name</code> </td><td>(1) </td><td>The name of the defined JIT. For static constants, this can include the value as well (taken as a string). </td></tr>
<tr>
<td><code>param</code> </td><td>(0/1) </td><td>This parameter value will be used as the value of this JIT definition. </td></tr>
<tr>
<td><code>type</code> </td><td>(0/1) </td><td>The parameter type. Accepted values: <code>int</code>, <code>float</code>, and <code>int[]</code>, <code>float[]</code> for arrays. </td></tr>
<tr>
<td><code>default</code> </td><td>(0/1) </td><td>The default value to be used if the specified parameters is missing from the layer in the IR. </td></tr>
</table>
<p><b>Sub-nodes:</b> None</p>
<p>The resulting JIT will be of the form: <code>#&zwj;define [name] [type] [value/default]</code>.</p>
<p><b>Buffers Node and Sub-node Structure</b></p>
<p><code>Buffers</code> node configures all input/output buffers for the OpenCL entry function. No buffers node structure exists.</p>
<p><b>Sub-nodes:</b> <code>Data</code> (0+), <code>Tensor</code> (1+)</p>
<p><b>Data Node and Sub-node Structure</b></p>
<p><code>Data</code> node configures a single input with static data (for example, weight or biases).</p>
<table class="doxtable">
<tr>
<th>Attribute Name </th><th># </th><th>Description  </th></tr>
<tr>
<td><code>name</code> </td><td>(1) </td><td>Name of a blob attached to a layer in the IR </td></tr>
<tr>
<td><code>arg-index</code> </td><td>(1) </td><td>0-based index in the entry function arguments to be bound to </td></tr>
</table>
<p><b>Sub-nodes</b>: None</p>
<p><b>Tensor Node and Sub-node Structure</b></p>
<p><code>Tensor</code> node configures a single input or output tensor.</p>
<table class="doxtable">
<tr>
<th>Attribute Name </th><th># </th><th>Description  </th></tr>
<tr>
<td><code>arg-index</code> </td><td>(1) </td><td>0-based index in the entry function arguments to be bound to. </td></tr>
<tr>
<td><code>type</code> </td><td>(1) </td><td><code>input</code> or <code>output</code> </td></tr>
<tr>
<td><code>port-index</code> </td><td>(1) </td><td>0-based index in the layer’s input/output ports in the IR </td></tr>
<tr>
<td><code>format</code> </td><td>(0/1) </td><td>Data layout declaration for the tensor. Accepted values: <code>BFYX</code>, <code>BYXF</code>, <code>YXFB</code>, <code>FYXB</code> (also in all lowercase). Default value: <code>BFYX</code> </td></tr>
</table>
<p><b>CompilerOptions Node and Sub-node Structure</b></p>
<p><code>CompilerOptions</code> node configures the compilation flags for the OpenCL sources.</p>
<table class="doxtable">
<tr>
<th>Attribute Name </th><th># </th><th>Description  </th></tr>
<tr>
<td><code>options</code> </td><td>(1) </td><td>Options string to be passed to the OpenCL compiler </td></tr>
</table>
<p><b>Sub-nodes</b>: None</p>
<p><b>WorkSizes Node and Sub-node Structure</b></p>
<p><code>WorkSizes</code> node configures the global/local work sizes to be used when queuing the OpenCL program for execution.</p>
<table class="doxtable">
<tr>
<th>Attribute Name </th><th># </th><th>Description  </th></tr>
<tr>
<td><code>global</code><br />
<code>local</code> </td><td>(0/1)<br />
(0/1) </td><td>An array of up to 3 integers (or formulas) for defining the OpenCL work-sizes to be used during execution.<br />
 The formulas can use the values of the B,F,Y,X dimensions and contain the operators: +,-,/,*,% (all evaluated in integer arithmetic). <br />
Default value: <code>global=”B*F*Y*X” local=””</code> </td></tr>
<tr>
<td><code>dim</code> </td><td>(0/1) </td><td>A tensor to take the work size from. Accepted values: <code>input N</code>, <code>output</code>, where <code>N</code> is an index of input tensor starting with 0. Default value: <code>output</code> </td></tr>
</table>
<p><b>Sub-nodes</b>: None</p>
<h4>Example Configuration file</h4>
<p>The following code sample provides an example configuration file (in <code>.xml</code> format). For information on configuration file structure, see <a href="#config-file-format">Configuration File Format</a>. </p><div class="fragment"><div class="line">&lt;<span class="keywordtype">CustomLayer</span> <span class="keyword">name</span>=<span class="stringliteral">&quot;ReLU&quot;</span> <span class="keyword">type</span>=<span class="stringliteral">&quot;SimpleGPU&quot;</span> <span class="keyword">version</span>=<span class="stringliteral">&quot;1&quot;</span>&gt;</div><div class="line">  &lt;<span class="keywordtype">Kernel</span> <span class="keyword">entry</span>=<span class="stringliteral">&quot;example_relu_kernel&quot;</span>&gt;</div><div class="line">    &lt;<span class="keywordtype">Source</span> <span class="keyword">filename</span>=<span class="stringliteral">&quot;custom_layer_kernel.cl&quot;</span>/&gt;</div><div class="line">    &lt;<span class="keywordtype">Define</span> <span class="keyword">name</span>=<span class="stringliteral">&quot;neg_slope&quot;</span> <span class="keyword">type</span>=<span class="stringliteral">&quot;float&quot;</span> <span class="keyword">param</span>=<span class="stringliteral">&quot;negative_slope&quot;</span> <span class="keyword">default</span>=<span class="stringliteral">&quot;0.0&quot;</span>/&gt;</div><div class="line">  &lt;/<span class="keywordtype">Kernel</span>&gt;</div><div class="line">  &lt;<span class="keywordtype">Buffers</span>&gt;</div><div class="line">    &lt;<span class="keywordtype">Tensor</span> <span class="keyword">arg-index</span>=<span class="stringliteral">&quot;0&quot;</span> <span class="keyword">type</span>=<span class="stringliteral">&quot;input&quot;</span> <span class="keyword">port-index</span>=<span class="stringliteral">&quot;0&quot;</span> <span class="keyword">format</span>=<span class="stringliteral">&quot;BFYX&quot;</span>/&gt;</div><div class="line">    &lt;<span class="keywordtype">Tensor</span> <span class="keyword">arg-index</span>=<span class="stringliteral">&quot;1&quot;</span> <span class="keyword">type</span>=<span class="stringliteral">&quot;output&quot;</span> <span class="keyword">port-index</span>=<span class="stringliteral">&quot;0&quot;</span> <span class="keyword">format</span>=<span class="stringliteral">&quot;BFYX&quot;</span>/&gt;</div><div class="line">  &lt;/<span class="keywordtype">Buffers</span>&gt;</div><div class="line">  &lt;<span class="keywordtype">CompilerOptions</span> <span class="keyword">options</span>=<span class="stringliteral">&quot;-cl-mad-enable&quot;</span>/&gt;</div><div class="line">  &lt;<span class="keywordtype">WorkSizes</span> <span class="keyword">global</span>=<span class="stringliteral">&quot;X,Y,B*F&quot;</span>/&gt;</div><div class="line">&lt;/<span class="keywordtype">CustomLayer</span>&gt;</div></div><!-- fragment --><h4>Built-In Defines for Custom Layers</h4>
<p>The following table includes definitions that will be attached before the user sources, where <code>&lt;TENSOR&gt;</code> is the actual input and output, (for example, <code>INPUT0</code> or <code>OUTPUT0</code>).</p>
<p>For an example, see <a href="#example-kernel">Example Kernel</a>.</p>
<table class="doxtable">
<tr>
<th>Name </th><th>Value  </th></tr>
<tr>
<td><code>NUM_INPUTS</code> </td><td>Number of the input tensors bound to this kernel </td></tr>
<tr>
<td><code>GLOBAL_WORKSIZE</code> </td><td>An array of global work sizes used to execute this kernel </td></tr>
<tr>
<td><code>GLOBAL_WORKSIZE_SIZE</code> </td><td>The size of the <code>GLOBAL_WORKSIZE</code> array </td></tr>
<tr>
<td><code>LOCAL_WORKSIZE</code> </td><td>An array of local work sizes used to execute this kernel </td></tr>
<tr>
<td><code>LOCAL_WORKSIZE_SIZE</code> </td><td>The size of the <code>LOCAL_WORKSIZE</code> array </td></tr>
<tr>
<td><code>&lt;TENSOR&gt;_DIMS</code> </td><td>An array of the tensor dimension sizes. Always ordered as <code>BFYX</code> </td></tr>
<tr>
<td><code>&lt;TENSOR&gt;_DIMS_SIZE</code> </td><td>The size of the <code>&lt;TENSOR&gt;_DIMS</code> array. </td></tr>
<tr>
<td><code>&lt;TENSOR&gt;_TYPE</code> </td><td>The data-type of the tensor: <code>float</code>, <code>half</code> or <code>char</code> </td></tr>
<tr>
<td><code>&lt;TENSOR&gt;_FORMAT_</code> </td><td>The format of the tensor, BFYX, BYXF, YXFB , FYXB or ANY. The format will be concatenated to the defined name. You can use the tensor format to define codepaths in your code with <code>#&zwj;ifdef/#&zwj;endif</code>. </td></tr>
<tr>
<td><code>&lt;TENSOR&gt;_LOWER_PADDING</code> </td><td>An array of padding elements used for the tensor dimensions before they start. Always ordered as BFYX. </td></tr>
<tr>
<td><code>&lt;TENSOR&gt;_ LOWER_PADDING_SIZE</code> </td><td>The size of the <code>&lt;TENSOR&gt;_LOWER_PADDING</code> array </td></tr>
<tr>
<td><code>&lt;TENSOR&gt;_UPPER_PADDING</code> </td><td>An array of padding elements used for the tensor dimensions after they end. Always ordered as BFYX. </td></tr>
<tr>
<td><code>&lt;TENSOR&gt;_UPPER_PADDING_SIZE</code> </td><td>The size of the <code>&lt;TENSOR&gt;_UPPER_PADDING</code> array </td></tr>
<tr>
<td><code>&lt;TENSOR&gt;_PITCHES</code> </td><td>The number of elements between adjacent elements in each dimension. Always ordered as BFYX. </td></tr>
<tr>
<td><code>&lt;TENSOR&gt;_PITCHES_SIZE</code> </td><td>The size of the <code>&lt;TENSOR&gt;_PITCHES</code> array </td></tr>
<tr>
<td><code>&lt;TENSOR&gt;_OFFSET</code> </td><td>The number of elements from the start of the tensor to the first valid element (bypassing the lower padding) </td></tr>
</table>
<p>All <code>&lt;TENSOR&gt;</code> values will be automatically defined for every tensor bound to this layer (<code>INPUT0</code>, <code>INPUT1</code>, <code>OUTPUT0</code>, and so on), as shown in the following for example:</p>
<div class="fragment"><div class="line">#define INPUT0_DIMS_SIZE 4</div><div class="line">#define INPUT0_DIMS (int []){ 1,96,55,55, }</div></div><!-- fragment --><h4>Example Kernel<a class="anchor" id="example-kernel"></a></h4>
<div class="fragment"><div class="line">#pragma OPENCL EXTENSION cl_khr_fp16 : enable</div><div class="line">__kernel void example_relu_kernel(</div><div class="line">    const __global INPUT0_TYPE*  input0,</div><div class="line">          __global OUTPUT0_TYPE* output)</div><div class="line">{</div><div class="line">    const uint idx  = get_global_id(0);</div><div class="line">    const uint idy  = get_global_id(1);</div><div class="line">    const uint idbf = get_global_id(2);//batches*features, as OpenCL supports 3D nd-ranges only</div><div class="line">    const uint feature = idbf%OUTPUT0_DIMS[1];</div><div class="line">    const uint batch   = idbf/OUTPUT0_DIMS[1];</div><div class="line">    //notice that pitches are in elements, not in bytes!</div><div class="line">    const uint in_id  = batch*INPUT0_PITCHES[0] + feature*INPUT0_PITCHES[1]   + idy*INPUT0_PITCHES[2]  + idx*INPUT0_PITCHES[3]  + INPUT0_OFFSET;</div><div class="line">    const uint out_id = batch*OUTPUT0_PITCHES[0] + feature*OUTPUT0_PITCHES[1]  + idy*OUTPUT0_PITCHES[2]  + idx*OUTPUT0_PITCHES[3]  + OUTPUT0_OFFSET;</div><div class="line"></div><div class="line">    INPUT0_TYPE value = input0[in_id];</div><div class="line">    //neg_slope (which is non-zero for leaky ReLU) is put automatically as #define, refer to the config xml</div><div class="line">    output[out_id] = value &lt; 0 ? value * neg_slope : value;</div><div class="line">}</div></div><!-- fragment --><blockquote class="doxtable">
<p><b>NOTE:</b> As described in the previous section, all the things like <code>INPUT0_TYPE</code> are actually defined as OpenCL (pre-)compiler inputs by the Inference Engine for efficiency reasons. See <a href="#debugging-tips">Debugging Tips</a> for information on debugging the results. </p>
</blockquote>
<h4>Debugging Tips<a class="anchor" id="debugging-tips"></a></h4>
<ul>
<li><b>Dumping the Resulting Kernels</b>. It is recommended to get a dump of the kernel with all of the values set by the Inference Engine (all of the tensors sizes, floating-point, and integer kernel parameters). To get the dump, add a following line to your code that configures the GPU plugin to output the custom kernels: <div class="fragment"><div class="line">core.<a class="code" href="classInferenceEngine_1_1Core.html#a268e2d24595061e9d804460cc6ca9ad3">SetConfig</a>({ { <a class="code" href="namespaceInferenceEngine_1_1PluginConfigParams.html#a492dd572580d2a31c98180403f39befb">PluginConfigParams::KEY_DUMP_KERNELS</a>, PluginConfigParams::YES } }, <span class="stringliteral">&quot;GPU&quot;</span>);</div></div><!-- fragment --> When the Inference Engine compiles the kernels for the specific network, it also outputs the resulting code for the custom kernels. In the directory of your executable, you will find files like <code>clDNN_program0.cl</code>, <code>clDNN_program1.cl</code>. There are as many files as distinct sets of parameters for your custom kernel (different input tensor sizes, and kernel parameters).</li>
<li><b>Using printf in the OpenCL™ Kernels</b>. To debug the specific values, you can use <code>printf</code> in your kernels. However, you should be careful: for instance, do not output excessively as it would generate too much data. Since the <code>printf</code> output is typical, your output can be truncated to fit the buffer. Also, because of buffering, you actually get an entire buffer of output when the execution ends.<br />
 For more information, refer to <a href="https://www.khronos.org/registry/OpenCL/sdk/1.2/docs/man/xhtml/printfFunction.html">printf Function</a>.</li>
</ul>
<h3>How to Implement Custom CPU Layers</h3>
<p>Since the primary vehicle for the performance of the CPU codepath in the Inference Engine is the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN), new CPU kernels extend the Inference Engine plugin for the Intel MKL-DNN. Implementing the <code><a class="el" href="classInferenceEngine_1_1ILayerImplFactory.html" title="This class provides interface for extension factories. ">InferenceEngine::ILayerImplFactory</a></code> defines a general CPU-side extension. There are no Intel MKL-DNN specifics in the way you need to implement a kernel.</p>
<p>All Inference Engine samples (except trivial <code>hello_classification</code>) feature a dedicated command-line option <code>-l</code> to CPU load custom kernels. Use the following command-line code to execute the Classification Sample with custom CPU kernels: </p><div class="fragment"><div class="line">$ ./classification_sample -m &lt;path_to_model&gt;/CustomAlexNet.xml -i &lt;path_to_image&gt;/inputImage.bmp -d CPU</div><div class="line">-l &lt;absolute_path_to_library&gt;/libmy_sample_extension.so</div></div><!-- fragment --><p>Consider simple <code>CustomLayerFactory</code> class that registers example kernels which make multiplication by two of its input data, but and does not change the dimensions:</p>
<ol type="1">
<li>Create your custom layer factory <code>CustomLayerFactory</code> class: <div class="fragment"><div class="line"><span class="comment">// custom_layer.h</span></div><div class="line"><span class="comment">// A CustomLayerFactory class is an example layer which make exponentiation by 2 for the input and doesn&#39;t change dimensions</span></div><div class="line"><span class="keyword">class </span>CustomLayerFactory {</div><div class="line"></div><div class="line">};</div></div><!-- fragment --></li>
<li>Inherit it from the abstract class: <code><a class="el" href="classInferenceEngine_1_1ILayerImplFactory.html" title="This class provides interface for extension factories. ">InferenceEngine::ILayerImplFactory</a></code> <div class="fragment"><div class="line"><span class="comment">// custom_layer.h</span></div><div class="line"><span class="keyword">class </span>CustomLayerFactory: <span class="keyword">public</span> <a class="code" href="classInferenceEngine_1_1ILayerImplFactory.html">InferenceEngine::ILayerImplFactory</a> {</div><div class="line"></div><div class="line">};</div></div><!-- fragment --></li>
<li>Create a constructor, a virtual destructor, and a data member to keep the layer info: <div class="fragment"><div class="line"><span class="comment">// custom_layer.h</span></div><div class="line"><span class="keyword">class </span>CustomLayerFactory: <span class="keyword">public</span> <a class="code" href="classInferenceEngine_1_1ILayerImplFactory.html">InferenceEngine::ILayerImplFactory</a> {</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="keyword">explicit</span> CustomLayerFactory(<span class="keyword">const</span> CNNLayer *layer): cnnLayer(*layer) {}</div><div class="line"><span class="keyword">private</span>:</div><div class="line">    CNNLayer cnnLayer;</div><div class="line">};</div></div><!-- fragment --></li>
<li>Overload and implement the abstract methods <code>getShapes</code> and <code>getImplementations</code> of the <code><a class="el" href="classInferenceEngine_1_1ILayerImplFactory.html" title="This class provides interface for extension factories. ">InferenceEngine::ILayerImplFactory</a></code> class: <div class="fragment"><div class="line"><span class="comment">// custom_layer.h</span></div><div class="line"><span class="keyword">class </span>CustomLayerFactory: <span class="keyword">public</span> <a class="code" href="classInferenceEngine_1_1ILayerImplFactory.html">InferenceEngine::ILayerImplFactory</a> {</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="comment">// ... constructor and destructor</span></div><div class="line"></div><div class="line">    <a class="code" href="namespaceInferenceEngine.html#a2ce897aa6a353c071958fe379f5d6421">StatusCode</a> <a class="code" href="classInferenceEngine_1_1ILayerImplFactory.html#a8e06d653a84f05bf252cb4d8fa3c8222">getShapes</a>(<span class="keyword">const</span> std::vector&lt;TensorDesc&gt;&amp; inShapes, std::vector&lt;TensorDesc&gt;&amp; outShapes, ResponseDesc *resp) noexcept <span class="keyword">override</span> {</div><div class="line">        <span class="keywordflow">if</span> (cnnLayer == <span class="keyword">nullptr</span>) {</div><div class="line">            std::string errorMsg = <span class="stringliteral">&quot;Cannot get cnn layer!&quot;</span>;</div><div class="line">            errorMsg.copy(resp-&gt;msg, <span class="keyword">sizeof</span>(resp-&gt;msg) - 1);</div><div class="line">            <span class="keywordflow">return</span> GENERAL_ERROR;</div><div class="line">        }</div><div class="line">        <span class="keywordflow">if</span> (inShapes.size() != 1) {</div><div class="line">            std::string errorMsg = <span class="stringliteral">&quot;Incorrect input shapes!&quot;</span>;</div><div class="line">            errorMsg.copy(resp-&gt;msg, <span class="keyword">sizeof</span>(resp-&gt;msg) - 1);</div><div class="line">            <span class="keywordflow">return</span> GENERAL_ERROR;</div><div class="line">        }</div><div class="line">        outShapes.clear();</div><div class="line">        outShapes.emplace_back(inShapes[0]);</div><div class="line">        <span class="keywordflow">return</span> OK;</div><div class="line">    }</div><div class="line"></div><div class="line">    <a class="code" href="namespaceInferenceEngine.html#a2ce897aa6a353c071958fe379f5d6421">StatusCode</a> <a class="code" href="classInferenceEngine_1_1ILayerImplFactory.html#a4ba7e7694fdb06099ef269815ae868df">getImplementations</a>(std::vector&lt;ILayerImpl::Ptr&gt;&amp; impls, ResponseDesc *resp) noexcept <span class="keyword">override</span> {</div><div class="line">        <span class="comment">// You can add cnnLayer to implementation if it is necessary</span></div><div class="line">        impls.push_back(ILayerImpl::Ptr(<span class="keyword">new</span> CustomLayerImpl()));</div><div class="line">        <span class="keywordflow">return</span> OK;</div><div class="line">    }</div><div class="line">};</div></div><!-- fragment --></li>
<li>Create your custom layer implementation <code>CustomLayerImpl</code> class: <div class="fragment"><div class="line"><span class="comment">// custom_layer.h</span></div><div class="line"><span class="comment">// A CustomLayerImpl class is an example implementation</span></div><div class="line"><span class="keyword">class </span>CustomLayerImpl {</div><div class="line"></div><div class="line">};</div></div><!-- fragment --></li>
<li>Because the layer uses <code>execute</code> method to change data, inherit it from the abstract class <code><a class="el" href="classInferenceEngine_1_1ILayerExecImpl.html" title="This class provides interface for the implementation with the custom execution code. ">InferenceEngine::ILayerExecImpl</a></code>, overload and implement the abstract methods of this class: <div class="fragment"><div class="line"><span class="comment">// custom_layer.h</span></div><div class="line"><span class="comment">// A CustomLayerImpl class is an example implementation</span></div><div class="line"><span class="keyword">class </span>CustomLayerImpl: <span class="keyword">public</span> ILayerExecImpl {</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="keyword">explicit</span> CustomLayerImpl(<span class="keyword">const</span> CNNLayer *layer): cnnLayer(*layer) {}</div><div class="line">    <a class="code" href="namespaceInferenceEngine.html#a2ce897aa6a353c071958fe379f5d6421">StatusCode</a> getSupportedConfigurations(std::vector&lt;LayerConfig&gt;&amp; conf, ResponseDesc *resp) noexcept <span class="keyword">override</span>;</div><div class="line">    <a class="code" href="namespaceInferenceEngine.html#a2ce897aa6a353c071958fe379f5d6421">StatusCode</a> init(LayerConfig&amp; config, ResponseDesc *resp) noexcept <span class="keyword">override</span>;</div><div class="line">    <a class="code" href="namespaceInferenceEngine.html#a2ce897aa6a353c071958fe379f5d6421">StatusCode</a> execute(std::vector&lt;Blob::Ptr&gt;&amp; inputs, std::vector&lt;Blob::Ptr&gt;&amp; outputs, ResponseDesc *resp) noexcept <span class="keyword">override</span>;</div><div class="line"><span class="keyword">private</span>:</div><div class="line">    CNNLayer cnnLayer;</div><div class="line">};</div></div><!-- fragment --></li>
<li>Implement the <code>getSupportedConfigurations</code> virtual method, which returns all supported configuration formats (input/output tensor layouts) for your implementation. To specify formats of data, use <code><a class="el" href="classInferenceEngine_1_1TensorDesc.html" title="This class defines Tensor description. ">InferenceEngine::TensorDesc</a></code>. Refer to the <a class="el" href="_docs_IE_DG_Memory_primitives.html">Memory Primitives</a> section for instructions on how to do it. <div class="fragment"><div class="line"><span class="comment">// custom_layer.cpp</span></div><div class="line"></div><div class="line"><span class="keyword">virtual</span> <a class="code" href="namespaceInferenceEngine.html#a2ce897aa6a353c071958fe379f5d6421">StatusCode</a> CustomLayerImpl::getSupportedConfigurations(std::vector&lt;LayerConfig&gt;&amp; conf, ResponseDesc *resp) noexcept {</div><div class="line">    <span class="keywordflow">try</span> {</div><div class="line">        <span class="comment">// This layer can be in-place but not constant</span></div><div class="line">        <span class="keywordflow">if</span> (cnnLayer == <span class="keyword">nullptr</span>)</div><div class="line">            <a class="code" href="ie__exception_8hpp.html#acc25f730b2c5d6d218163924e5d00705">THROW_IE_EXCEPTION</a> &lt;&lt; <span class="stringliteral">&quot;Cannot get CNN layer&quot;</span>;</div><div class="line">        <span class="keywordflow">if</span> (cnnLayer-&gt;insData.size() != 1 || cnnLayer-&gt;outData.empty())</div><div class="line">            <a class="code" href="ie__exception_8hpp.html#acc25f730b2c5d6d218163924e5d00705">THROW_IE_EXCEPTION</a> &lt;&lt; <span class="stringliteral">&quot;Incorrect number of input/output edges&quot;</span>;</div><div class="line"></div><div class="line">        LayerConfig config;</div><div class="line">        <a class="code" href="namespaceInferenceEngine.html#a91f97c826d2753815815c119ba383e63">DataPtr</a> dataPtr = cnnLayer-&gt;insData[0].lock();</div><div class="line">        <span class="keywordflow">if</span> (!dataPtr)</div><div class="line">            <a class="code" href="ie__exception_8hpp.html#acc25f730b2c5d6d218163924e5d00705">THROW_IE_EXCEPTION</a> &lt;&lt; <span class="stringliteral">&quot;Cannot get input data&quot;</span>;</div><div class="line">        DataConfig dataConfig;</div><div class="line">        dataConfig.inPlace = -1;</div><div class="line">        dataConfig.constant = <span class="keyword">false</span>;</div><div class="line">        <a class="code" href="namespaceInferenceEngine.html#a9400de686d3d0f48c30cd73d40e48576">SizeVector</a> order;</div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; dataPtr-&gt;getTensorDesc().getDims().size(); i++) {</div><div class="line">            order.push_back(i);</div><div class="line">        }</div><div class="line">        <span class="comment">// Planar formats for N dimensions</span></div><div class="line">        dataConfig.desc = TensorDesc(dataPtr-&gt;getTensorDesc().getPrecision(),</div><div class="line">                                     dataPtr-&gt;getTensorDesc().getDims(),</div><div class="line">                                     {dataPtr-&gt;getTensorDesc().getDims(), order});</div><div class="line">        config.inConfs.push_back(dataConfig);</div><div class="line"></div><div class="line">        DataConfig outConfig;</div><div class="line">        outConfig.constant = <span class="keyword">false</span>;</div><div class="line">        outConfig.inPlace = 0;</div><div class="line">        order.clear();</div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; cnnLayer-&gt;outData[0]-&gt;getTensorDesc().getDims().size(); i++) {</div><div class="line">            order.push_back(i);</div><div class="line">        }</div><div class="line">        outConfig.desc = TensorDesc(cnnLayer-&gt;outData[0]-&gt;getTensorDesc().getPrecision(),</div><div class="line">                                    cnnLayer-&gt;outData[0]-&gt;getDims(),</div><div class="line">                                    {cnnLayer-&gt;outData[0]-&gt;getDims(), order});</div><div class="line">        config.outConfs.push_back(outConfig);</div><div class="line">        config.dynBatchSupport = 0;</div><div class="line">        conf.push_back(config);</div><div class="line">        <span class="keywordflow">return</span> OK;</div><div class="line">    } <span class="keywordflow">catch</span> (InferenceEngine::details::InferenceEngineException&amp; ex) {</div><div class="line">        std::string errorMsg = ex.what();</div><div class="line">        errorMsg.copy(resp-&gt;msg, <span class="keyword">sizeof</span>(resp-&gt;msg) - 1);</div><div class="line">        <span class="keywordflow">return</span> GENERAL_ERROR;</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --></li>
<li>Implement the <code>init</code> method to get a runtime-selected configuration from a vector that populated in the previous step and check the parameters: <div class="fragment"><div class="line"><span class="comment">// custom_layer.cpp</span></div><div class="line"></div><div class="line"><span class="keyword">virtual</span> <a class="code" href="namespaceInferenceEngine.html#a2ce897aa6a353c071958fe379f5d6421">StatusCode</a> CustomLayerImpl::init(LayerConfig&amp; config, ResponseDesc *resp) noexcept {</div><div class="line">    <a class="code" href="namespaceInferenceEngine.html#a2ce897aa6a353c071958fe379f5d6421">StatusCode</a> rc = OK;</div><div class="line">    <span class="keywordflow">if</span> (config.dynBatchSupport) {</div><div class="line">        config.dynBatchSupport = 0;</div><div class="line">        rc = NOT_IMPLEMENTED;</div><div class="line">    }</div><div class="line">    <span class="keywordflow">for</span> (<span class="keyword">auto</span>&amp; input : config.inConfs) {</div><div class="line">        <span class="keywordflow">if</span> (input.inPlace &gt;= 0) {</div><div class="line">            input.inPlace = -1;</div><div class="line">            rc = NOT_IMPLEMENTED;</div><div class="line">        }</div><div class="line">        <span class="keywordflow">for</span> (<span class="keyword">auto</span>&amp; offset : input.desc.getBlockingDesc().getOffsetPaddingToData()) {</div><div class="line">            <span class="keywordflow">if</span> (offset) {</div><div class="line">                <span class="keywordflow">return</span> GENERAL_ERROR; <span class="comment">// our simplified implementation does not support data offsets</span></div><div class="line">            }</div><div class="line">        }</div><div class="line">        <span class="keywordflow">if</span> (input.desc.getBlockingDesc().getOffsetPadding()) {</div><div class="line">            <span class="keywordflow">return</span> GENERAL_ERROR; <span class="comment">// our simplified implementation does not support padding</span></div><div class="line">        }</div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; input.desc.getBlockingDesc().getOrder().<a class="code" href="classInferenceEngine_1_1MemoryBlob.html#a733d578f1a002e9f84b65229a61b05d6">size</a>(); i++) {</div><div class="line">            <span class="keywordflow">if</span> (input.desc.getBlockingDesc().getOrder()[i] != i) {</div><div class="line">              <span class="comment">// our simplified tensors support only 4D dimensions with regular order</span></div><div class="line">                <span class="keywordflow">if</span> (i != 4 || input.desc.getBlockingDesc().getOrder()[i] != 1)</div><div class="line">                    <span class="keywordflow">return</span> GENERAL_ERROR;</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="keywordflow">for</span> (<span class="keyword">auto</span>&amp; output : config.outConfs) {</div><div class="line">        <span class="keywordflow">if</span> (output.inPlace &lt; 0) {</div><div class="line">            <span class="comment">// no in-place support for the output</span></div><div class="line">            <span class="keywordflow">return</span> GENERAL_ERROR;</div><div class="line">        }</div><div class="line">        <span class="keywordflow">for</span> (<span class="keyword">auto</span>&amp; offset : output.desc.getBlockingDesc().getOffsetPaddingToData()) {</div><div class="line">            <span class="keywordflow">if</span> (offset) {</div><div class="line">                <span class="keywordflow">return</span> GENERAL_ERROR;</div><div class="line">            }</div><div class="line">        }</div><div class="line">        <span class="keywordflow">if</span> (output.desc.getBlockingDesc().getOffsetPadding()) {</div><div class="line">            <span class="keywordflow">return</span> GENERAL_ERROR;</div><div class="line">        }</div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; output.desc.getBlockingDesc().getOrder().size(); i++) {</div><div class="line">            <span class="keywordflow">if</span> (output.desc.getBlockingDesc().getOrder()[i] != i) {</div><div class="line">                <span class="keywordflow">if</span> (i != 4 || output.desc.getBlockingDesc().getOrder()[i] != 1)</div><div class="line">                    <span class="keywordflow">return</span> GENERAL_ERROR;</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">    <span class="keywordflow">return</span> rc;</div><div class="line">}</div></div><!-- fragment --></li>
<li>Implement the <code>execute</code> method, which accepts and processes the actual tenors as input/output blobs: <div class="fragment"><div class="line"><span class="comment">// custom_layer.cpp</span></div><div class="line"><span class="keyword">virtual</span> <a class="code" href="namespaceInferenceEngine.html#a2ce897aa6a353c071958fe379f5d6421">StatusCode</a> CustomLayerImpl::execute(std::vector&lt;Blob::Ptr&gt;&amp; inputs, std::vector&lt;Blob::Ptr&gt;&amp; outputs, ResponseDesc *resp) noexcept {</div><div class="line">    <span class="keywordflow">if</span> (inputs.size() != 1 || outputs.empty()) {</div><div class="line">        std::string errorMsg = <span class="stringliteral">&quot;Incorrect number of input or output edges!&quot;</span>;</div><div class="line">        errorMsg.copy(resp-&gt;msg, <span class="keyword">sizeof</span>(resp-&gt;msg) - 1);</div><div class="line">        <span class="keywordflow">return</span> GENERAL_ERROR;</div><div class="line">    }</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">float</span>* src_data = inputs[0]-&gt;buffer();</div><div class="line">    <span class="keywordtype">float</span>* dst_data = outputs[0]-&gt;buffer();</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> o = 0; o &lt; outputs-&gt;size(); o++) {</div><div class="line">        <span class="keywordflow">if</span> (dst_data == src_data) {</div><div class="line">            dst_data[o] *= dst_data[o];</div><div class="line">        } <span class="keywordflow">else</span> {</div><div class="line">            dst_data[o] = src_data[o]*src_data[o];</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --></li>
<li>Pack the kernels into a shared library:<ol type="a">
<li>Create a factory for your own primitives inherited from the abstract class <code><a class="el" href="classInferenceEngine_1_1IExtension.html" title="This class is the main extension interface. ">InferenceEngine::IExtension</a></code>, which defines the functions that you need to implement: <div class="fragment"><div class="line"><span class="comment">// custom_extension.h</span></div><div class="line"><span class="keyword">class </span>CustomExtention : <span class="keyword">public</span> <a class="code" href="classInferenceEngine_1_1IExtension.html">InferenceEngine::IExtension</a> {</div><div class="line"></div><div class="line">};</div></div><!-- fragment --></li>
<li>Implement the utility methods <code>Unload</code>, <code>Release</code>, <code>SetLogCallback</code>: <div class="fragment"><div class="line"><span class="comment">// custom_extension.h</span></div><div class="line"></div><div class="line"><span class="keyword">class </span>CustomExtention : <span class="keyword">public</span> <a class="code" href="classInferenceEngine_1_1IExtension.html">InferenceEngine::IExtension</a> {</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="comment">// cleans up resources, in this case, does nothing</span></div><div class="line">    <span class="keywordtype">void</span> <a class="code" href="classInferenceEngine_1_1IShapeInferExtension.html#a9dc3b954fe5cdecf73136c9423342843">Unload</a>() noexcept<span class="keyword"> override </span>{</div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="comment">// is used when destruction happens</span></div><div class="line">    <span class="keywordtype">void</span> Release() noexcept<span class="keyword"> override </span>{</div><div class="line">        <span class="keyword">delete</span> <span class="keyword">this</span>;</div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="comment">// logging is used to track what is going on inside</span></div><div class="line">    <span class="keywordtype">void</span> <a class="code" href="classInferenceEngine_1_1IShapeInferExtension.html#a525c1152b2db61e69954ca7efc6e681c">SetLogCallback</a>(<a class="code" href="classInferenceEngine_1_1IErrorListener.html">InferenceEngine::IErrorListener</a> &amp;listener) noexcept <span class="keyword">override</span> {}</div><div class="line">};</div></div><!-- fragment --></li>
<li>Implement the utility method <code>GetVersion</code>: <div class="fragment"><div class="line"><span class="comment">// custom_extension.h</span></div><div class="line"></div><div class="line"><span class="keyword">class </span>CustomExtention : <span class="keyword">public</span> <a class="code" href="classInferenceEngine_1_1IExtension.html">InferenceEngine::IExtension</a> {</div><div class="line"><span class="keyword">private</span>:</div><div class="line">    <span class="keyword">static</span> <a class="code" href="structInferenceEngine_1_1Version.html">InferenceEngine::Version</a> ExtensionDescription = {</div><div class="line">        {1, 0},             <span class="comment">// extension API version</span></div><div class="line">        <span class="stringliteral">&quot;1.0&quot;</span>,</div><div class="line">        <span class="stringliteral">&quot;CustomExtention&quot;</span>   <span class="comment">// extension description message</span></div><div class="line">    };</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="comment">// gets extension version information</span></div><div class="line">    <span class="keywordtype">void</span> <a class="code" href="classInferenceEngine_1_1IShapeInferExtension.html#a8feb5097f4f43db07ed914002639ef82">GetVersion</a>(<span class="keyword">const</span> <a class="code" href="structInferenceEngine_1_1Version.html">InferenceEngine::Version</a> *&amp; versionInfo) <span class="keyword">const</span> noexcept <span class="keyword">override</span> {</div><div class="line">        versionInfo = &amp;ExtensionDescription;</div><div class="line">    }</div><div class="line">};</div></div><!-- fragment --></li>
<li>Implement main extension methods: <div class="fragment"><div class="line"><span class="comment">// custom_extension.h</span></div><div class="line"></div><div class="line"><span class="keyword">class </span>CustomExtention : <span class="keyword">public</span> <a class="code" href="classInferenceEngine_1_1IExtension.html">InferenceEngine::IExtension</a> {</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="comment">// ... utility methods</span></div><div class="line">    <span class="comment">// retrunes the list of supported kernels/layers</span></div><div class="line">    <a class="code" href="namespaceInferenceEngine.html#a2ce897aa6a353c071958fe379f5d6421">StatusCode</a> <a class="code" href="classInferenceEngine_1_1IExtension.html#a57782e9b8a51b3a92614ada4a2118619">getPrimitiveTypes</a>(<span class="keywordtype">char</span>**&amp; types, <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>&amp; size, ResponseDesc* resp) noexcept <span class="keyword">override</span> {</div><div class="line">        std::string type_name = <span class="stringliteral">&quot;CustomLayer&quot;</span>;</div><div class="line">        types = <span class="keyword">new</span> <span class="keywordtype">char</span> *[1];</div><div class="line">        size = 1;</div><div class="line">        types[0] = <span class="keyword">new</span> <span class="keywordtype">char</span>[type_name.size() + 1];</div><div class="line">        std::copy(type_name.begin(), type_name.end(), types[0]);</div><div class="line">        types[0][type_name.size()] = <span class="charliteral">&#39;\0&#39;</span>;</div><div class="line">        <span class="keywordflow">return</span> OK;</div><div class="line">    }</div><div class="line">    <span class="comment">// main function</span></div><div class="line">    <a class="code" href="namespaceInferenceEngine.html#a2ce897aa6a353c071958fe379f5d6421">StatusCode</a> getFactoryFor(ILayerImplFactory *&amp;factory, <span class="keyword">const</span> CNNLayer *cnnLayer, ResponseDesc *resp) noexcept <span class="keyword">override</span> {</div><div class="line">        <span class="keywordflow">if</span> (cnnLayer-&gt;type != <span class="stringliteral">&quot;CustomLayer&quot;</span>) {</div><div class="line">            std::string errorMsg = std::string(<span class="stringliteral">&quot;Factory for &quot;</span>) + cnnLayer-&gt;type + <span class="stringliteral">&quot; wasn&#39;t found!&quot;</span>;</div><div class="line">            errorMsg.copy(resp-&gt;msg, <span class="keyword">sizeof</span>(resp-&gt;msg) - 1);</div><div class="line">            <span class="keywordflow">return</span> NOT_FOUND;</div><div class="line">        }</div><div class="line">        factory = <span class="keyword">new</span> CustomLayerFactory(cnnLayer);</div><div class="line">        <span class="keywordflow">return</span> OK;</div><div class="line">    }</div><div class="line">};</div></div><!-- fragment --></li>
</ol>
</li>
<li>To use your custom layers, you need to compile the code as the shared library. After that use the <code>AddExtension</code> method of the general plugin interface to load your primitives: <div class="fragment"><div class="line"><a class="code" href="classInferenceEngine_1_1Core.html">InferenceEngine::Core</a> core;</div><div class="line"><span class="comment">// Load CPU extension as a shared library</span></div><div class="line"><span class="keyword">auto</span> extension_ptr = make_so_pointer&lt;InferenceEngine::IExtension&gt;(<span class="stringliteral">&quot;&lt;shared lib path&gt;&quot;</span>);</div><div class="line"><span class="comment">// Add extension to the CPU device</span></div><div class="line">core.<a class="code" href="classInferenceEngine_1_1Core.html#aa1ddb53f4160bf0735239f4aa0c12320">AddExtension</a>(extension_ptr, <span class="stringliteral">&quot;CPU&quot;</span>);</div></div><!-- fragment --></li>
</ol>
<h3>How to Implement Custom Layers for VPU (Intel® Neural Compute Stick 2)</h3>
<blockquote class="doxtable">
<p><b>NOTE:</b> OpenCL™ custom layer support is available in the preview mode. </p>
</blockquote>
<blockquote class="doxtable">
<p><b>NOTE:</b> This section assumes you are familiar with developing kernels using OpenCL™. </p>
</blockquote>
<p>To customize your topology with an OpenCL™ layer, follow the steps below:</p>
<ol type="1">
<li>Write and compile you OpenCL™ code with the standalone offline OpenCL™ compiler (<code>clc</code>).</li>
<li>Write a configuration file to bind the OpenCL™ kernel to the topology file (<code>.xml</code>) of the model IR.</li>
<li>Pass the configuration file to Inference engine with the model IR.</li>
</ol>
<h4>Compile OpenCL™ code for VPU (Intel® Neural Compute Stick 2)</h4>
<blockquote class="doxtable">
<p><b>NOTE:</b> OpenCL compiler, targeting Intel® Neural Compute Stick 2 for SHAVE* processor only, is re-distributed with OpenVINO. </p>
</blockquote>
<p>OpenCL support is provided by ComputeAorta*, and is distributed under a license agreement between Intel and Codeplay Software Ltd.</p>
<p>Since the OpenCL™ toolchain for Intel® Neural Compute Stick 2 supports offline compilation only, you should first compile OpenCL C code using the standalone <code>clc</code> compiler. You can find compiler binary at <code>&lt;INSTALL_DIR&gt;/deployment_tools/tools/cl_compiler</code>.</p>
<blockquote class="doxtable">
<p><b>NOTE:</b> By design, custom OpenCL layers support any OpenCL kernels written with 1.2 version assumed. It also supports half float </p>
</blockquote>
<p>extension and optimized for this type since it's a native type for Movidius VPU.</p>
<ol type="1">
<li>Prior to running compilation, make sure that the following variables are set:<ul>
<li><code>SHAVE_MA2X8XLIBS_DIR=&lt;INSTALL_DIR&gt;/deployment_tools/tools/cl_compiler/lib/</code></li>
<li><code>SHAVE_LDSCRIPT_DIR=&lt;INSTALL_DIR&gt;/deployment_tools/tools/cl_compiler/ldscripts/</code></li>
<li><code>SHAVE_MYRIAD_LD_DIR=&lt;INSTALL_DIR&gt;/deployment_tools/tools/cl_compiler/bin/</code></li>
<li><code>SHAVE_MOVIASM_DIR=&lt;INSTALL_DIR&gt;/deployment_tools/tools/cl_compiler/bin/</code></li>
</ul>
</li>
<li>Run the compilation with the command below. You should use <code>--strip-binary-header</code> to make an OpenCL runtime-agnostic binary runnable with inference engine. <div class="fragment"><div class="line">cd &lt;INSTALL_DIR&gt;/deployment_tools/tools/cl_compiler/bin</div><div class="line">./clc --strip-binary-header custom_layer.cl -o custom_layer.bin</div></div><!-- fragment --></li>
</ol>
<h4>Write a configuration file</h4>
<p>To tie the topology IR for a layer you customize, you need to prepare a configuration file. The main purpose of this is to tell the Inference Engine where to find parameters for your kernel and describe the execution work grid. For example, given the following OpenCL kernel signature: </p><div class="fragment"><div class="line">__kernel <span class="keywordtype">void</span> reorg_nhwc(__global <span class="keyword">const</span> half *src, __global half *out, <span class="keywordtype">int</span> w, <span class="keywordtype">int</span> h, <span class="keywordtype">int</span> c, <span class="keywordtype">int</span> stride);</div></div><!-- fragment --><p> Configuration file for this kernel might be the following: </p><div class="fragment"><div class="line">&lt;<span class="keywordtype">CustomLayer</span> <span class="keyword">name</span>=<span class="stringliteral">&quot;ReorgYolo&quot;</span> <span class="keyword">type</span>=<span class="stringliteral">&quot;MVCL&quot;</span> <span class="keyword">version</span>=<span class="stringliteral">&quot;1&quot;</span>&gt;</div><div class="line">   &lt;<span class="keywordtype">Kernel</span> <span class="keyword">entry</span>=<span class="stringliteral">&quot;reorg_nhwc&quot;</span>&gt;</div><div class="line">       &lt;<span class="keywordtype">Source</span> <span class="keyword">filename</span>=<span class="stringliteral">&quot;reorg.bin&quot;</span>/&gt;</div><div class="line">   &lt;/<span class="keywordtype">Kernel</span>&gt;</div><div class="line">   &lt;<span class="keywordtype">Parameters</span>&gt;</div><div class="line">       &lt;<span class="keywordtype">Tensor</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;src&quot;</span>    <span class="keyword">type</span>=<span class="stringliteral">&quot;input&quot;</span>  <span class="keyword">port-index</span>=<span class="stringliteral">&quot;0&quot;</span>                <span class="keyword">format</span>=<span class="stringliteral">&quot;BYXF&quot;</span>/&gt;</div><div class="line">       &lt;<span class="keywordtype">Tensor</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;out&quot;</span>    <span class="keyword">type</span>=<span class="stringliteral">&quot;output&quot;</span> <span class="keyword">port-index</span>=<span class="stringliteral">&quot;0&quot;</span>                <span class="keyword">format</span>=<span class="stringliteral">&quot;BYXF&quot;</span>/&gt;</div><div class="line">       &lt;<span class="keywordtype">Scalar</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;w&quot;</span>      <span class="keyword">type</span>=<span class="stringliteral">&quot;int&quot;</span>    <span class="keyword">port-index</span>=<span class="stringliteral">&quot;0&quot;</span> <span class="keyword">source</span>=<span class="stringliteral">&quot;I.X&quot;</span>                /&gt;</div><div class="line">       &lt;<span class="keywordtype">Scalar</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;h&quot;</span>      <span class="keyword">type</span>=<span class="stringliteral">&quot;int&quot;</span>    <span class="keyword">port-index</span>=<span class="stringliteral">&quot;0&quot;</span> <span class="keyword">source</span>=<span class="stringliteral">&quot;I.Y&quot;</span>                /&gt;</div><div class="line">       &lt;<span class="keywordtype">Scalar</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;c&quot;</span>      <span class="keyword">type</span>=<span class="stringliteral">&quot;int&quot;</span>    <span class="keyword">port-index</span>=<span class="stringliteral">&quot;0&quot;</span> <span class="keyword">source</span>=<span class="stringliteral">&quot;I.F&quot;</span>                /&gt;</div><div class="line">       &lt;<span class="keywordtype">Scalar</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;stride&quot;</span> <span class="keyword">type</span>=<span class="stringliteral">&quot;int&quot;</span>                   <span class="keyword">source</span>=<span class="stringliteral">&quot;stride&quot;</span>             /&gt;</div><div class="line">   &lt;/<span class="keywordtype">Parameters</span>&gt;</div><div class="line">   &lt;<span class="keywordtype">WorkSizes</span> <span class="keyword">dim</span>=<span class="stringliteral">&quot;input,0&quot;</span> <span class="keyword">global</span>=<span class="stringliteral">&quot;(Y+7)/8*8,1,1&quot;</span> <span class="keyword">local</span>=<span class="stringliteral">&quot;8,1,1&quot;</span>/&gt;</div><div class="line">&lt;/<span class="keywordtype">CustomLayer</span>&gt;</div></div><!-- fragment --><p> Each custom layer is described with the <code>CustomLayer</code> node. It has the following nodes and attributes:</p><ul>
<li>Root node <code>CustomLayer</code> contains the following attributes:<ul>
<li><code>name</code> — (Required) A name of the Inference Engine layer to bind the kernel with.</li>
<li><code>type</code> and <code>version</code> — (Required) Reserved for future use. Set them to <code>MVCL</code> and <code>1</code> respectively.</li>
<li><code>max-shaves</code> — (Optional) The maximum number of SHAVE cores that should be dedicated for the layer. It's useful for debugging concurrency issues or for resource saving if memory bound kernel doesn't scale well with the number of cores, so more resources can be left for the rest of a topology.</li>
</ul>
</li>
<li>Sub-node <code>Kernel</code> must contain the following attributes:<ul>
<li><code>entry</code> — A name of your kernel function as you defined it in a source file (in the example above, it is <code>reorg_nhwc</code>).</li>
<li>Node <code>Source</code> must contain the following attributes:<ul>
<li><code>filename</code> — A path to a compiled binary relative to the <code>.xml</code> binding file.</li>
</ul>
</li>
</ul>
</li>
<li>Sub-node <code>Parameters</code> — Describes parameters bindings. For more information, see the description below.</li>
<li>Sub-node <code>WorkSizes</code> — Describes local and global work group sizes and the source for dimension deduction as a pair <code>direction,port</code>. In the example above, the work group is described relatively to the dimension of the input tensor that comes thought port 0 in the IR. <code>global</code> and <code>local</code> work group configurations support any simple math expressions with +,-,*,/, and () from <code>B</code>(batch), <code>Y</code>(height), <code>X</code>(width) and <code>F</code>(channels).</li>
<li>Sub-node <code>Where</code> — Allows to customize bindings with the <code>key="value"</code> attribute. For example, to substitute only 3x3 convolutions write <code>&lt;Where kernel="3,3"/&gt;</code> in the binging xml.</li>
</ul>
<p>Parameter description supports <code>Tensor</code> of one of tensor types such as <code>input</code>, <code>output</code>, <code>input_buffer</code>, <code>output_buffer</code> or <code>data</code> and <code>Scalar</code> nodes and has the following format:</p><ul>
<li>Each <code>Tensor</code> node that has type <code>input</code> or <code>output</code> must contain the following attribute:<ul>
<li><code>arg-name</code> — A name of a kernel parameter in the kernel signature.</li>
<li><code>type</code> — <code>input</code> or <code>output</code> as in the IR.</li>
<li><code>port-index</code> — A number of input/output port as in the IR.</li>
<li><code>format</code> — Specifies the channel order in the tensor. Optional conversion layers are generated if the custom layer format is not compatible with formats of neighboring layers. <code>BFXY</code>, <code>BYXF</code> and <code>ANY</code> formats are supported currently.</li>
</ul>
</li>
<li><p class="startli">Each <code>Tensor</code> node that has type <code>input_buffer</code> or <code>output_buffer</code> must contain the following attribute:</p><ul>
<li><code>arg-name</code> — A name of a kernel parameter in the kernel signature.</li>
<li><code>type</code> — <code>input_buffer</code> or <code>output_buffer</code>. Use the appropriate type to bind multiple kernels that correspond to different stages of the same layer.</li>
<li><code>port-index</code> — The unique identifier to bind by.</li>
<li><code>dim</code> — The dim source with the same <code>direction,port</code> format used for <code>WorkSizes</code> bindings.</li>
<li><code>size</code> — Amount of bytes needed. Current expression syntax supports only expression over dimensions of over selected input/output tensor or constants and might be expended in the future.</li>
</ul>
<p class="startli">Here is an example of multi-stage MVN layer binding: </p><div class="fragment"><div class="line">&lt;<span class="keywordtype">CustomLayer</span> <span class="keyword">name</span>=<span class="stringliteral">&quot;MVN&quot;</span> <span class="keyword">stage</span>=<span class="stringliteral">&quot;0&quot;</span> <span class="keyword">type</span>=<span class="stringliteral">&quot;MVCL&quot;</span> <span class="keyword">version</span>=<span class="stringliteral">&quot;1&quot;</span>&gt;</div><div class="line">    &lt;<span class="keywordtype">Kernel</span> <span class="keyword">entry</span>=<span class="stringliteral">&quot;reduction_mean&quot;</span>&gt;</div><div class="line">        &lt;<span class="keywordtype">Source</span> <span class="keyword">filename</span>=<span class="stringliteral">&quot;mvn.bin&quot;</span>/&gt;</div><div class="line">    &lt;/<span class="keywordtype">Kernel</span>&gt;</div><div class="line">    &lt;<span class="keywordtype">Parameters</span>&gt;</div><div class="line">        &lt;<span class="keywordtype">Tensor</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;src&quot;</span>                <span class="keyword">type</span>=<span class="stringliteral">&quot;input&quot;</span>         <span class="keyword">port-index</span>=<span class="stringliteral">&quot;0&quot;</span>               <span class="keyword">format</span>=<span class="stringliteral">&quot;BFYX&quot;</span>/&gt;</div><div class="line">        &lt;<span class="keywordtype">Tensor</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;mean&quot;</span>               <span class="keyword">type</span>=<span class="stringliteral">&quot;output_buffer&quot;</span> <span class="keyword">port-index</span>=<span class="stringliteral">&quot;0&quot;</span> <span class="keyword">dim</span>=<span class="stringliteral">&quot;output,0&quot;</span> <span class="keyword">size</span>=<span class="stringliteral">&quot;Y*F*4&quot;</span>/&gt;</div><div class="line">        &lt;<span class="keywordtype">Tensor</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;variance&quot;</span>           <span class="keyword">type</span>=<span class="stringliteral">&quot;output_buffer&quot;</span> <span class="keyword">port-index</span>=<span class="stringliteral">&quot;1&quot;</span> <span class="keyword">dim</span>=<span class="stringliteral">&quot;output,0&quot;</span> <span class="keyword">size</span>=<span class="stringliteral">&quot;Y*F*4&quot;</span>/&gt;</div><div class="line">        </div><div class="line">    &lt;/<span class="keywordtype">Parameters</span>&gt;</div><div class="line">    &lt;<span class="keywordtype">WorkSizes</span> <span class="keyword">dim</span>=<span class="stringliteral">&quot;output,0&quot;</span> <span class="keyword">global</span>=<span class="stringliteral">&quot;((Y+7)/8)*8,F,1&quot;</span> <span class="keyword">local</span>=<span class="stringliteral">&quot;8,1,1&quot;</span>/&gt;</div><div class="line">&lt;/<span class="keywordtype">CustomLayer</span>&gt;</div><div class="line">&lt;<span class="keywordtype">CustomLayer</span> <span class="keyword">name</span>=<span class="stringliteral">&quot;MVN&quot;</span> <span class="keyword">stage</span>=<span class="stringliteral">&quot;1&quot;</span> <span class="keyword">type</span>=<span class="stringliteral">&quot;MVCL&quot;</span> <span class="keyword">version</span>=<span class="stringliteral">&quot;1&quot;</span>&gt;</div><div class="line">    &lt;<span class="keywordtype">Kernel</span> <span class="keyword">entry</span>=<span class="stringliteral">&quot;mvn_scale&quot;</span>&gt;</div><div class="line">        &lt;<span class="keywordtype">Source</span> <span class="keyword">filename</span>=<span class="stringliteral">&quot;mvn_scale_changed_orded.bin&quot;</span>/&gt;</div><div class="line">    &lt;/<span class="keywordtype">Kernel</span>&gt;</div><div class="line">    &lt;<span class="keywordtype">Parameters</span>&gt;</div><div class="line">        &lt;<span class="keywordtype">Tensor</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;src_data&quot;</span>           <span class="keyword">type</span>=<span class="stringliteral">&quot;input&quot;</span>        <span class="keyword">port-index</span>=<span class="stringliteral">&quot;0&quot;</span>               <span class="keyword">format</span>=<span class="stringliteral">&quot;BFYX&quot;</span>/&gt;</div><div class="line">        &lt;<span class="keywordtype">Tensor</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;dst_data&quot;</span>           <span class="keyword">type</span>=<span class="stringliteral">&quot;output&quot;</span>       <span class="keyword">port-index</span>=<span class="stringliteral">&quot;0&quot;</span>               <span class="keyword">format</span>=<span class="stringliteral">&quot;BFYX&quot;</span>/&gt;</div><div class="line">        &lt;<span class="keywordtype">Tensor</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;mean_part&quot;</span>          <span class="keyword">type</span>=<span class="stringliteral">&quot;input_buffer&quot;</span> <span class="keyword">port-index</span>=<span class="stringliteral">&quot;0&quot;</span> <span class="keyword">dim</span>=<span class="stringliteral">&quot;output,0&quot;</span> <span class="keyword">size</span>=<span class="stringliteral">&quot;Y*F*4&quot;</span>/&gt;</div><div class="line">        &lt;<span class="keywordtype">Tensor</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;power_mean&quot;</span>         <span class="keyword">type</span>=<span class="stringliteral">&quot;input_buffer&quot;</span> <span class="keyword">port-index</span>=<span class="stringliteral">&quot;1&quot;</span> <span class="keyword">dim</span>=<span class="stringliteral">&quot;output,0&quot;</span> <span class="keyword">size</span>=<span class="stringliteral">&quot;Y*F*4&quot;</span>/&gt;</div><div class="line">        </div><div class="line">    &lt;/<span class="keywordtype">Parameters</span>&gt;</div><div class="line">    &lt;<span class="keywordtype">WorkSizes</span> <span class="keyword">dim</span>=<span class="stringliteral">&quot;output,0&quot;</span> <span class="keyword">global</span>=<span class="stringliteral">&quot;((Y+7)/8)*8,F,1&quot;</span> <span class="keyword">local</span>=<span class="stringliteral">&quot;8,1,1&quot;</span>/&gt;</div><div class="line">&lt;/<span class="keywordtype">CustomLayer</span>&gt;</div></div><!-- fragment --></li>
<li>Each <code>Tensor</code> node that has type <code>data</code> must contain the following attribute:<ul>
<li><code>source</code> — A name of the blob as it's in the IR (typical example is <code>weights</code> for convolution).</li>
<li><code>format</code> — Specifies the channel order in the tensor. Optional conversion layers are generated if custom layer format is not. <div class="fragment"><div class="line">&lt;<span class="keywordtype">CustomLayer</span> <span class="keyword">name</span>=<span class="stringliteral">&quot;BinaryConvolution&quot;</span> <span class="keyword">type</span>=<span class="stringliteral">&quot;MVCL&quot;</span> <span class="keyword">version</span>=<span class="stringliteral">&quot;1&quot;</span>&gt;</div><div class="line">  &lt;<span class="keywordtype">Kernel</span> <span class="keyword">entry</span>=<span class="stringliteral">&quot;binary_convolution&quot;</span>&gt;</div><div class="line">      &lt;<span class="keywordtype">Source</span> <span class="keyword">filename</span>=<span class="stringliteral">&quot;binary_layers.bin&quot;</span>/&gt;</div><div class="line">  &lt;/<span class="keywordtype">Kernel</span>&gt;</div><div class="line">  &lt;<span class="keywordtype">Parameters</span>&gt;</div><div class="line">      &lt;<span class="keywordtype">Tensor</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;src_data&quot;</span>      <span class="keyword">type</span>=<span class="stringliteral">&quot;input&quot;</span>   <span class="keyword">port-index</span>=<span class="stringliteral">&quot;0&quot;</span>                      <span class="keyword">format</span>=<span class="stringliteral">&quot;BFYX&quot;</span>/&gt;</div><div class="line">      &lt;<span class="keywordtype">Data</span>   <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;weights_data&quot;</span>  <span class="keyword">type</span>=<span class="stringliteral">&quot;data&quot;</span>                     <span class="keyword">source</span>=<span class="stringliteral">&quot;weights&quot;</span>   <span class="keyword">format</span>=<span class="stringliteral">&quot;ANY&quot;</span>/&gt;</div><div class="line">      &lt;<span class="keywordtype">Tensor</span> <span class="keyword">arg-name</span>=<span class="stringliteral">&quot;dst_data&quot;</span>      <span class="keyword">type</span>=<span class="stringliteral">&quot;output&quot;</span>  <span class="keyword">port-index</span>=<span class="stringliteral">&quot;0&quot;</span>                      <span class="keyword">format</span>=<span class="stringliteral">&quot;BFYX&quot;</span>/&gt;</div><div class="line">      </div><div class="line">  &lt;/<span class="keywordtype">Parameters</span>&gt;</div><div class="line">  &lt;<span class="keywordtype">WorkSizes</span> <span class="keyword">dim</span>=<span class="stringliteral">&quot;output,0&quot;</span> <span class="keyword">global</span>=<span class="stringliteral">&quot;X,Y,F&quot;</span> <span class="keyword">local</span>=<span class="stringliteral">&quot;1,1,1&quot;</span>/&gt;</div><div class="line">&lt;/<span class="keywordtype">CustomLayer</span>&gt;</div></div><!-- fragment --></li>
</ul>
</li>
<li>Each <code>Scalar</code> node must contain the following attributes:<ul>
<li><code>arg-name</code> — A name of a kernel parameter in the kernel signature</li>
<li><code>type</code> — <code>int</code> or <code>float</code> value. It is used for correct argument extraction from IR parameters.</li>
<li><code>source</code> — Contains the name of the parameter in the IR file or input/output (<code>I</code>/<code>O</code>, <code>In</code>/<code>On</code>, where <code>n</code> is a port number) followed by dimension <code>B</code>(batch), <code>Y</code>(height), <code>X</code>(width) or <code>F</code>(channels).</li>
</ul>
</li>
</ul>
<h4>Pass the configuration file to the inference runtime</h4>
<blockquote class="doxtable">
<p><b>NOTE</b>: If both native and custom layer implementations are present, the custom kernel has a priority over the native one. </p>
</blockquote>
<p>Before loading the network that features the custom layers, provide a separate configuration file and load it using the <code><a class="el" href="classInferenceEngine_1_1Core.html#a268e2d24595061e9d804460cc6ca9ad3" title="Sets configuration for device, acceptable keys can be found in ie_plugin_config.hpp. ">InferenceEngine::Core::SetConfig()</a></code> method with the <code>PluginConfigParams::KEY_CONFIG_FILE</code> key and the configuration file name as a value: </p><div class="fragment"><div class="line"><a class="code" href="classInferenceEngine_1_1Core.html">InferenceEngine::Core</a> core;</div><div class="line"><span class="comment">// Load custom layers</span></div><div class="line">core.<a class="code" href="classInferenceEngine_1_1Core.html#a268e2d24595061e9d804460cc6ca9ad3">SetConfig</a>({ { <a class="code" href="namespaceInferenceEngine_1_1PluginConfigParams.html#a8b8c53f571862c6c394c67f3a66e7db2">InferenceEngine::PluginConfigParams::KEY_CONFIG_FILE</a>, <span class="stringliteral">&quot;&lt;path to the xml file&gt;&quot;</span> } }, <span class="stringliteral">&quot;MYRIAD&quot;</span>);</div></div><!-- fragment --><p> Optionally, you can set a path to a custom layers description with a pair of <code>VPU_CUSTOM_LAYERS</code> and <code>/path/to/your/customLayers.xml</code> as a network configuration: </p><div class="fragment"><div class="line"><a class="code" href="classInferenceEngine_1_1Core.html">InferenceEngine::Core</a> core;</div><div class="line">std::map&lt;std::string, std::string&gt; networkConfig;</div><div class="line">config[<span class="stringliteral">&quot;VPU_CUSTOM_LAYERS&quot;</span>] = <span class="stringliteral">&quot;/path/to/your/customLayers.xml&quot;</span>;</div><div class="line"><span class="comment">// Load custom layers in network config</span></div><div class="line"><span class="keyword">auto</span> exeNetwork = core.<a class="code" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff">LoadNetwork</a>(cnnNetwork, <span class="stringliteral">&quot;MYRIAD&quot;</span>, networkConfig);</div></div><!-- fragment --><h4>Optimizing kernels with OpenCL™ for VPU (Intel® Neural Compute Stick 2)</h4>
<p>This section provides optimization guidelines on writing custom layers with OpenCL for VPU devices. Knowledge about general OpenCL programming model and OpenCL kernel language is assumed and not a subject of this section. The OpenCL model mapping to VPU is described in the table below.</p>
<table class="doxtable">
<tr>
<th>OpenCL Model </th><th>VPU Mapping  </th></tr>
<tr>
<td>Device code </td><td>Executed on SHAVE cores </td></tr>
<tr>
<td>Private memory </td><td>Mapped to CMX internal memory, limited to 100KB per work group, valid only while work group is executed </td></tr>
<tr>
<td>Local memory </td><td>Mapped to CMX internal memory, limited to 100KB per work group, valid only while work group is executed </td></tr>
<tr>
<td>Global memory </td><td>Mapped to DDR, used to pass execution preserved parameters for inputs, outputs and blobs </td></tr>
<tr>
<td>Work group </td><td>Executed on a single SHAVE core iterating over multiple work items </td></tr>
</table>
<p>Note that by the OpenCL specification, the work group execution order is not specified. This means that it's your responsibility to ensure that race conditions among work groups are not introduced. Custom layer runtime spits evenly work grid among available compute resources and executes them in an arbitrary order. This static scheduling approach works best if the load is evenly spread out across work groups, which is a typical case for Deep Learning kernels. The following guidelines are recommended to use for work group partitioning:</p>
<ol type="1">
<li>Split work evenly across work groups.</li>
<li>Adjust work group granularity to maintain equal workload for all compute codes.</li>
<li>Set the maximum number of cores (using the <code>max-shaves</code> attribute for the <code>CustomLayer</code> node). This keeps more resources for the rest of topology. It also useful if the kernel scalability reached its limits, which may happen while optimizing memory bound kernels or kernels with poor parallelization.</li>
<li>Try an alternate data layout (<code>BFXY</code>/<code>BYXF</code>) for the kernel if it improves work group partitioning or data access patterns. Consider full topology performance (not just specific layer boost) since data conversion layers would be automatically inserted as appropriate.</li>
</ol>
<p>Offline OpenCL compiler (<code>clc</code>) features automatic vectorization over <code>get_global_id(0)</code> usage, if uniform access is detected. For example, the kernel below could be automatically vectorized: </p><div class="fragment"><div class="line">__kernel void cvtf32f16(__global float* restrict inImage, __global half*  restrict outImage,</div><div class="line">                        float   scale, float   bais)</div><div class="line">{</div><div class="line">    int idx = get_global_id(0) + get_global_id(1) * get_global_size(0) + get_global_id(2) * get_global_size(0) * get_global_size(1);</div><div class="line">    outImage[idx] = convert_half(inImage[idx]*scale+bais);</div><div class="line">}</div></div><!-- fragment --><p> However, this work-group based vectorizer (WGV) conflicts with the default LLVM vectorizer based on superword-level parallelism (SLP) for the current compiler version. Manual vectorization is recommended to provide the best performance for non-uniform code patterns. WGV works if and only if vector types aren't used in the code.</p>
<p>Here is a short list of optimization tips:</p>
<ol type="1">
<li>Help auto-vectorizer ensure non-aliasing pointers for kernel parameters by putting <code>restrict</code> where it possible.<ul>
<li>This may give a performance boost, especially for kernels with unrolling, like <code>ocl_grn</code> from example below.</li>
<li>It's important to place <code>restrict</code> markers for kernels with manually vectorized codes. In the <code>ocl_grn</code> kernel below, the unrolled version without <code>restrict</code> up to 20% slower then the most optimal one, which combines unrolling and <code>restrict</code>.</li>
</ul>
</li>
<li>Put <code>#&zwj;pragma unroll N</code> to your loop header. Since the compiler doesn't trigger unrolling by default, it's your responsibility to annotate the code with pragmas as appropriate. In <code>ocl_grn</code> version with <code>#&zwj;pragma unroll 4</code> is up to 50% faster, most of which comes from unrolling the first loop. The reason for it is that LLVM, in general, is better in scheduling 3-stage loops (load-compute-store), while the fist loop <code>variance += (float)(src_data[c*H*W + y*W + x] * src_data[c*H*W + y*W + x]);</code> is only 2-stage (load-compute). Please, pay attention to unrolling such cases first. Unrolling factor is loop-dependent. Choose the smallest number that still improves performance as an optimum between the kernel size and execution speed. For this specific kernel, changing the unroll factor from <code>4</code>to <code>6</code> results in the same performance, so unrolling factor equal to 4 is an optimum. For Intel® Neural Compute Stick 2, unrolling is conjugated with the automatic software pipelining for load, store and compute stages: <div class="fragment"><div class="line">__kernel void ocl_grn(__global const half* restrict src_data, __global half* restrict dst_data, int C, float bias)</div><div class="line">{</div><div class="line">    int x = get_global_id(0);</div><div class="line">    int W = get_global_size(0);</div><div class="line">    int y = get_global_id(1);</div><div class="line">    int H = get_global_size(1);</div><div class="line"></div><div class="line">    float variance = bias + 1e-9f;</div><div class="line"></div><div class="line">    #pragma unroll 4</div><div class="line">    for (int c = 0; c &lt; C; c++)</div><div class="line">        variance += (float)(src_data[c*H*W + y*W + x] * src_data[c*H*W + y*W + x]);</div><div class="line"></div><div class="line">    variance = 1.f / native_sqrt(variance);</div><div class="line"></div><div class="line">    #pragma unroll 4</div><div class="line">    for (int c = 0; c &lt; C; c++)</div><div class="line">        dst_data[c*H*W + y*W + x] = (half)((float)src_data[c*H*W + y*W + x] * variance);</div><div class="line">}</div></div><!-- fragment --> To check the efficiency of WGV, you can compare performance of the kernel above with the kernel below, which is manually vectorized over width: <div class="fragment"><div class="line">__kernel void ocl_grn_line(__global const half* restrict src_data,  __global half* restrict dst_data, int C, int W, float bias)</div><div class="line">{</div><div class="line">    int y   = get_global_id(1);</div><div class="line">    int H   = get_global_size(1);</div><div class="line"></div><div class="line">    for (int x = 0; x &lt; W/8; x++)</div><div class="line">    {</div><div class="line">        float8 variance = (float8)(bias+1e-9f);</div><div class="line"></div><div class="line">        #pragma unroll 4</div><div class="line">        for (int c = 0; c &lt; C; c++)</div><div class="line">        {</div><div class="line">            __global const half8* restrict src_line = ((__global const half8 * restrict)(src_data + c*H*W + y*W));</div><div class="line">            half8 sh = src_line[x];</div><div class="line">            variance += convert_float8(sh*sh);</div><div class="line">        }</div><div class="line"></div><div class="line">        variance = 1.f/native_sqrt(variance);</div><div class="line"></div><div class="line">        #pragma unroll 4</div><div class="line">        for (int c = 0; c &lt; C; c++)</div><div class="line">        {</div><div class="line">            __global const half8* restrict src_line = ((__global const half8 * restrict)(src_data + c*H*W + y*W));</div><div class="line">            __global       half8* restrict dst_line = ((__global       half8 * restrict)(dst_data + c*H*W + y*W));</div><div class="line"></div><div class="line">            dst_line[x] = convert_half8(convert_float8(src_line[x])*variance);</div><div class="line">        }</div><div class="line">    }</div><div class="line">    for (int x = W/8*8; x &lt; W; x++)</div><div class="line">    {</div><div class="line">        float variance = bias+1e-9f;</div><div class="line">        #pragma unroll 4</div><div class="line">        for (int c = 0; c &lt; C; c++)</div><div class="line">            variance += (float)(src_data[c*H*W + y*W + x]*src_data[c*H*W + y*W + x]);</div><div class="line"></div><div class="line">        variance = 1.f/native_sqrt(variance);</div><div class="line"></div><div class="line">        #pragma unroll 4</div><div class="line">        for (int c = 0; c &lt; C; c++)</div><div class="line">            dst_data[c*H*W + y*W + x] = (float)src_data[c*H*W + y*W + x]*variance;</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --> Both versions perform the same, but the second one has more complex code.</li>
<li>If it's easy to predict the work group size, you can also use the <code>reqd_work_group_size</code> kernel attribute to ask the compiler to unroll the code up to local size of the work group. Please note that if the kernel is actually executed with the different work group configuration, the result is undefined.</li>
<li>Prefer to use the <code>half</code> compute, if it keeps reasonable accuracy. 16-bit float is a native type for Intel® Neural Compute Stick 2, most of the functions <code>half_*</code> are mapped to a single hardware instruction. Use the standard <code>native_*</code> function for the rest of types.</li>
<li>Prefer to use the <code>convert_half</code> function over <code>vstore_half</code> if conversion to 32-bit float is required. <code>convert_half</code> is mapped to a single hardware instruction. For the <code>cvtf32f16</code> kernel above, the line <code>outImage[idx] = convert_half(inImage[idx]*scale+bais);</code> is 8 times slower than the code with <code>vstore_half</code>.</li>
<li>Mind early exits. Early exit may be extremely costly for the current version of the <code>clc</code> compiler due to conflicts with the auto-vectorizer. The generic advice would be to setup local size by <code>x</code> dimension equal to inputs or/and outputs width. If it's impossible to define the work grid that exactly matches inputs or/and outputs to eliminate checks (for example, <code>if (get_global_id(0) &gt;= width) return</code>), use line-wise kernel variant with manual vectorization.</li>
</ol>
<p>The kernel example below demonstrates the impact of early exits on kernel performance. </p><div class="fragment"><div class="line">// Initial version</div><div class="line">__kernel void reorg(const __global half* restrict src, __global half* restrict out, int stride)</div><div class="line">{</div><div class="line">    int w = get_global_id(0);</div><div class="line">    int W = get_global_size(0);</div><div class="line"></div><div class="line">    int h = get_global_id(1);</div><div class="line">    int H = get_global_size(1);</div><div class="line"></div><div class="line">    int c = get_global_id(2);</div><div class="line">    int C = get_global_size(2);</div><div class="line"></div><div class="line">    int C2 = C/(stride*stride);</div><div class="line">    int offset = c / C2;</div><div class="line">    int c2 = c - C2 * offset;</div><div class="line"></div><div class="line">    int H2 = H*stride;</div><div class="line">    int W2 = W*stride;</div><div class="line"></div><div class="line">    int h2 = h*stride + offset / stride;</div><div class="line">    int w2 = w*stride + offset - stride * (offset / stride);</div><div class="line"></div><div class="line">    out[W*H*c + W*h + w] = src[W2*H2*c2 + W2*h2 + w2];</div><div class="line">}</div></div><!-- fragment --><p> This <code>reorg</code> kernel is auto-vectorizable, but an input for Yolo v2 topology is <code>NCHW=&lt;1,64,26,26&gt;</code> and it's not multiple of vector width (which is 8 for <code>half</code> data type). As a result, Inference Engine doesn't select the auto-vectorized kernel. To compare performance of auto-vectorized and scalar version of the kernel, change the input size to<code>NCHW=&lt;1,64,26,32&gt;</code>. This allows the auto-vectorized version to be selected by Inference Engine and can give you about 30% uplift. Since the auto-vectorized version is faster, it makes sense to enable it for the Yolo v2 topology input size by setting the local size multiple of vector (e.g. 32) and adjust global sizes accordingly. As the result, the execution work grid exceeds actual input dimension so out-of-bound checks should be inserted. See the updated kernel version below: </p><div class="fragment"><div class="line">// Version with out-of-bound checks added</div><div class="line">__kernel void reorg(const __global half* restrict src, __global half* restrict out, int W, int stride)</div><div class="line">{</div><div class="line">    int w = get_global_id(0);</div><div class="line">    w = min(w, W-1);</div><div class="line"></div><div class="line">    int h = get_global_id(1);</div><div class="line">    int H = get_global_size(1);</div><div class="line"></div><div class="line">    int c = get_global_id(2);</div><div class="line">    int C = get_global_size(2);</div><div class="line"></div><div class="line">    int C2 = C/(stride*stride);</div><div class="line">    int offset = c / C2;</div><div class="line">    int c2 = c - C2 * offset;</div><div class="line"></div><div class="line">    int H2 = H*stride;</div><div class="line">    int W2 = W*stride;</div><div class="line"></div><div class="line">    int h2 = h*stride + offset / stride;</div><div class="line">    int w2 = w*stride + offset - stride * (offset / stride);</div><div class="line"></div><div class="line">    out[W*H*c + W*h + w] = src[W2*H2*c2 + W2*h2 + w2];</div><div class="line">}</div></div><!-- fragment --><p> This code performs the same as the initial kernel above (scalar) due to branching overhead. If you replace min/max expression <code>w = min(w, W-1);</code> with <code>if (w &gt;= W) return;</code>, runtime increases up to 2x against to code without branching (initial version).</p>
<p>If branching is inevitable for your element-based kernel, it's recommended to change the scheme to line-based. See the kernel variant below: </p><div class="fragment"><div class="line">// Line-wise version</div><div class="line">__kernel void reorg(const __global half* restrict src, __global half* restrict out, int H, int W, int stride)</div><div class="line">{</div><div class="line">    int h = min((int)get_global_id(0), H-1);</div><div class="line"></div><div class="line">    int c = get_global_id(1);</div><div class="line">    int C = get_global_size(1);</div><div class="line">    int C2 = C/(stride*stride);</div><div class="line">    int offset = c / C2;</div><div class="line">    int c2 = c - C2 * offset;</div><div class="line"></div><div class="line">    int H2 = H*stride;</div><div class="line">    int W2 = W*stride;</div><div class="line"></div><div class="line">    for (int w = 0; w &lt; W; ++w)</div><div class="line">    {</div><div class="line">        int h2 = h*stride + offset / stride;</div><div class="line">        int w2 = w*stride + offset - stride * (offset / stride);</div><div class="line"></div><div class="line">        out[W*H*c + W*h + w] = src[W2*H2*c2 + W2*h2 + w2];</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p> This decreases the execution time up to 40% against the best performing vectorized kernel without early exits (initial version).</p><ol type="1">
<li>Reuse computations among work items by using line-based kernels or sharing values though <code>__local</code> memory.</li>
<li>Improve data access locality. Most of custom kernels are memory bound while convolution and fully connected layers are hardware-implemented. The code below demonstrates a further optimized version of the <code>reorg</code> kernel unrolled by <code>stride</code>: <div class="fragment"><div class="line">// Unrolled line-wise version</div><div class="line">__kernel void reorg_unrolled_by_stride(const __global half* restrict src, __global half* restrict dst,</div><div class="line">                                       int H, int W, int stride)</div><div class="line">{</div><div class="line">    int h = min((int)get_global_id(0), H-1);</div><div class="line"></div><div class="line">    int c2 = get_global_id(1);</div><div class="line">    int C2 = get_global_size(1);</div><div class="line">    int C = C2*stride*stride;</div><div class="line"></div><div class="line">    int H2 = H*stride;</div><div class="line">    int W2 = W*stride;</div><div class="line"></div><div class="line">    for (int stride_y = 0; stride_y &lt; stride; stride_y++)</div><div class="line">        for (int stride_x = 0; stride_x &lt; stride; stride_x++)</div><div class="line">            for (int w2 = 0, w = 0; w &lt; W; w2 += stride, w++)</div><div class="line">                dst[W*H*C2*(stride_y*stride+stride_x) + W*H*c2 + W*h + w] = src[W2*H2*c2 + W2*h*stride + W2*stride_y + w2 + stride_x];</div><div class="line">}</div></div><!-- fragment --> <code>scr</code> data in this case loaded only once. As the result, the cycle count drops up to 45% against the line-wise version.</li>
<li>Copy data from <code>__dlobal</code> to <code>__local</code> or <code>__private</code> memory if the data is accessed more than once. Access to <code>__dlobal</code> memory is orders of magnitude slower than access to <code>__local</code>/<code>__private</code> due to statically scheduled pipeline, which stalls completely on memory access without any prefetch. The same recommendation is applicable for scalar load/store from/to a <code>__blobal</code> pointer since work-group copying could be done in a vector fashion.</li>
</ol>
<h2>See Also</h2>
<ul>
<li><a class="el" href="_docs_IE_DG_Samples_Overview.html">Using Inference Engine Samples</a> </li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>