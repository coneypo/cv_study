<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Optimization Guide - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Optimization Guide </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Introduction</h2>
<p>The purpose of this document is to give you performance-related insights to every step of the network deployment process.</p>
<p>For information on the general workflow, refer to the documentation in <a href="#see-also">See Also</a>. For an example Inference Engine API snippet, see <a href="#new-request-based-api">Request-Based API and “GetBlob” Idiom</a>.</p>
<h3>Deep Learning Inference Engine Overview <a class="anchor" id="dldt-overview"></a></h3>
<p>Deep Learning Inference Engine is a part of Intel&reg; Deep Learning Deployment Toolkit (Intel&reg; DL Deployment Toolkit) and OpenVINO&trade; toolkit. Inference Engine facilitates deployment of deep learning solutions by delivering a unified, device-agnostic API.</p>
<p>Below, there are the three main steps of the deployment process:</p>
<ol type="1">
<li><b>Conversion</b><br />
 Trained models are converted from a specific framework (like Caffe* or TensorFlow*) to a framework-agnostic Intermediate Representation (IR) format.<ul>
<li><em>Performance flow</em>: This is an offline step where general topology-level optimizations happen automatically (see <a href="#mo-knobs-related-to-performance">Model Optimizer Knobs Related to Performance</a>).</li>
<li><em>Tools</em>: Intel DL Deployment Toolkit features the Model Optimizer that enables automatic and seamless transition from the training environment to the deployment environment.</li>
</ul>
</li>
<li><b>Model Inference/Execution</b><br />
 After conversion, Inference Engine consumes the IR to perform inference. While Inference Engine API itself is target-agnostic, internally, it has a notion of plugins, which are device-specific libraries facilitating the hardware-assisted acceleration.<ul>
<li><em>Performance flow</em>: Upon conversion to IR, the execution starts with existing <a class="el" href="_docs_IE_DG_Samples_Overview.html">Inference Engine samples</a> to measure and tweak the performance of the network on different devices.<br />
 &gt; <b>NOTE</b>: While consuming the same IR, each plugin performs additional device-specific optimizations at load time, so the resulting accuracy might differ. Also, enabling and optimizing custom kernels is error-prone (see <a href="#optimizing-custom-kernels">Optimizing Custom Kernels</a>).</li>
<li><em>Tools</em>: Beyond inference performance that samples report (see <a href="#latency-vs-throughput">Latency vs. Throughput</a>), you can get further device- and kernel-level timing with the <a href="#performance-counters">Inference Engine performance counters</a> and <a href="#vtune-examples">Intel&reg; VTune&trade;</a>.</li>
</ul>
</li>
<li><b>Integration to the product</b><br />
 After model inference is verified with the <a class="el" href="_docs_IE_DG_Samples_Overview.html">samples</a>, the Inference Engine code is typically integrated into a real application or pipeline.<ul>
<li><em>Performance flow</em>: The most important point is to preserve the sustained performance achieved with the stand-alone model execution. Take precautions when combining with other APIs and be careful testing the performance of every integration step.</li>
<li><em>Tools</em>: Beyond tracking the actual wall-clock time of your application, see <a href="#vtune-examples">Intel&reg; VTune&trade; Examples</a> for application-level and system-level information.</li>
</ul>
</li>
</ol>
<h2>Gathering the Performance Numbers <a class="anchor" id="gathering-performance-numbers"></a></h2>
<p>Performance data comes in a variety of forms. For example, one of the the most common performance metrics is latency, which represents the time required to complete a unit of work (for instance, inference time for a single image). In the following sections, you will see important recommendations for measuring the performance.</p>
<h3>Measure the Proper Set of Operations <a class="anchor" id="measure-proper-set-of-operations"></a></h3>
<p>When evaluating performance of your model with the Inference Engine, you must measure the proper set of operations. To do so, consider the following tips:</p>
<ul>
<li>Avoid including one-time costs like model loading. For examples, refer to the <a class="el" href="_docs_IE_DG_Samples_Overview.html">Inference Engine samples</a>.</li>
<li>Track separately the operations that happen outside the Inference Engine, like video decoding.</li>
</ul>
<blockquote class="doxtable">
<p><b>NOTE</b>: Some image pre-processing can be baked into the IR and accelerated. For more information, refer to <a href="#mo-knobs-related-to-performance">Model Optimizer Knobs Related to Performance</a>. </p>
</blockquote>
<h3>Latency vs. Throughput <a class="anchor" id="latency-vs-throughput"></a></h3>
<p>In the asynchronous case (see <a href="#new-request-based-api">Request-Based API and “GetBlob” Idiom</a>), the performance of an individual infer request is usually of less concern. Instead, you typically execute multiple requests asynchronously and measure the throughput in images per second by dividing the number of images that were processed by the processing time. In contrast, for the latency-oriented tasks, the time to a single frame is more important.</p>
<p>Refer to the <a class="el" href="_inference_engine_samples_benchmark_app_README.html">Benchmark App</a> sample, which allows latency vs. throughput measuring.</p>
<blockquote class="doxtable">
<p><b>NOTE</b>: Most samples also support batching (automatically packing multiple input images into a single request). However, high batch size results in a latency penalty. So for more real-time oriented usages, lower batch sizes (as low as a single input) are usually used. However, devices like CPU, Intel&reg; Movidius&trade; Myriad&trade; 2 VPU, Intel&reg; Movidius&trade; Myriad&trade; X VPU, or Intel® Vision Accelerator Design with Intel® Movidius™ VPU require a number of parallel requests instead of batching to leverage the performance. </p>
</blockquote>
<h3>Comparing Performance with Native/Framework Code <a class="anchor" id="comparing-performance-with-native-framework-code"></a></h3>
<p>When comparing the Inference Engine performance with the framework or another reference code, make sure that both versions are as similar as possible:</p>
<ul>
<li>Wrap exactly the inference execution (refer to the <a class="el" href="_docs_IE_DG_Samples_Overview.html">Inference Engine Samples</a> for examples).</li>
<li>Do not include model loading time.</li>
<li>Ensure the inputs are identical for the Inference Engine and the framework. For example, Caffe* allows to auto-populate the input with random values. Notice that it might give different performance than on real images.</li>
<li>Similarly, for correct performance comparison, make sure the access pattern, for example, input layouts, is optimal for Inference Engine (currently, it is NCHW).</li>
<li>Any user-side pre-processing should be tracked separately.</li>
<li>Make sure to try the same environment settings that the framework developers recommend, for example, for TensorFlow*. In many cases, things that are more machine friendly, like respecting NUMA (see <a href="#cpu-checklist">CPU Checklist</a>), might work well for the Inference Engine as well.</li>
<li>If applicable, use batching with the Inference Engine.</li>
<li>If possible, demand the same accuracy. For example, TensorFlow allows <code>FP16</code> support, so when comparing to that, make sure to test the Inference Engine with the <code>FP16</code> as well.</li>
</ul>
<h3>Getting Credible Performance Numbers <a class="anchor" id="getting-credible-performance-numbers"></a></h3>
<p>You need to build your performance conclusions on reproducible data. Do the performance measurements with a large number of invocations of the same routine. Since the first iteration is almost always significantly slower than the subsequent ones, you can use an aggregated value for the execution time for final projections:</p>
<ul>
<li>If the warm-up run does not help or execution time still varies, you can try running a large number of iterations and then average or find a mean of the results.</li>
<li>For time values that range too much, use geomean.</li>
</ul>
<p>Refer to the <a class="el" href="_docs_IE_DG_Samples_Overview.html">Inference Engine Samples</a> for code examples for the performance measurements. Almost every sample, except interactive demos, has a <code>-ni</code> option to specify the number of iterations.</p>
<h2>Model Optimizer Knobs Related to Performance <a class="anchor" id="mo-knobs-related-to-performance"></a></h2>
<p>Networks training is typically done on high-end data centers, using popular training frameworks like Caffe*, TensorFlow*, and MXNet*. Model Optimizer converts the trained model in original proprietary formats to IR that describes the topology. IR is accompanied by a binary file with weights. These files in turn are consumed by the Inference Engine and used for scoring.</p>
<div class="image">
<img src="workflow_steps.png" alt="workflow_steps.png"/>
</div>
<p>As described in the <a class="el" href="_docs_MO_DG_prepare_model_Prepare_Trained_Model.html">Model Optimizer Guide</a>, there are a number of device-agnostic optimizations the tool performs. For example, certain primitives like linear operations (BatchNorm and ScaleShift), are automatically fused into convolutions. Generally, these layers should not be manifested in the resulting IR:</p>
<div class="image">
<img src="resnet_269.png" alt="resnet_269.png"/>
</div>
<p>The picture above shows Caffe* Resnet269* topology. The left model is the original model, and the one on the right (after conversion) is the resulting model that the Model Optimizer produces, with BatchNorm and ScaleShift layers fused into the convolution weights rather than constituting separate layers.</p>
<p>If you still see these operations, inspect the Model Optimizer output carefully while searching for warnings, such as on the tool being unable to fuse. For example, non-linear operations (like activations) in between convolutions and linear operations might prevent the fusing. If performance is of concern, try to change (and potentially re-train) the topology. Refer to the <a class="el" href="_docs_MO_DG_prepare_model_Model_Optimization_Techniques.html">Model Optimizer Guide</a> for more optimizations.</p>
<p>Notice that the activation (<code>_relu</code>) is not touched by the Model Optimizer, and while it can be merged into convolution as well, this is rather a device-specific optimization, covered by Inference Engine during the model loading time. You are encouraged to inspect performance counters from plugins that should indicate that these particular layers are not executed (“Optimized out”). For more information, refer to <a href="#performance-counters">Internal Inference Performance Counters</a>.</p>
<p>Also:</p>
<ul>
<li><b>Image mean/scale parameters</b><br />
 Make sure to use the input image mean/scale parameters (<code>--scale</code> and <code>–mean_values</code>) with the Model Optimizer when you need pre-processing. It allows the tool to bake the pre-processing into the IR to get accelerated by the Inference Engine.</li>
<li><b>RGB vs. BGR inputs</b><br />
 If, for example, your network assumes the RGB inputs, the Model Optimizer can swap the channels in the first convolution using the <code>--reverse_input_channels</code> command line option, so you do not need to convert your inputs to RGB every time you get the BGR image, for example, from OpenCV*.</li>
<li><b>Larger batch size</b><br />
 Notice that the devices like GPU are doing better with larger batch size. While it is possible to set the batch size in the runtime using the Inference Engine <a class="el" href="_docs_IE_DG_ShapeInference.html">ShapeInference feature</a>.</li>
<li><b>Resulting IR precision</b><br />
 The resulting IR precision, for instance, <code>FP16</code> or <code>FP32</code>, directly affects performance. As CPU now supports <code>FP16</code> (while internally upscaling to <code>FP32</code> anyway) and because this is the best precision for a GPU target, you may want to always convert models to <code>FP16</code>. Notice that this is the only precision that Intel&reg; Movidius&trade; Myriad&trade; 2 and Intel&reg; Myriad&trade; X VPUs support.</li>
</ul>
<h2>Device-Specific Optimizations <a class="anchor" id="device-specific-optimizations"></a></h2>
<p>The Inference Engine supports several target devices (CPU, GPU, Intel&reg; Movidius&trade; Myriad&trade; 2 VPU, Intel&reg; Movidius&trade; Myriad&trade; X VPU, Intel® Vision Accelerator Design with Intel® Movidius™ Vision Processing Units (VPU) and FPGA), and each of them has a corresponding plugin. If you want to optimize a specific device, you must keep in mind the following tips to increase the performance.</p>
<h3>CPU Checklist <a class="anchor" id="cpu-checklist"></a></h3>
<p>CPU plugin completely relies on the Intel&reg; Math Kernel Library for Deep Neural Networks (Intel&reg; MKL-DNN) for major primitives acceleration, for example, Convolutions or FullyConnected.</p>
<p>The only hint you can get from that is how the major primitives are accelerated (and you cannot change this). For example, on the Core machines, you should see variations of the <code>jit_avx2</code> when inspecting the <a href="#performance-counters">internal inference performance counters</a> (and additional '_int8' postfix for <a class="el" href="_docs_IE_DG_Int8Inference.html">int8 inference</a>). If you are an advanced user, you can further trace the CPU execution with (see <a href="#vtune-examples">Intel&reg; VTune&trade;</a>).</p>
<p>Internally, the Inference Engine has a threading abstraction level, which allows for compiling the <a href="https://github.com/opencv/dldt">open source version</a> with either Intel&reg; Threading Building Blocks (Intel&reg; TBB) which is now default, or OpenMP* as an alternative parallelism solution. When using inference on the CPU, this is particularly important to align threading model with the rest of your application (and any third-party libraries that you use) to avoid oversubscription. For more information, see <a href="#note-on-app-level-threading">Note on the App-Level Threading</a> section.</p>
<p>Since R1 2019, the OpenVINO&trade; toolkit comes pre-compiled with Intel TBB, so any OpenMP* API or environment settings (like <code>OMP_NUM_THREADS</code>) has no effect anymore. Certain tweaks (like number of threads used for inference on the CPU) are still possible via <a class="el" href="_docs_IE_DG_supported_plugins_CPU.html">CPU configuration options</a>.</p>
<p>Other general recommendations:</p><ul>
<li>Usually, batching improves CPU performance. However, the need to gather frames in the batch might complicate the application logic. Instead, you can keep a separate infer request per camera or other source of input and process the requests in parallel. For more information, see the next section.</li>
<li>If your application simultaneously performs inference of multiple models on the same CPU, make sure you do not oversubscribe the machine. See <a href="#running-multiple-requests-simultaneously">Performance Aspects of Running Multiple Requests Simultaneously</a> for more information.</li>
<li>Notice that the heterogeneous execution might implicitly load the CPU. For details, refer to the <a href="#heterogeneity">Heterogeneity</a> section.</li>
<li>Consider <a class="el" href="_docs_IE_DG_Int8Inference.html">8-bit integer inference on the CPU</a>.</li>
</ul>
<h4>Throughput Mode for CPU <a class="anchor" id="cpu-streams"></a></h4>
<p>Unlike most accelerators, CPU is perceived as an inherently latency-oriented device. In fact, the OpenVINO does support the "throughput" mode for the CPU, which allows the Inference Engine to efficiently run multiple inference requests on the CPU simultaneously, greatly improving the overall throughput.</p>
<p>Internally, the execution resources are split/pinned into execution "streams". This feature usually provides much better performance for the networks than batching. This is especially pronounced for the many-core server machines.</p>
<p>Try the <a class="el" href="_inference_engine_samples_benchmark_app_README.html">Benchmark App</a> sample and play with number of streams running in parallel. The rule of thumb is tying up to a number of CPU cores on your machine. For example, on an 8-core CPU, compare the <code>-nstreams 1</code> (which is a legacy, latency-oriented scenario) to the 2, 4, and 8 streams.</p>
<p>In addition, you can play with the batch size to find the throughput sweet spot.</p>
<p>If your application is hard or impossible to change in accordance with the multiple-requests logic, consider the "multiple-instance" trick to improve the throughput:</p><ul>
<li>For multi-socket execution, it is recommended to set <a class="el" href="_docs_IE_DG_supported_plugins_CPU.html">`KEY_CPU_THREADS_NUM`</a> to the number of cores per socket, and run as many instances of the application as you have sockets.</li>
<li>Similarly, for extremely lightweight networks (running faster than 1ms) and/or many-core machines (16+ cores), try limiting the number of CPU inference threads to just <code>#&zwj;phys</code> cores and further, while trying to saturate the machine with running multiple instances of the application.</li>
</ul>
<h3>GPU Checklist <a class="anchor" id="gpu-checklist"></a></h3>
<p>Inference Engine relies on the <a href="https://01.org/cldnn">Compute Library for Deep Neural Networks (clDNN)</a> for Convolutional Neural Networks acceleration on Intel&reg; GPUs. Internally, clDNN uses OpenCL&trade; to implement the kernels. Thus, many general tips apply:</p>
<ul>
<li>Prefer <code>FP16</code> over <code>FP32</code>, as the Model Optimizer can generate both variants and the <code>FP32</code> is default.</li>
<li>Try to group individual infer jobs by using batches.</li>
<li>Notice that using the GPU introduces one-time overhead (order of few seconds) of compiling the OpenCL kernels. The compilation happens upon loading the network to the GPU plugin and does not affect the inference time.</li>
<li>If your application is simultaneously using the inference on the CPU or otherwise loads the host heavily, make sure that the OpenCL driver threads do not starve. You can use <a class="el" href="_docs_IE_DG_supported_plugins_CPU.html">CPU configuration options</a> to limit number of inference threads for the CPU plugin.</li>
<li>In the GPU-only scenario, a GPU driver might occupy a CPU core with spin-looped polling for completion. If the <em>CPU</em> utilization is a concern, consider the <code>KEY_CLDND_PLUGIN_THROTTLE</code> configuration option.</li>
</ul>
<blockquote class="doxtable">
<p><b>NOTE</b>: See the <a class="el" href="_inference_engine_samples_benchmark_app_README.html">Benchmark App Sample</a> code for a usage example. </p>
</blockquote>
<p>Notice that while disabling the polling, this option might reduce the GPU performance, so usually this option is used with multiple <a class="el" href="_docs_IE_DG_supported_plugins_CL_DNN.html">GPU streams</a>.</p>
<h3>Intel&reg; Movidius&trade; Myriad&trade; X Visual Processing Unit and Intel&reg; Vision Accelerator Design with Intel&reg; Movidius&trade; VPUs <a class="anchor" id="myriad"></a></h3>
<p>Since Intel&reg; Movidius&trade; Myriad&trade; X Visual Processing Unit (Intel&reg; Movidius&trade; Myriad&trade; 2 VPU) communicates with the host over USB, minimum four infer requests in flight are recommended to hide the data transfer costs. See <a href="#new-request-based-api">Request-Based API and “GetBlob” Idiom</a> and <a class="el" href="_inference_engine_samples_benchmark_app_README.html">Benchmark App Sample</a> for more information.</p>
<p>Intel&reg; Vision Accelerator Design with Intel&reg; Movidius&trade; VPUs requires to keep at least 32 inference requests in flight to fully saturate the device.</p>
<h3>FPGA <a class="anchor" id="fpga"></a></h3>
<p>Below are listed the most important tips for the efficient usage of the FPGA:</p>
<ul>
<li>Just like for the Intel&reg; Movidius&trade; Myriad&trade; VPU flavors, for the FPGA, it is important to hide the communication overheads by running multiple inference requests in parallel. For examples, refer to the <a class="el" href="_inference_engine_samples_benchmark_app_README.html">Benchmark App Sample</a>.</li>
<li>Since the first inference iteration with FPGA is always significantly slower than the subsequent ones, make sure you run multiple iterations (all samples, except GUI-based demos, have the <code>-ni</code> or 'niter' option to do that).</li>
<li>FPGA performance heavily depends on the bitstream.</li>
<li>Number of the infer request per executable network is limited to five, so “channel” parallelism (keeping individual infer request per camera/video input) would not work beyond five inputs. Instead, you need to mux the inputs into some queue that will internally use a pool of (5) requests.</li>
<li>In most scenarios, the FPGA acceleration is leveraged through <a href="heterogeneity">heterogeneous execution</a> with further specific tips.</li>
<li>For multi-device FPGA execution please refer to the <a class="el" href="_docs_IE_DG_supported_plugins_FPGA.html">FPGA plugin documentation</a></li>
</ul>
<h2>Heterogeneity <a class="anchor" id="heterogeneity"></a></h2>
<p>Heterogeneous execution (constituted by the dedicated Inference Engine <a class="el" href="_docs_IE_DG_supported_plugins_HETERO.html">“Hetero” plugin</a>) enables to schedule a network inference to the multiple devices.</p>
<h3>Typical Heterogeneous Scenarios of Concern <a class="anchor" id="heterogeneous-scenarios-of-concern"></a></h3>
<p>The primary points for executing a network in heterogeneous mode are as follows:</p>
<ul>
<li>Calculate the heaviest pieces of the network with an accelerator while falling back to the CPU for the layers that are not supported by the accelerator.<br />
 This is particularly useful when certain custom (user) kernels are implemented only for the CPU (and much harder or even impossible to implement for the accelerator).</li>
<li>Use all available compute devices more efficiently, for example, by running branches of the network on the different devices.</li>
</ul>
<h3>Heterogeneous Flow <a class="anchor" id="heterogeneous-flow"></a></h3>
<p>The execution through heterogeneous plugin has three distinct steps:</p>
<ol type="1">
<li><b>Applying affinity setting for the layers</b>, that is, binding them to the devices.<ul>
<li>This can be done automatically using <em>fallback priorities</em>, or on the <em>per-layer</em> basis.</li>
<li>The affinity setting is made before loading the network to the (heterogeneous) plugin, so this is always a <b>static</b> setup with respect to execution.</li>
</ul>
</li>
<li><b>Loading a network to the heterogeneous plugin</b>, which internally splits the network into subgraphs.<br />
 You can check the decisions the plugin makes, see <a href="#analyzing-heterogeneous-execution">Analysing the Heterogeneous Execution</a>.</li>
<li><b>Executing the infer requests</b>. From user’s side, this looks identical to a single-device case, while internally, the subgraphs are executed by actual plugins/devices.</li>
</ol>
<p>Performance benefits of the heterogeneous execution depend heavily on the communications granularity between devices. If transmitting/converting data from one part device to another takes more time than the execution, the heterogeneous approach makes little or no sense. Using Intel&reg; VTune&trade; helps to visualize the execution flow on a timeline (see <a href="#vtune-examples">Intel&reg; VTune&trade; Examples</a>).</p>
<p>Similarly, if there are too much subgraphs, the synchronization and data transfers might eat the entire performance. In some cases, you can define the (coarser) affinity manually to avoid sending data back and forth many times during one inference.</p>
<p>The general affinity “rule of thumb” is to keep computationally-intensive kernels on the accelerator, and "glue" or helper kernels on the CPU. Notice that this includes the granularity considerations. For example, running some custom activation (that comes after every accelerator-equipped convolution) on the CPU might result in performance degradation due to too much data type and/or layout conversions, even though the activation itself can be extremely fast. In this case, it might make sense to consider implementing the kernel for the accelerator (see <a href="#optimizing-custom-kernels">Optimizing Custom Kernels</a>). The conversions typically manifest themselves as outstanding (comparing to CPU-only execution) 'Reorder' entries (see <a href="#performance-counters">Internal Inference Performance Counters</a>).</p>
<p>For general details on the heterogeneous plugin, refer to the <a class="el" href="_docs_IE_DG_supported_plugins_HETERO.html">corresponding section in the Inference Engine Developer Guide</a>.</p>
<h3>Trying the Heterogeneous Plugin with Inference Engine Samples <a class="anchor" id="heterogeneous-plugin-with-samples"></a></h3>
<p>Every Inference Engine sample supports the <code>-d</code> (device) option.</p>
<p>For example, here is a command to run an <a class="el" href="_inference_engine_samples_object_detection_sample_ssd_README.html">Object Detection Sample SSD Sample</a>:</p>
<div class="fragment"><div class="line">./object_detection_sample_ssd -m  &lt;path_to_model&gt;/ModelSSD.xml -i &lt;path_to_pictures&gt;/picture.jpg -d HETERO:FPGA,CPU</div></div><!-- fragment --><p>where:</p>
<ul>
<li><code>HETERO</code> stands for Heterogeneous plugin.</li>
<li><code>FPGA,CPU</code> points to fallback policy with first priority on FPGA and further fallback to CPU.</li>
</ul>
<p>You can point more than two devices: <code>-d HETERO:FPGA,GPU,CPU</code>.</p>
<h3>Heterogeneous Scenarios with FPGA <a class="anchor" id="heterogeneous-scenarios-fpga"></a></h3>
<p>As FPGA is considered as an inference accelerator, most performance issues are related to the fact that due to the fallback, the CPU can be still used quite heavily.</p><ul>
<li>Yet in most cases, the CPU does only small/lightweight layers, for example, post-processing (<code>SoftMax</code> in most classification models or <code>DetectionOutput</code> in the SSD*-based topologies). In that case, limiting the number of CPU threads with <a class="el" href="_docs_IE_DG_supported_plugins_CPU.html">`KEY_CPU_THREADS_NUM`</a> config would further reduce the CPU utilization without significantly degrading the overall performance.</li>
<li>Also, if you are still using OpenVINO version earlier than R1 2019, or if you have recompiled the Inference Engine with OpemMP (say for backward compatibility), setting the <code>KMP_BLOCKTIME</code> environment variable to something less than default 200ms (we suggest 1ms) is particularly helpful. Use <code>KMP_BLOCKTIME=0</code> if the CPU subgraph is small.</li>
</ul>
<blockquote class="doxtable">
<p><b>NOTE</b>: General threading tips (see <a href="#note-on-app-level-threading">Note on the App-Level Threading</a>) apply well, even when the entire topology fits the FPGA, because there is still a host-side code for data pre- and post-processing. </p>
</blockquote>
<h3>General Tips on GPU/CPU Execution <a class="anchor" id="tips-on-gpu-cpu-execution"></a></h3>
<p>The following tips are provided to give general guidance on optimizing execution on GPU/CPU devices.</p>
<ul>
<li>Generally, GPU performance is better on heavy kernels (like Convolutions) and large inputs. So if the network inference time is already too small (~1ms of execution time), using the GPU would unlikely give a boost.</li>
<li>A typical strategy to start with is to test the CPU-only and GPU-only scenarios first (with samples this is plain <code>-d CPU</code> or <code>-d GPU</code>). If there are specific kernels that are not supported by the GPU, the best option to try is the <code>HETERO:GPU,CPU</code> that automatically applies default splitting (based on the plugins layers support). Then, you can play with the manual affinity settings (for example, to further minimize the number of subgraphs).</li>
<li>The general affinity “rule of thumb” is to keep computationally-intensive kernels on the accelerator, and "glue" (or helper) kernels on the CPU. Notice that this includes the granularity considerations. For example, running some (custom) activation on the CPU would result in too many conversions.</li>
<li>It is advised to do <a href="#analyzing-hetero-execution">performance analysis</a> to determine “hotspot” kernels, which should be the first candidates for offloading. At the same time, it is often more efficient to offload some reasonably sized sequence of kernels, rather than individual kernels, to minimize scheduling and other run-time overheads.</li>
<li>Notice that GPU can be busy with other tasks (like rendering). Similarly, the CPU can be in charge for the general OS routines and other application threads (see <a href="#note-on-app-level-threading">Note on the App-Level Threading</a>). Also, a high interrupt rate due to many subgraphs can raise the frequency of the one device and drag the frequency of another down.</li>
<li>Device performance can be affected by dynamic frequency scaling. For example, running long kernels on both devices simultaneously might eventually result in one or both devices stopping use of the Intel&reg; Turbo Boost Technology. This might result in overall performance decrease, even comparing to single-device scenario.</li>
<li>Mixing the <code>FP16</code> (GPU) and <code>FP32</code> (CPU) execution results in conversions and, thus, performance issues. If you are seeing a lot of heavy outstanding (compared to the CPU-only execution) Reorders, consider implementing actual GPU kernels. Refer to <a href="#performance-counters">Internal Inference Performance Counters</a> for more information.</li>
</ul>
<h3>Analyzing Heterogeneous Execution <a class="anchor" id="analyzing-heterogeneous-execution"></a></h3>
<p>There is a dedicated configuration option that enables dumping the visualization of the subgraphs created by the heterogeneous plugin:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;<a class="code" href="ie__plugin__config_8hpp.html">ie_plugin_config.hpp</a>&quot;</span></div><div class="line"><span class="preprocessor">#include &quot;<a class="code" href="hetero__plugin__config_8hpp.html">hetero/hetero_plugin_config.hpp</a>&quot;</span></div><div class="line"><span class="keyword">using namespace </span><a class="code" href="namespaceInferenceEngine_1_1PluginConfigParams.html">InferenceEngine::PluginConfigParams</a>;</div><div class="line"><span class="keyword">using namespace </span><a class="code" href="namespaceInferenceEngine_1_1HeteroConfigParams.html">InferenceEngine::HeteroConfigParams</a>;</div><div class="line"></div><div class="line">...</div><div class="line">enginePtr = dispatcher.getPluginByDevice(<span class="stringliteral">&quot;HETERO:FPGA,CPU&quot;</span>);</div><div class="line">InferencePlugin plugin(enginePtr);</div><div class="line">plugin.<a class="code" href="classInferenceEngine_1_1InferencePlugin.html#a3b07e64cd84d41bf8e0bf334168d2873">SetConfig</a>({ {KEY_HETERO_DUMP_GRAPH_DOT, YES} });</div></div><!-- fragment --><p>After enabling the configuration key, the heterogeneous plugin generates two files:</p>
<ul>
<li><code>hetero_affinity.dot</code> - per-layer affinities. This file is generated only if default fallback policy was executed (as otherwise you have set the affinities by yourself, so you know them).</li>
<li><code>hetero_subgraphs.dot</code> - affinities per sub-graph. This file is written to the disk during execution of <code>ICNNNetwork::LoadNetwork</code> for the heterogeneous plugin.</li>
</ul>
<p>You can use GraphViz* utility or <code>*.dot</code> converters (for example, to <code>.png</code> or <code>.pdf</code>), like xdot*, available on Linux* OS with <code>sudo apt-get install xdot</code>. Below is an example of the output trimmed to the two last layers (one executed on the FPGA and another on the CPU):</p>
<div class="image">
<img src="output_trimmed.png" alt="output_trimmed.png"/>
</div>
<p>You can also use performance data (in samples, it is an option <code>-pc</code>) to get performance data on each subgraph. Refer to <a href="#performance-counters">Internal Inference Performance Counters</a> for more information.</p>
<h2>Optimizing Custom Kernels <a class="anchor" id="optimizing-custom-kernels"></a></h2>
<h3>Few Initial Performance Considerations <a class="anchor" id="initial-performance-considerations"></a></h3>
<p>Today, the Inference Engine supports only CPU and GPU custom kernels. Typically, custom kernels are used to quickly implement missing layers for new topologies. You should not override standard layers implementation, especially on the critical path, for example, Convolutions. Also, overriding existing layers can disable some existing performance optimizations, such as fusing.</p>
<p>It is usually easier to start with the CPU extension and switch to the GPU after debugging with the CPU path. Sometimes, when the custom layers are at the very end of your pipeline, it is easier to implement them as regular post-processing in your application without wrapping them as kernels. This is particularly true for the kernels that do not fit the GPU well, for example, output bounding boxes sorting. In many cases, you can do such post-processing on the CPU.</p>
<p>There are many cases when sequence of the custom kernels can be implemented as a “super” kernel allowing to save on data accesses.</p>
<p>Finally, with the heterogeneous execution, it is possible to execute the vast majority of intensive computations with the accelerator and keep the custom pieces on the CPU. The tradeoff is granularity/costs of communication between different devices.</p>
<p>For more details on the API of the custom layers, see <a class="el" href="_docs_IE_DG_Integrate_your_kernels_into_IE.html">Custom Layers Support in Inference Engine</a></p>
<h3>Understanding Performance Contribution of Your Custom Kernels <a class="anchor" id="performance-contribution-of-custom-kernels"></a></h3>
<p>In most cases, before actually implementing a full-blown code for the kernel, you can estimate the final performance by doing a simple stub kernel that does nothing (and thus is "infinitely" fast) just to let the topology execute end-to-end. Of course, the estimation is valid only if the kernel output does not affect the performance, for instance, if its output is not driving any branches or loops.</p>
<p>Other than that, when implementing the kernels, you can try the methods from the previous chapter to understand actual contribution and, if any custom kernel is in the hotspots, optimize that.</p>
<h3>Few Device-Specific Tips <a class="anchor" id="device-specific-tips"></a></h3>
<ul>
<li>As already outlined in the <a href="#cpu-checklist">CPU Checklist</a>, align the threading model that you use in your CPU kernels with the model that the rest of the Inference Engine compiled with.</li>
<li>For CPU extensions, consider kernel flavor that supports blocked layout, if your kernel is in the hotspots (see <a href="#performance-counters">Internal Inference Performance Counters</a>). Since Intel MKL-DNN internally operates on the blocked layouts, this would save you a data packing (Reorder) on tensor inputs/outputs of your kernel. For example of the blocked layout support, please, refer to the extensions in the <code>&lt;OPENVINO_INSTALL_DIR&gt;/deployment_tools/samples/extension/</code>.</li>
</ul>
<h2>Plugging Inference Engine to Applications <a class="anchor" id="plugging-ie-to-applications"></a></h2>
<h3>Note on the App-Level Threading <a class="anchor" id="note-on-app-level-threading"></a></h3>
<ul>
<li>As explained in the <a href="#cpu-checklist">CPU Checklist</a> section, by default the Inference Engine uses Intel TBB as a parallel engine. Thus, any OpenVINO-internal threading (including CPU inference) uses the same threads pool, provided by the TBB. But there are also other threads in your application, so oversubscription is possible at the application level:</li>
<li>The rule of thumb is that you should try to have the overall number of active threads in your application equal to the number of cores in your machine. Keep in mind the spare core(s) that the OpenCL driver under the GPU plugin might also need.</li>
<li>One specific workaround to limit the number of threads for the Inference Engine is using the <a class="el" href="_docs_IE_DG_supported_plugins_CPU.html">CPU configuration options</a>.</li>
<li>To avoid further oversubscription, use the same threading model in all modules/libraries that your application uses. Notice that third party components might bring their own threading. For example, using Inference Engine which is now compiled with the TBB by default might lead to <a href="https://www.threadingbuildingblocks.org/docs/help/reference/appendices/known_issues/interoperability.html">performance troubles</a> when mixed in the same app with another computationally-intensive library, but compiled with OpenMP. You can try to compile the <a href="https://github.com/opencv/dldt">open source version</a> of the Inference Engine to use the OpenMP as well. But notice that in general, the TBB offers much better composability, than other threading solutions.</li>
<li>If your code (or third party libraries) uses GNU OpenMP, the Intel&reg; OpenMP (if you have recompiled Inference Engine with that) must be initialized first. This can be achieved by linking your application with the Intel OpenMP instead of GNU OpenMP, or using <code>LD_PRELOAD</code> on Linux* OS.</li>
</ul>
<h3>Letting the Inference Engine Accelerate Image Pre-processing/Conversion <a class="anchor" id="image-preprocessing"></a></h3>
<p>In many cases, a network expects a pre-processed image, so make sure you do not perform unnecessary steps in your code:</p>
<ul>
<li>Model Optimizer can efficiently bake the mean and normalization (scale) values into the model (for example, weights of the first convolution). See <a href="#mo-knobs-related-to-performance">Model Optimizer Knobs Related to Performance</a>.</li>
<li>If regular 8-bit per channel images are your native media (for instance, decoded frames), do not convert to the <code>FP32</code> on your side, as this is something that plugins can accelerate. Use the <code><a class="el" href="classInferenceEngine_1_1Precision.html#ade75bd7073b4aa966c0dda4025bcd0f5a046eaf31a4345f526ed54271c9fcd39c">InferenceEngine::Precision::U8</a></code> as your input format:<br />
 <div class="fragment"><div class="line"><a class="code" href="namespaceInferenceEngine.html#a08270747275eb79985154365aa782a2a">InferenceEngine::InputsDataMap</a> info(netReader.getNetwork().getInputsInfo());</div><div class="line"><span class="keyword">auto</span>&amp; inputInfoFirst = info.begin()-&gt;second;</div><div class="line">info-&gt;setInputPrecision(Precision::U8);</div></div><!-- fragment --></li>
</ul>
<p>Notice that in many cases, you can directly share the (input) data with the Inference Engine.</p>
<h3>Basic Interoperability with Other APIs <a class="anchor" id="basic-interoperability-with-other-apis"></a></h3>
<p>The general approach for sharing data between Inference Engine and media/graphics APIs like Intel&reg; Media Server Studio (Intel&reg; MSS) is based on sharing the <em>system</em> memory. That is, in your code, you should map or copy the data from the API to the CPU address space first.</p>
<p>For Intel MSS, it is recommended to perform a viable pre-processing, for example, crop/resize, and then convert to RGB again with the <a href="https://software.intel.com/en-us/node/696108">Video Processing Procedures (VPP)</a>. Then lock the result and create an Inference Engine blob on top of that. The resulting pointer can be used for the <code>SetBlob</code>:</p>
<div class="fragment"><div class="line"><span class="comment">//Lock Intel MSS surface  </span></div><div class="line">mfxFrameSurface1 *frame_in;   <span class="comment">//Input MSS surface.</span></div><div class="line">mfxFrameAllocator* pAlloc = &amp;m_mfxCore.FrameAllocator();    </div><div class="line">pAlloc-&gt;Lock(pAlloc-&gt;pthis, frame_in-&gt;Data.MemId, &amp;frame_in-&gt;Data);</div><div class="line"><span class="comment">//Inference Engine code</span></div></div><!-- fragment --><p><b>WARNING</b>: The <code>InferenceEngine::NHWC</code> layout is not supported natively by most <a class="el" href="namespaceInferenceEngine.html" title="Inference Engine API. ">InferenceEngine</a> plugins so internal conversion might happen.</p>
<div class="fragment"><div class="line"><a class="code" href="namespaceInferenceEngine.html#a9400de686d3d0f48c30cd73d40e48576">InferenceEngine::SizeVector</a> dims_src = {</div><div class="line">    1       <span class="comment">/* batch, N*/</span>,</div><div class="line">    (size_t) frame_in-&gt;Info.Height  <span class="comment">/* Height */</span>,</div><div class="line">    (<span class="keywordtype">size_t</span>) frame_in-&gt;Info.Width    <span class="comment">/* Width */</span>,</div><div class="line">    3 <span class="comment">/*Channels,*/</span>,</div><div class="line">    };</div><div class="line">TensorDesc desc(<a class="code" href="classInferenceEngine_1_1Precision.html#ade75bd7073b4aa966c0dda4025bcd0f5a046eaf31a4345f526ed54271c9fcd39c">InferenceEngine::Precision::U8</a>, dims_src, InferenceEngine::NHWC);</div><div class="line"><span class="comment">/* wrapping the surface data, as RGB is interleaved, need to pass only ptr to the R, notice that this wouldn’t work with planar formats as these are 3 separate planes/pointers*/</span></div><div class="line"><a class="code" href="classInferenceEngine_1_1TBlob.html#a6af98afe8c25be14b916348df0712a09">InferenceEngine::TBlob&lt;uint8_t&gt;::Ptr</a> p = InferenceEngine::make_shared_blob&lt;uint8_t&gt;( desc, (uint8_t*) frame_in-&gt;Data.R);</div><div class="line">inferRequest.SetBlob(“input”, p);</div><div class="line">inferRequest.Infer();</div><div class="line"><span class="comment">//Make sure to unlock the surface upon inference completion, to return the ownership back to the Intel MSS</span></div><div class="line">pAlloc-&gt;Unlock(pAlloc-&gt;pthis, frame_in-&gt;Data.MemId, &amp;frame_in-&gt;Data);</div></div><!-- fragment --><p>Alternatively, you can use RGBP (planar RGB) output from Intel MSS. This allows to wrap the (locked) result as regular NCHW which is generally friendly for most plugins (unlike NHWC). Then you can use it with <code>SetBlob</code> just like in previous example:</p>
<div class="fragment"><div class="line"><a class="code" href="namespaceInferenceEngine.html#a9400de686d3d0f48c30cd73d40e48576">InferenceEngine::SizeVector</a> dims_src = {</div><div class="line">       1        <span class="comment">/* batch, N*/</span>,</div><div class="line">       3        <span class="comment">/*Channels,*/</span>,</div><div class="line">       (size_t) frame_in-&gt;Info.Height  <span class="comment">/* Height */</span>,</div><div class="line">       (<span class="keywordtype">size_t</span>) frame_in-&gt;Info.Width    <span class="comment">/* Width */</span>,</div><div class="line">       };</div><div class="line">TensorDesc desc(<a class="code" href="classInferenceEngine_1_1Precision.html#ade75bd7073b4aa966c0dda4025bcd0f5a046eaf31a4345f526ed54271c9fcd39c">InferenceEngine::Precision::U8</a>, dims_src, InferenceEngine::NCHW);</div><div class="line"><span class="comment">/* wrapping the RGBP surface data*/</span></div><div class="line"><a class="code" href="classInferenceEngine_1_1TBlob.html#a6af98afe8c25be14b916348df0712a09">InferenceEngine::TBlob&lt;uint8_t&gt;::Ptr</a> p = InferenceEngine::make_shared_blob&lt;uint8_t&gt;( desc, (uint8_t*) frame_in-&gt;Data.R);</div><div class="line">inferRequest.SetBlob(<span class="stringliteral">&quot;input&quot;</span>, p);</div><div class="line">…</div></div><!-- fragment --><p>The only downside of this approach is that VPP conversion to RGBP is not hardware accelerated (and performed on the GPU EUs). Also, it is available only on LInux.</p>
<h3>OpenCV* Interoperability Example <a class="anchor" id="opencv-interoperability"></a></h3>
<p>Unlike APIs that use dedicated address space and/or special data layouts (for instance, compressed OpenGL* textures), regular OpenCV data objects like <code>cv::Mat</code> reside in the conventional system memory. That is, the memory can be actually shared with the Inference Engine and only data ownership to be transferred.</p>
<p>Again, if the OpenCV and Inference Engine layouts match, the data can be wrapped as Inference Engine (input/output) blob. Notice that by default, Inference Engine accepts the <b>planar</b> and <b>not interleaved</b> inputs in NCHW, so the NHWC (which is exactly the interleaved layout) should be specified explicitly:</p>
<p><b>WARNING</b>: The <code>InferenceEngine::NHWC</code> layout is not supported natively by most <a class="el" href="namespaceInferenceEngine.html" title="Inference Engine API. ">InferenceEngine</a> plugins so internal conversion might happen.</p>
<div class="fragment"><div class="line">cv::Mat frame = ...;  <span class="comment">// regular CV_8UC3 image, interleaved</span></div><div class="line"><span class="comment">// creating blob that wraps the OpenCV’s Mat</span></div><div class="line"><span class="comment">// (the data it points should persists until the blob is released):</span></div><div class="line"><a class="code" href="namespaceInferenceEngine.html#a9400de686d3d0f48c30cd73d40e48576">InferenceEngine::SizeVector</a> dims_src = {</div><div class="line">    1       <span class="comment">/* batch, N*/</span>,</div><div class="line">    (size_t)frame.rows  <span class="comment">/* Height */</span>,</div><div class="line">    (<span class="keywordtype">size_t</span>)frame.cols    <span class="comment">/* Width */</span>,</div><div class="line">    (size_t)frame.channels() <span class="comment">/*Channels,*/</span>,</div><div class="line">    };</div><div class="line">TensorDesc desc(<a class="code" href="classInferenceEngine_1_1Precision.html#ade75bd7073b4aa966c0dda4025bcd0f5a046eaf31a4345f526ed54271c9fcd39c">InferenceEngine::Precision::U8</a>, dims_src, InferenceEngine::NHWC);</div><div class="line"><a class="code" href="classInferenceEngine_1_1TBlob.html#a6af98afe8c25be14b916348df0712a09">InferenceEngine::TBlob&lt;uint8_t&gt;::Ptr</a> p = InferenceEngine::make_shared_blob&lt;uint8_t&gt;( desc, (uint8_t*)frame.data, frame.step[0] * frame.rows);</div><div class="line">inferRequest.SetBlob(“input”, p);</div><div class="line">inferRequest.Infer();</div><div class="line">…</div><div class="line"><span class="comment">// similarly, you can wrap the output tensor (let’s assume it is FP32)</span></div><div class="line"><span class="comment">// notice that the output should be also explicitly stated as NHWC with setLayout</span></div><div class="line"><span class="keyword">const</span> <span class="keywordtype">float</span>* output_data = output_blob-&gt;buffer().</div><div class="line">        as&lt;PrecisionTrait&lt;Precision::FP32&gt;::value_type*&gt;();</div><div class="line">cv::Mat res (rows, cols, CV_32FC3, output_data, CV_AUTOSTEP);</div></div><!-- fragment --><p>Notice that original <code>cv::Mat</code>/blobs cannot be used simultaneously by the application and the Inference Engine. Alternatively, the data that the pointer references to can be copied to unlock the original data and return ownership to the original API.</p>
<h3>Request-Based API and “GetBlob” Idiom <a class="anchor" id="new-request-based-api"></a></h3>
<p>Infer Request based API offers two types of request: Sync and Async. The Sync is considered below. The Async splits (synchronous) <code>Infer</code> into <code>StartAsync</code> and <code>Wait</code> (see <a href="#ie-async-api">Inference Engine Async API</a>).</p>
<p>More importantly, an infer request encapsulates the reference to the “executable” network and actual inputs/outputs. Now, when you load the network to the plugin, you get a reference to the executable network (you may consider that as a queue). Actual infer requests are created by the executable network:</p>
<div class="fragment"><div class="line">CNNNetReader network_reader;</div><div class="line">network_reader.ReadNetwork(<span class="stringliteral">&quot;Model.xml&quot;</span>);</div><div class="line">network_reader.ReadWeights(<span class="stringliteral">&quot;Model.bin&quot;</span>);</div><div class="line"><span class="keyword">auto</span> network = network_reader.getNetwork();</div><div class="line"><a class="code" href="namespaceInferenceEngine.html#a08270747275eb79985154365aa782a2a">InferenceEngine::InputsDataMap</a> input_info(network.getInputsInfo());</div><div class="line"></div><div class="line"><a class="code" href="namespaceInferenceEngine.html#a31b044b371eef8faf8751c9d9d26bd7c">InferenceEnginePluginPtr</a> engine_ptr = PluginDispatcher(pluginDirs).getSuitablePlugin(TargetDevice::eGPU);</div><div class="line">InferencePlugin plugin(engine_ptr);</div><div class="line"></div><div class="line"><span class="keyword">auto</span> executable_network = plugin.<a class="code" href="classInferenceEngine_1_1InferencePlugin.html#a0ca00d832aa35ecdefdfb456b62e51d4">LoadNetwork</a>(network, {<span class="comment">/*opt config*/</span>});</div><div class="line"><span class="keyword">auto</span> infer_request = executable_network.CreateInferRequest();</div><div class="line"></div><div class="line"><span class="keywordflow">for</span> (<span class="keyword">auto</span> &amp; item : inputInfo) {</div><div class="line">    std::string input_name = item-&gt;first;</div><div class="line">    <span class="keyword">auto</span> input = infer_request.GetBlob(input_name);<span class="comment"></span></div><div class="line"><span class="comment">    /** Lock/Fill input tensor with data **/</span></div><div class="line">           <span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>* data =</div><div class="line">    input-&gt;<a class="code" href="classInferenceEngine_1_1TBlob.html#a33b98d132e397ff24f1d307ea63fafb3">buffer</a>().as&lt;PrecisionTrait&lt;Precision::U8&gt;::value_type*&gt;();</div><div class="line">    ...</div><div class="line">}</div><div class="line"></div><div class="line">infer_request-&gt;Infer();</div></div><!-- fragment --><p><code>GetBlob</code> is a recommend way to communicate with the network, as it internally allocates the data with right padding/alignment for the device. For example, the GPU inputs/outputs blobs are mapped to the host (which is fast) if the <code>GetBlob</code> is used. But if you called the <code>SetBlob</code>, the copy (from/to the blob you have set) into the internal GPU plugin structures will happen.</p>
<h3>Performance Aspects of Running Multiple Requests Simultaneously <a class="anchor" id="running-multiple-requests-simultaneously"></a></h3>
<p>If your application simultaneously executes multiple infer requests:</p>
<ul>
<li>For the CPU, the best solution, you can use the <a href="#cpu-streams">CPU "throughput" mode</a>.<ul>
<li>If latency is of more concern, you can try the <code>EXCLUSIVE_ASYNC_REQUESTS</code> <a class="el" href="_docs_IE_DG_supported_plugins_CPU.html">configuration option</a> that limits the number of the simultaneously executed requests for all (executable) networks that share the specific device to just one:<br />
 ```cpp //these two networks go thru same plugin (aka device) and their requests will not overlap. auto executable_network0 = plugin.LoadNetwork(network0, {{PluginConfigParams::KEY_EXCLUSIVE_ASYNC_REQUESTS, PluginConfigParams::YES}}); auto executable_network1 = plugin.LoadNetwork(network1, {{PluginConfigParams::KEY_EXCLUSIVE_ASYNC_REQUESTS, PluginConfigParams::YES}}); ``` <br />
For more information on the executable networks notation, see <a href="#new-request-based-api">Request-Based API and “GetBlob” Idiom</a>.</li>
<li>The heterogeneous device uses the <code>EXCLUSIVE_ASYNC_REQUESTS</code> by default.</li>
<li><code>KEY_EXCLUSIVE_ASYNC_REQUESTS</code> option affects only device queues of the individual application.</li>
</ul>
</li>
<li>For FPGA and GPU, the actual work is serialized by a plugin and/or a driver anyway.</li>
<li>Finally, for <a href="#myriad">any VPU flavor</a>, using multiple requests is a must for achieving good throughput.</li>
</ul>
<p>In the Inference Engine, there is no notion of requests priorities. It is left to the user side (for example, not queuing the low priority infer request, until another higher priority is waiting). Notice that it would require additional logic to synchronize between executable networks (queues) in your application code.</p>
<h3>Inference Engine Async API <a class="anchor" id="ie-async-api"></a></h3>
<p>Inference Engine Async API can improve overall frame rate of the application. While accelerator is busy with the inference, the application can continue doing things on the host rather than wait for the inference to complete.</p>
<p>In the example below, inference is applied to the results of the video decoding. So it is possible to keep two parallel infer requests, and while the current is processed, the input frame for the next is being captured. This essentially hides the latency of capturing, so that the overall frame rate is rather determined only by the slowest part of the pipeline (decoding IR inference) and not by the sum of the stages.</p>
<p>You can compare the pseudo-codes for the regular and async-based approaches:</p>
<ul>
<li>In the regular way, the frame is captured with OpenCV and then immediately processed:<br />
 <div class="fragment"><div class="line"><span class="keywordflow">while</span>(…) {</div><div class="line">    capture frame</div><div class="line">    populate CURRENT InferRequest</div><div class="line">    Infer CURRENT InferRequest <span class="comment">//this call is synchronous</span></div><div class="line">    display CURRENT result</div><div class="line">}</div></div><!-- fragment --> <div class="image">
<img src="vtune_regular.png" alt="vtune_regular.png"/>
<div class="caption">
Intel&reg; VTune&trade; screenshot</div></div>
</li>
<li>In the "true" async mode, the <code>NEXT</code> request is populated in the main (application) thread, while the <code>CURRENT</code> request is processed:<br />
 <div class="fragment"><div class="line"><span class="keywordflow">while</span>(…) {</div><div class="line">    capture frame</div><div class="line">    populate NEXT InferRequest</div><div class="line">    start NEXT InferRequest <span class="comment">//this call is async and returns immediately</span></div><div class="line">    wait <span class="keywordflow">for</span> the CURRENT InferRequest <span class="comment">//processed in a dedicated thread</span></div><div class="line">    display CURRENT result</div><div class="line">    swap CURRENT and NEXT InferRequests</div><div class="line">}</div></div><!-- fragment --> <div class="image">
<img src="vtune_async.png" alt="vtune_async.png"/>
<div class="caption">
Intel&reg; VTune&trade; screenshot</div></div>
 The technique can be generalized to any available parallel slack. For example, you can do inference and simultaneously encode the resulting or previous frames or run further inference, like emotion detection on top of the face detection results.</li>
</ul>
<p>There are important performance caveats though: for example, the tasks that run in parallel should try to avoid oversubscribing the shared compute resources. If the inference is performed on the FPGA and the CPU is essentially idle, it makes sense to do things on the CPU in parallel. However, multiple infer requests can oversubscribe that. Notice that heterogeneous execution can implicitly use the CPU, refer to <a href="#heterogeneity">Heterogeneity</a>.</p>
<p>Also, if the inference is performed on the graphics processing unit (GPU), it can take little gain to do the encoding, for instance, of the resulting video, on the same GPU in parallel, because the device is already busy.</p>
<p>Refer to the <a class="el" href="_demos_object_detection_demo_ssd_async_README.html">Object Detection SSD Demo</a> (latency-oriented Async API showcase) and <a class="el" href="_inference_engine_samples_benchmark_app_README.html">Benchmark App Sample</a> (which has both latency and throughput-oriented modes) for complete examples of the Async API in action.</p>
<h2>Using Tools <a class="anchor" id="using-tools"></a></h2>
<p>Whether you are tuning for the first time or doing advanced performance optimization, you need a a tool that provides accurate insights. Intel&reg; VTune&trade; Amplifier gives you the tool to mine it and interpret the profiling data.</p>
<p>Alternatively, you can gather the raw profiling data that samples report, the second chapter provides example of how to interpret these.</p>
<h3>Intel&reg; VTune&trade; Examples <a class="anchor" id="vtune-examples"></a></h3>
<p>All major performance calls of the Inference Engine are instrumented with Instrumentation and Tracing Technology APIs. This allows viewing the Inference Engine calls on the Intel&reg; VTune&trade; timelines and aggregations plus correlating them to the underlying APIs, like OpenCL. In turn, this enables careful per-layer execution breakdown.</p>
<p>When choosing the Analysis type in Intel&reg; VTune&trade; Amplifier, make sure to select the <b>Analyze user tasks, events, and counters</b> option:</p>
<div class="image">
<img src="vtune_option.jpg" alt="vtune_option.jpg"/>
</div>
<p>See the <a href="https://software.intel.com/en-us/vtune-amplifier-help-task-analysis">corresponding section in the Intel® VTune™ Amplifier User's Guide</a> for details.</p>
<p>Example of Inference Engine calls:</p>
<ul>
<li><p class="startli">On the Intel VTune Amplifier timeline. Notice that <code>Task_runNOThrow</code> is an Async API wrapper and it is executed in a different thread and triggers the Intel MKL-DNN execution:</p>
<div class="image">
<img src="vtune_timeline.png" alt="vtune_timeline.png"/>
</div>
</li>
<li><p class="startli">In the Intel VTune Amplifier <b>Top-down view</b>, grouped by the <b>Task Domain</b>. Notice the <code>Task_runNoThrow</code> and <code>MKLDNN _INFER</code> that are bracketing the actual Intel MKL-DNN kernels execution:</p>
<div class="image">
<img src="vtune_topdown_view.jpg" alt="vtune_topdown_view.jpg"/>
</div>
</li>
</ul>
<p>Similarly, you can use any GPU analysis in the Intel VTune Amplifier and get general correlation with Inference Engine API as well as the execution breakdown for OpenCL kernels.</p>
<p>Just like with regular native application, further drill down in the counters is possible, however, this is mostly useful for <a href="#optimizing-custom-kernels">optimizing custom kernels</a>. Finally, with the Intel VTune Amplifier, the profiling is not limited to your user-level code (see the <a href="https://software.intel.com/en-us/vtune-amplifier-help-analyze-performance">corresponding section in the Intel&reg; VTune&trade; Amplifier User's Guide</a>).</p>
<h3>Internal Inference Performance Counters <a class="anchor" id="performance-counters"></a></h3>
<p>Almost every sample (inspect command-line options for a specific sample with <code>-h</code>) supports a <code>-pc</code> command that outputs internal execution breakdown. Refer to the <a class="el" href="_docs_IE_DG_Samples_Overview.html">samples code</a> for the actual Inference Engine API behind that.</p>
<p>Below is example of CPU plugin output for a network (since the device is CPU, the layers wall clock <code>realTime</code> and the <code>cpu</code> time are the same):</p>
<div class="fragment"><div class="line">conv1      EXECUTED       layerType: Convolution        realTime: 706        cpu: 706            execType: jit_avx2</div><div class="line">conv2_1_x1  EXECUTED       layerType: Convolution        realTime: 137        cpu: 137            execType: jit_avx2_1x1</div><div class="line">fc6        EXECUTED       layerType: Convolution        realTime: 233        cpu: 233            execType: jit_avx2_1x1</div><div class="line">fc6_nChw8c_nchw      EXECUTED  layerType: Reorder           realTime: 20         cpu: 20             execType: reorder</div><div class="line">out_fc6         EXECUTED       layerType: Output            realTime: 3          cpu: 3              execType: unknown</div><div class="line">relu5_9_x2    OPTIMIZED_OUT     layerType: ReLU             realTime: 0          cpu: 0              execType: undef</div></div><!-- fragment --><p>This contains layers name (as seen in IR), layers type and execution statistics. Notice the <code>OPTIMIZED_OUT</code>, which indicates that the particular activation was fused into adjacent convolution. Also, the <code>unknown</code> stays for the Inference Engine specific CPU (helper) primitives that are not part of the Intel MKL-DNN.</p>
<p>Notice that there are some helper layers in the CPU execution breakdown, which were not presented in the original topology. These are automatically added by the plugin. For example, the <code>Reorder</code> re-packs the Intel MKL-DNN internal (blocked) layout to the regular plain NCHW (that the user expects as the output). As explained in the <a href="#device-specific-tips">Few Device-Specific Tips</a>, if your custom kernels introduces a lot of outstanding/expensive Reorders, consider blocked implementation for the kernels.</p>
<p>Notice that in the heterogeneous cases, there will be additional information on which subgraph the statistics is about (the first subgraph is GPU, so its <code>cpu</code>/host time is really small compared to the actual <code>realTime</code>):</p>
<div class="fragment"><div class="line">subgraph1: squeeze1x1         EXECUTED       layerType: Convolution        realTime: 227    cpu:3    execType: GPU</div><div class="line">…</div><div class="line">subgraph2: detection_out      EXECUTED       layerType: DetectionOutput    realTime: 121 cpu:121  execType: unknown</div><div class="line">…</div></div><!-- fragment --><p>As mentioned earlier, <code>unknown</code> here means CPU kernel with unknown (for example, not AVX2 or AVX512) acceleration path. Since FPGA execution does not separate individual kernels, only bulk execution/data transfer statistics is available:</p>
<div class="fragment"><div class="line">subgraph1: 1. input preprocessing (mean data/FPGA):EXECUTED   layerType: preprocessing   realTime: 129     cpu: 129</div><div class="line">subgraph1: 2. input transfer to DDR:EXECUTED       layerType:                    realTime: 201        cpu: 0              </div><div class="line">subgraph1: 3. FPGA execute time:EXECUTED           layerType:                    realTime: 3808       cpu: 0              subgraph1: 4. output transfer from DDR:EXECUTED    layerType:                    realTime: 55         cpu: 0              </div><div class="line">subgraph1: 5. FPGA output postprocessing:EXECUTED  layerType:                    realTime: 7          cpu: 7              </div><div class="line">subgraph1: 6. softmax/copy:   EXECUTED       layerType:                    realTime: 2          cpu: 2              </div><div class="line">subgraph2: out_prob:          NOT_RUN        layerType: Output             realTime: 0          cpu: 0              </div><div class="line">subgraph2: prob:              EXECUTED       layerType: SoftMax            realTime: 10         cpu: 10             </div><div class="line">Total time: 4212     microseconds</div></div><!-- fragment --><p>The <code>softmax/copy</code> is a glue layer that connects the FPGA subgraph to the CPU subgraph (and copies the data).</p>
<h2>See Also <a class="anchor" id="see-also"></a></h2>
<ul>
<li><a href="https://software.intel.com/en-us/articles/OpenVINO-inferengine">Inference Engine Developer Guide</a></li>
<li><a href="https://software.intel.com/en-us/articles/OpenVINO-ModelOptimizer">Model Optimizer Developer Guide</a> </li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>