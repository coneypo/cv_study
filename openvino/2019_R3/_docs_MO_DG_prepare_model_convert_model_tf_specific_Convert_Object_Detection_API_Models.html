<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Converting TensorFlow* Object Detection API Models - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Converting TensorFlow* Object Detection API Models </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><blockquote class="doxtable">
<p><b>NOTES</b>:</p>
<ul>
<li>Starting with the 2019 R1 release, the Model Optimizer supports the <code>--keep_shape_ops</code> command line parameter that allows you to convert the TensorFlow* Object Detection API Faster and Mask RCNNs topologies so they can be re-shaped in the Inference Engine using dedicated reshape API. Refer to <a class="el" href="_docs_IE_DG_ShapeInference.html">Using Shape Inference</a> for more information on how to use this feature. It is possible to change the both spatial dimensions of the input image and batch size.</li>
<li>Starting with the 2018 R4 release, the Model Optimizer supports the <code>--input_shape</code> command line parameter for the TensorFlow* Object Detection API topologies. Refer to the <a href="#tf_od_custom_input_shape">Custom Input Shape</a> for more information.</li>
<li>To generate IRs for SSD topologies, the Model Optimizer creates a number of <code>PriorBoxClustered</code> layers instead of a constant node with prior boxes calculated for the particular input image size. This change allows you to reshape the topology in the Inference Engine using dedicated Inference Engine API. The reshaping is supported for all SSD topologies except FPNs which contain hardcoded shapes for some operations preventing from changing topology input shape. </li>
</ul>
</blockquote>
<h2>How to Convert a Model</h2>
<p>With 2018 R3 release, the Model Optimizer introduces a new approach to convert models created using the TensorFlow* Object Detection API. Compared with the previous approach, the new process produces inference results with higher accuracy and does not require modifying any configuration files and providing intricate command line parameters.</p>
<p>You can download TensorFlow* Object Detection API models from the <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md">Object Detection Model Zoo</a>.</p>
<p><b>NOTE</b>: Before converting, make sure you have configured the Model Optimizer. For configuration steps, refer to <a class="el" href="_docs_MO_DG_prepare_model_Config_Model_Optimizer.html">Configuring the Model Optimizer</a>.</p>
<p>To convert a TensorFlow* Object Detection API model, go to the <code>&lt;INSTALL_DIR&gt;/deployment_tools/model_optimizer</code> directory and run the <code>mo_tf.py</code> script with the following required parameters:</p>
<ul>
<li><code>--input_model &lt;path_to_frozen.pb&gt;</code> &mdash; File with a pre-trained model (binary or text .pb file after freezing)</li>
<li><code>--tensorflow_use_custom_operations_config &lt;path_to_subgraph_replacement_configuration_file.json&gt;</code> &mdash; A subgraph replacement configuration file that describes rules to convert specific TensorFlow* topologies. For the models downloaded from the TensorFlow* Object Detection API zoo, you can find the configuration files in the <code>&lt;INSTALL_DIR&gt;/deployment_tools/model_optimizer/extensions/front/tf</code> directory. Use:<ul>
<li><code>ssd_v2_support.json</code> &mdash; for frozen SSD topologies from the models zoo</li>
<li><code>faster_rcnn_support.json</code> &mdash; for frozen Faster R-CNN topologies from the models zoo</li>
<li><code>faster_rcnn_support_api_v1.7.json</code> &mdash; for Faster R-CNN topologies trained manually using the TensorFlow* Object Detection API version 1.7.0 or higher</li>
<li><code>faster_rcnn_support_api_v1.10.json</code> &mdash; for Faster R-CNN topologies trained manually using the TensorFlow* Object Detection API version 1.7.0 or higher</li>
<li><code>mask_rcnn_support.json</code> &mdash; for frozen Mask R-CNN topologies from the models zoo</li>
<li><code>mask_rcnn_support_api_v1.7.json</code> &mdash; for Mask R-CNN topologies trained manually using the TensorFlow* Object Detection API version 1.7.0 or higher up to 1.9.0 inclusively</li>
<li><code>mask_rcnn_support_api_v1.11.json</code> &mdash; for Mask R-CNN topologies trained manually using the TensorFlow* Object Detection API version 1.10.0 or higher</li>
<li><code>rfcn_support.json</code> &mdash; for the frozen RFCN topology from the models zoo frozen with TensorFlow* version 1.9.0 or lower.</li>
<li><code>rfcn_support_api_v1.10.json</code> &mdash; for the frozen RFCN topology from the models zoo frozen with TensorFlow* version 1.10.0 or higher.</li>
</ul>
</li>
<li><code>--tensorflow_object_detection_api_pipeline_config &lt;path_to_pipeline.config&gt;</code> &mdash; A special configuration file that describes the topology hyper-parameters and structure of the TensorFlow Object Detection API model. For the models downloaded from the TensorFlow* Object Detection API zoo, the configuration file is named <code>pipeline.config</code>. If you plan to train a model yourself, you can find templates for these files in the <a href="https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs">models repository</a>.</li>
<li><code>--input_shape</code> (optional) &mdash; A custom input image shape. Refer to <a href="#tf_od_custom_input_shape">Custom Input Shape</a> for more information how the <code>--input_shape</code> parameter is handled for the TensorFlow* Object Detection API models.</li>
</ul>
<blockquote class="doxtable">
<p><b>NOTE</b>: If you convert a TensorFlow* Object Detection API model to use with the Inference Engine sample applications, you must specify the <code>--reverse_input_channels</code> parameter also. </p>
</blockquote>
<p>Additionally to the mandatory parameters listed above you can use optional conversion parameters if needed. A full list of parameters is available in the <a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html">Converting a TensorFlow* Model</a> topic.</p>
<p>For example, if you downloaded the <a href="http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz">pre-trained SSD InceptionV2 topology</a> and extracted archive to the directory <code>/tmp/ssd_inception_v2_coco_2018_01_28</code>, the sample command line to convert the model looks as follows:</p>
<div class="fragment"><div class="line">&lt;INSTALL_DIR&gt;/deployment_tools/model_optimizer/mo_tf.py --input_model=/tmp/ssd_inception_v2_coco_2018_01_28/frozen_inference_graph.pb --tensorflow_use_custom_operations_config &lt;INSTALL_DIR&gt;/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json --tensorflow_object_detection_api_pipeline_config /tmp/ssd_inception_v2_coco_2018_01_28/pipeline.config --reverse_input_channels</div></div><!-- fragment --><h2>Custom Input Shape <a class="anchor" id="tf_od_custom_input_shape"></a></h2>
<p>Model Optimizer handles command line parameter <code>--input_shape</code> for TensorFlow* Object Detection API models in a special way depending on the image resizer type defined in the <code>pipeline.config</code> file. TensorFlow* Object Detection API generates different <code>Preprocessor</code> sub-graph based on the image resizer type. Model Optimizer supports two types of image resizer:</p><ul>
<li><code>fixed_shape_resizer</code> &mdash; <em>Stretches</em> input image to the specific height and width. The <code>pipeline.config</code> snippet below shows a <code>fixed_shape_resizer</code> sample definition: <div class="fragment"><div class="line">image_resizer {</div><div class="line">  fixed_shape_resizer {</div><div class="line">    height: 300</div><div class="line">    width: 300</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --></li>
<li><code>keep_aspect_ratio_resizer</code> &mdash; Resizes the input image <em>keeping aspect ratio</em> to satisfy the minimum and maximum size constraints. The <code>pipeline.config</code> snippet below shows a <code>keep_aspect_ratio_resizer</code> sample definition: <div class="fragment"><div class="line">image_resizer {</div><div class="line">  keep_aspect_ratio_resizer {</div><div class="line">    min_dimension: 600</div><div class="line">    max_dimension: 1024</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --></li>
</ul>
<h3>Fixed Shape Resizer Replacement</h3>
<ul>
<li>If the <code>--input_shape</code> command line parameter is not specified, the Model Optimizer generates an input layer with the height and width as defined in the <code>pipeline.config</code>.</li>
<li>If the <code>--input_shape [1, H, W, 3]</code> command line parameter is specified, the Model Optimizer sets the input layer height to <code>H</code> and width to <code>W</code> and convert the model. However, the conversion may fail because of the following reasons:<ul>
<li>The model is not reshape-able, meaning that it's not possible to change the size of the model input image. For example, SSD FPN models have <code>Reshape</code> operations with hard-coded output shapes, but the input size to these <code>Reshape</code> instances depends on the input image size. In this case, the Model Optimizer shows an error during the shape inference phase. Run the Model Optimizer with <code>--log_level DEBUG</code> to see the inferred layers output shapes to see the mismatch.</li>
<li>Custom input shape is too small. For example, if you specify <code>--input_shape [1,100,100,3]</code> to convert a SSD Inception V2 model, one of convolution or pooling nodes decreases input tensor spatial dimensions to non-positive values. In this case, the Model Optimizer shows error message like this: '[ ERROR ] Shape [ 1 -1 -1 256] is not fully defined for output X of "node_name".'</li>
</ul>
</li>
</ul>
<h3>Keep Aspect Ratio Resizer Replacement</h3>
<ul>
<li>If the <code>--input_shape</code> command line parameter is not specified, the Model Optimizer generates an input layer with both height and width equal to the value of parameter <code>min_dimension</code> in the <code>keep_aspect_ratio_resizer</code>.</li>
<li>If the <code>--input_shape [1, H, W, 3]</code> command line parameter is specified, the Model Optimizer scales the specified input image height <code>H</code> and width <code>W</code> to satisfy the <code>min_dimension</code> and <code>max_dimension</code> constraints defined in the <code>keep_aspect_ratio_resizer</code>. The following function calculates the input layer height and width:</li>
</ul>
<div class="fragment"><div class="line">def calculate_shape_keeping_aspect_ratio(H: int, W: int, min_dimension: int, max_dimension: int):</div><div class="line">    ratio_min = min_dimension / min(H, W)</div><div class="line">    ratio_max = max_dimension / max(H, W)</div><div class="line">    ratio = min(ratio_min, ratio_max)</div><div class="line">    return int(round(H * ratio)), int(round(W * ratio))</div></div><!-- fragment --><p>Models with <code>keep_aspect_ratio_resizer</code> were trained to recognize object in real aspect ratio, in contrast with most of the classification topologies trained to recognize objects stretched vertically and horizontally as well. By default, the Model Optimizer converts topologies with <code>keep_aspect_ratio_resizer</code> to consume a square input image. If the non-square image is provided as input, it is stretched without keeping aspect ratio that results to objects detection quality decrease.</p>
<blockquote class="doxtable">
<p><b>NOTE</b>: It is highly recommended to specify the <code>--input_shape</code> command line parameter for the models with <code>keep_aspect_ratio_resizer</code> if the input image dimensions are known in advance. </p>
</blockquote>
<h2>Important Notes About Feeding Input Images to the Samples</h2>
<p>Inference Engine comes with a number of samples that use Object Detection API models including:</p>
<ul>
<li><a class="el" href="_inference_engine_samples_object_detection_sample_ssd_README.html">Object Detection for SSD Sample</a> &mdash; for RFCN, SSD and Faster R-CNNs</li>
<li><a class="el" href="_demos_mask_rcnn_demo_README.html">Mask R-CNN Sample for TensorFlow* Object Detection API Models</a> &mdash; for Mask R-CNNs</li>
</ul>
<p>There are a number of important notes about feeding input images to the samples:</p>
<ol type="1">
<li>Inference Engine samples stretch input image to the size of the input layer without preserving aspect ratio. This behavior is usually correct for most topologies (including SSDs), but incorrect for the following Faster R-CNN topologies: Inception ResNet, Inception V2, ResNet50 and ResNet101. Images pre-processing for these topologies keeps aspect ratio. Also all Mask R-CNN and R-FCN topologies require keeping aspect ratio. The type of pre-processing is defined in the pipeline configuration file in the section <code>image_resizer</code>. If keeping aspect ratio is required, then it is necessary to resize image before passing it to the sample.</li>
<li>TensorFlow* implementation of image resize may be different from the one implemented in the sample. Even reading input image from compressed format (like <code>.jpg</code>) could give different results in the sample and TensorFlow*. So, if it is necessary to compare accuracy between the TensorFlow* and the Inference Engine it is recommended to pass pre-scaled input image in a non-compressed format (like <code>.bmp</code>).</li>
<li>If you want to infer the model with the Inference Engine samples, convert the model specifying the <code>--reverse_input_channels</code> command line parameter. The samples load images in BGR channels order, while TensorFlow* models were trained with images in RGB order. When the <code>--reverse_input_channels</code> command line parameter is specified, the Model Optimizer performs first convolution or other channel dependent operation weights modification so the output will be like the image is passed with RGB channels order.</li>
</ol>
<h2>Detailed Explanations of Model Conversion Process</h2>
<p>This section is intended for users who want to understand how the Model Optimizer performs Object Detection API models conversion in details. The knowledge given in this section is also useful for users having complex models that are not converted with the Model Optimizer out of the box. It is highly recommended to read <a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Subgraph_Replacement_Model_Optimizer.html">Sub-Graph Replacement in Model Optimizer</a> chapter first to understand sub-graph replacement concepts which are used here.</p>
<p>Implementation of the sub-graph replacers for Object Detection API models is located in the file <code>&lt;INSTALL_DIR&gt;/deployment_tools/model_optimizer/extensions/front/tf/ObjectDetectionAPI.py</code>.</p>
<p>It is also important to open the model in the <a href="https://www.tensorflow.org/guide/summaries_and_tensorboard">TensorBoard</a> to see the topology structure. Model Optimizer can create an event file that can be then fed to the TensorBoard* tool. Run the Model Optimizer with providing two command line parameters:</p><ul>
<li><code>--input_model &lt;path_to_frozen.pb&gt;</code> &mdash; Path to the frozen model</li>
<li><code>--tensorboard_logdir</code> &mdash; Path to the directory where TensorBoard looks for the event files.</li>
</ul>
<h3>SSD (Single Shot Multibox Detector) Topologies</h3>
<p>The SSD topologies are the simplest ones among Object Detection API topologies, so they will be analyzed first. The sub-graph replacement configuration file <code>ssd_v2_support.json</code>, which should be used to convert these models, contains three sub-graph replacements: <code>ObjectDetectionAPIPreprocessorReplacement</code>, <code>ObjectDetectionAPISSDPostprocessorReplacement</code> and <code>ObjectDetectionAPIOutputReplacement</code>. Their implementation is described below.</p>
<h4>Preprocessor Block</h4>
<p>All Object Detection API topologies contain <code>Preprocessor</code> block of nodes (aka <a href="https://www.tensorflow.org/guide/graph_viz">"scope"</a>) that performs two tasks:</p>
<ul>
<li>Scales image to the size required by the topology.</li>
<li>Applies mean and scale values if needed.</li>
</ul>
<p>Model Optimizer cannot convert the part of the <code>Preprocessor</code> block performing scaling because the TensorFlow implementation uses <code>while</code>- loops which the Inference Engine does not support. Another reason is that the Inference Engine samples scale input images to the size of the input layer from the Intermediate Representation (IR) automatically. Given that it is necessary to cut-off the scaling part of the <code>Preprocessor</code> block and leave only operations applying mean and scale values. This task is solved using the Model Optimizer <a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Subgraph_Replacement_Model_Optimizer.html">sub-graph replacer mechanism</a>.</p>
<p>The <code>Preprocessor</code> block has two outputs: the tensor with pre-processed image(s) data and a tensor with pre-processed image(s) size(s). While converting the model, Model Optimizer keeps only the nodes producing the first tensor. The second tensor is a constant which can be obtained from the <code>pipeline.config</code> file to be used in other replacers.</p>
<p>The implementation of the <code>Preprocessor</code> block sub-graph replacer is the following (file <code>&lt;INSTALL_DIR&gt;/deployment_tools/model_optimizer/extensions/front/tf/ObjectDetectionAPI.py</code>):</p>
<div class="fragment"><div class="line">class ObjectDetectionAPIPreprocessorReplacement(FrontReplacementFromConfigFileSubGraph):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    The class replaces the &quot;Preprocessor&quot; block resizing input image and applying mean/scale values. Only nodes related</div><div class="line">    to applying mean/scaling values are kept.</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    replacement_id = &#39;ObjectDetectionAPIPreprocessorReplacement&#39;</div><div class="line"></div><div class="line">    def run_before(self):</div><div class="line">        return [Pack, Sub]</div><div class="line"></div><div class="line">    def nodes_to_remove(self, graph: Graph, match: SubgraphMatch):</div><div class="line">        new_nodes_to_remove = match.matched_nodes_names()</div><div class="line">        # do not remove nodes that perform input image scaling and mean value subtraction</div><div class="line">        for node_to_keep in (&#39;Preprocessor/sub&#39;, &#39;Preprocessor/sub/y&#39;, &#39;Preprocessor/mul&#39;, &#39;Preprocessor/mul/x&#39;):</div><div class="line">            if node_to_keep in new_nodes_to_remove:</div><div class="line">                new_nodes_to_remove.remove(node_to_keep)</div><div class="line">        return new_nodes_to_remove</div><div class="line"></div><div class="line">    def generate_sub_graph(self, graph: Graph, match: SubgraphMatch):</div><div class="line">        argv = graph.graph[&#39;cmd_params&#39;]</div><div class="line">        layout = graph.graph[&#39;layout&#39;]</div><div class="line">        if argv.tensorflow_object_detection_api_pipeline_config is None:</div><div class="line">            raise Error(missing_param_error)</div><div class="line">        pipeline_config = PipelineConfig(argv.tensorflow_object_detection_api_pipeline_config)</div><div class="line"></div><div class="line">        sub_node = match.output_node(0)[0]</div><div class="line">        if not sub_node.has(&#39;op&#39;) or sub_node.op != &#39;Sub&#39;:</div><div class="line">            raise Error(&#39;The output op of the Preprocessor sub-graph is not of type &quot;Sub&quot;. Looks like the topology is &#39;</div><div class="line">                        &#39;not created with TensorFlow Object Detection API.&#39;)</div><div class="line"></div><div class="line">        mul_node = None</div><div class="line">        if sub_node.in_node(0).has(&#39;op&#39;) and sub_node.in_node(0).op == &#39;Mul&#39;:</div><div class="line">            log.info(&#39;There is image scaling node in the Preprocessor block.&#39;)</div><div class="line">            mul_node = sub_node.in_node(0)</div><div class="line"></div><div class="line">        initial_input_node_name = &#39;image_tensor&#39;</div><div class="line">        if initial_input_node_name not in graph.nodes():</div><div class="line">            raise Error(&#39;Input node &quot;{}&quot; of the graph is not found. Do not run the Model Optimizer with &#39;</div><div class="line">                        &#39;&quot;--input&quot; command line parameter.&#39;.format(initial_input_node_name))</div><div class="line">        placeholder_node = Node(graph, initial_input_node_name)</div><div class="line"></div><div class="line">        # set default value of the batch size to 1 if user didn&#39;t specify batch size and input shape</div><div class="line">        batch_dim = get_batch_dim(layout, 4)</div><div class="line">        if argv.batch is None and placeholder_node.shape[batch_dim] == -1:</div><div class="line">            placeholder_node.shape[batch_dim] = 1</div><div class="line">        if placeholder_node.shape[batch_dim] &gt; 1:</div><div class="line">            print(&quot;[ WARNING ] The batch size more than 1 is supported for SSD topologies only.&quot;)</div><div class="line">        height, width = calculate_placeholder_spatial_shape(graph, match, pipeline_config)</div><div class="line">        placeholder_node.shape[get_height_dim(layout, 4)] = height</div><div class="line">        placeholder_node.shape[get_width_dim(layout, 4)] = width</div><div class="line"></div><div class="line">        # save the pre-processed image spatial sizes to be used in the other replacers</div><div class="line">        graph.graph[&#39;preprocessed_image_height&#39;] = placeholder_node.shape[get_height_dim(layout, 4)]</div><div class="line">        graph.graph[&#39;preprocessed_image_width&#39;] = placeholder_node.shape[get_width_dim(layout, 4)]</div><div class="line"></div><div class="line">        to_float_node = placeholder_node.out_node(0)</div><div class="line">        if not to_float_node.has(&#39;op&#39;) or to_float_node.op != &#39;Cast&#39;:</div><div class="line">            raise Error(&#39;The output of the node &quot;{}&quot; is not Cast operation. Cannot apply replacer.&#39;.format(</div><div class="line">                initial_input_node_name))</div><div class="line"></div><div class="line">        # connect to_float_node directly with node performing scale on mean value subtraction</div><div class="line">        if mul_node is None:</div><div class="line">            create_edge(to_float_node, sub_node, 0, 0)</div><div class="line">        else:</div><div class="line">            create_edge(to_float_node, mul_node, 0, 1)</div><div class="line"></div><div class="line">        print(&#39;The Preprocessor block has been removed. Only nodes performing mean value subtraction and scaling (if&#39;</div><div class="line">              &#39; applicable) are kept.&#39;)</div><div class="line">        return {}</div></div><!-- fragment --><p> The <code>run_before</code> function defines a list of replacers which current replacer should be run before. In this case it is <code>Pack</code> and <code>Sub</code>. The <code>Sub</code> operation is not supported by Inference Engine plugins so Model Optimizer replaces it with a combination of the <code>Eltwise</code> layer (element-wise sum) and the <code>ScaleShift</code> layer. But the <code>Preprocessor</code> replacer expects to see <code>Sub</code> node, so it should be called before the <code>Sub</code> is replaced.</p>
<p>The <code>nodes_to_remove</code> function returns list of nodes that should be removed after the replacement happens. In this case it removes all nodes matched in the <code>Preprocessor</code> scope except the <code>Sub</code> and <code>Mul</code> nodes performing mean value subtraction and scaling.</p>
<p>The <code>generate_sub_graph</code> function performs the following actions:</p>
<ul>
<li>Lines 20-24: Reads the <code>pipeline.config</code> configuration file to get the model hyper-parameters and other attributes.</li>
<li>Lines 25-29: Checks that the output node of the <code>Preprocessor</code> scope is of type <code>Sub</code>.</li>
<li>Lines 31-34: Checks that the input of the <code>Sub</code> node is of type <code>Mul</code>. This information is needed to correctly connect the input node of the topology later.</li>
<li>Lines 36-50: Finds the topology input (placeholder) node and sets its weight and height according to the image resizer defined in the <code>pipeline.config</code> file and the <code>--input_shape</code> provided by the user. The batch size is set to 1 by default, but it will be overridden if you specify a batch size using command-line option <code>-b</code>. Refer to the <a href="#tf_od_custom_input_shape">Custom Input Shape</a> on how the Model Optimizer calculates input layer height and width.</li>
<li>Lines 52-54: Saves the placeholder shape in the <code>graph</code> object for other sub-graph replacements.</li>
<li>Lines 56-59: Checks that the placeholder node follows the 'Cast' node which converts model input data from UINT8 to FP32.</li>
<li>Lines 61-65: Creates edge from the placeholder node to the <code>Mul</code> (if present) or <code>Sub</code> node to a correct input port (0 for <code>Sub</code> and 1 for <code>Mul</code>).</li>
<li>Line 69: The replacer returns a dictionary with nodes mapping that is used by other sub-graph replacement functions. In this case, it is not needed, so the empty dictionary is returned.</li>
</ul>
<h4>Postprocessor Block</h4>
<p>A distinct feature of any SSD topology is a part performing non-maximum suppression of proposed images bounding boxes. This part of the topology is implemented with dozens of primitive operations in TensorFlow, while in Inference Engine, it is one <a class="el" href="_docs_MO_DG_prepare_model_convert_model_IRLayersCatalogSpec.html">layer</a> called <code>DetectionOutput</code>. Thus, to convert a SSD model from the TensorFlow, the Model Optimizer should replace the entire sub-graph of operations that implement the <code>DetectionOutput</code> layer with a single <code>DetectionOutput</code> node.</p>
<p>The Inference Engine <code>DetectionOutput</code> layer implementation consumes three tensors in the following order:</p>
<ol type="1">
<li>Tensor with locations of bounding boxes</li>
<li>Tensor with confidences for each bounding box</li>
<li>Tensor with prior boxes ("anchors" in a TensorFlow terminology)</li>
</ol>
<p>The Inference Engine <code>DetectionOutput</code> layer implementation produces one tensor with seven numbers for each actual detection:</p>
<ul>
<li>batch index</li>
<li>class label</li>
<li>class probability</li>
<li>x_1 box coordinate</li>
<li>y_1 box coordinate</li>
<li>x_2 box coordinate</li>
<li>y_2 box coordinate.</li>
</ul>
<p>There are more output tensors in the TensorFlow Object Detection API: "detection_boxes", "detection_classes", "detection_scores" and "num_detections", but the values in them are consistent with the output values of the Inference Engine DetectionOutput layer.</p>
<p>The sub-graph replacement by points is used in the <code>ssd_v2_support.json</code> to match the <code>Postprocessor</code> block. The start points are defined the following way:</p>
<ul>
<li>"Postprocessor/Shape" receives tensor with bounding boxes;</li>
<li>"Postprocessor/scale_logits" receives tensor with confidences(probabilities) for each box;</li>
<li>"Postprocessor/Tile" receives tensor with prior boxes (anchors);</li>
<li>"Postprocessor/Reshape_1" is specified only to match the whole <code>Postprocessor</code> scope. Not used in the replacement code;</li>
<li>"Postprocessor/ToFloat" is specified only to match the whole <code>Postprocessor</code> scope. Not used in the replacement code.</li>
</ul>
<p>There are a number of differences in layout, format and content of in input tensors to <code>DetectionOutput</code> layer and what tensors generates TensorFlow, so additional tensors processing before creating <code>DetectionOutput</code> layer is required. It is described below. The sub-graph replacement class for the <code>DetectionOutput</code> layer is given below:</p>
<div class="fragment"><div class="line">class ObjectDetectionAPISSDPostprocessorReplacement(FrontReplacementFromConfigFileSubGraph):</div><div class="line">    replacement_id = &#39;ObjectDetectionAPISSDPostprocessorReplacement&#39;</div><div class="line"></div><div class="line">    def run_after(self):</div><div class="line">        return [ObjectDetectionAPIPreprocessorReplacement]</div><div class="line"></div><div class="line">    def run_before(self):</div><div class="line">        # the replacer uses node of type &quot;RealDiv&quot; as one of the start points, but Model Optimizer replaces nodes of</div><div class="line">        # type &quot;RealDiv&quot; with a new ones, so it is necessary to replace the sub-graph before replacing the &quot;RealDiv&quot;</div><div class="line">        # nodes</div><div class="line">        return [Div, StandaloneConstEraser]</div><div class="line"></div><div class="line">    def output_edges_match(self, graph: Graph, match: SubgraphMatch, new_sub_graph: dict):</div><div class="line">        # the DetectionOutput in IE produces single tensor, but in TF it produces two tensors, so create only one output</div><div class="line">        # edge match</div><div class="line">        return {match.output_node(0)[0].id: new_sub_graph[&#39;detection_output_node&#39;].id}</div><div class="line"></div><div class="line">    def generate_sub_graph(self, graph: Graph, match: SubgraphMatch):</div><div class="line">        argv = graph.graph[&#39;cmd_params&#39;]</div><div class="line">        if argv.tensorflow_object_detection_api_pipeline_config is None:</div><div class="line">            raise Error(missing_param_error)</div><div class="line">        pipeline_config = PipelineConfig(argv.tensorflow_object_detection_api_pipeline_config)</div><div class="line">        num_classes = _value_or_raise(match, pipeline_config, &#39;num_classes&#39;)</div><div class="line"></div><div class="line">        # reshapes confidences to 4D before applying activation function</div><div class="line">        expand_dims_op = Reshape(graph, {&#39;dim&#39;: int64_array([0, 1, -1, num_classes + 1])})</div><div class="line">        # do not convert from NHWC to NCHW this node shape</div><div class="line">        expand_dims_node = expand_dims_op.create_node([match.input_nodes(1)[0][0].in_node(0)],</div><div class="line">                                                      dict(name=&#39;do_ExpandDims_conf&#39;))</div><div class="line"></div><div class="line">        activation_function = _value_or_raise(match, pipeline_config, &#39;postprocessing_score_converter&#39;)</div><div class="line">        activation_conf_node = add_activation_function_after_node(graph, expand_dims_node, activation_function)</div><div class="line">        PermuteAttrs.set_permutation(expand_dims_node, expand_dims_node.out_node(), None)</div><div class="line"></div><div class="line">        # IE DetectionOutput layer consumes flattened tensors</div><div class="line">        # reshape operation to flatten locations tensor</div><div class="line">        reshape_loc_op = Reshape(graph, {&#39;dim&#39;: int64_array([0, -1])})</div><div class="line">        reshape_loc_node = reshape_loc_op.create_node([match.input_nodes(0)[0][0].in_node(0)],</div><div class="line">                                                      dict(name=&#39;do_reshape_loc&#39;))</div><div class="line"></div><div class="line">        # IE DetectionOutput layer consumes flattened tensors</div><div class="line">        # reshape operation to flatten confidence tensor</div><div class="line">        reshape_conf_op = Reshape(graph, {&#39;dim&#39;: int64_array([0, -1])})</div><div class="line">        reshape_conf_node = reshape_conf_op.create_node([activation_conf_node], dict(name=&#39;do_reshape_conf&#39;))</div><div class="line"></div><div class="line">        if pipeline_config.get_param(&#39;ssd_anchor_generator_num_layers&#39;) is not None or \</div><div class="line">                        pipeline_config.get_param(&#39;multiscale_anchor_generator_min_level&#39;) is not None:</div><div class="line">            # change the Reshape operations with hardcoded number of output elements of the convolution nodes to be</div><div class="line">            # reshapable</div><div class="line">            _relax_reshape_nodes(graph, pipeline_config)</div><div class="line"></div><div class="line">            # create PriorBoxClustered nodes instead of a constant value with prior boxes so the model could be reshaped</div><div class="line">            if pipeline_config.get_param(&#39;ssd_anchor_generator_num_layers&#39;) is not None:</div><div class="line">                priors_node = _create_prior_boxes_node(graph, pipeline_config)</div><div class="line">            elif pipeline_config.get_param(&#39;multiscale_anchor_generator_min_level&#39;) is not None:</div><div class="line">                priors_node = _create_multiscale_prior_boxes_node(graph, pipeline_config)</div><div class="line">        else:</div><div class="line">            log.info(&#39;The anchor generator is not known. Save constant with prior-boxes to IR.&#39;)</div><div class="line">            priors_node = match.input_nodes(2)[0][0].in_node(0)</div><div class="line"></div><div class="line">        # creates DetectionOutput Node object from Op class</div><div class="line">        detection_output_op = DetectionOutput(graph, match.custom_replacement_desc.custom_attributes)</div><div class="line">        detection_output_op.attrs[&#39;old_infer&#39;] = detection_output_op.attrs[&#39;infer&#39;]</div><div class="line">        detection_output_op.attrs[&#39;infer&#39;] = __class__.do_infer</div><div class="line">        detection_output_node = detection_output_op.create_node(</div><div class="line">            [reshape_loc_node, reshape_conf_node, priors_node],</div><div class="line">            dict(name=detection_output_op.attrs[&#39;type&#39;],</div><div class="line">                 clip=1,</div><div class="line">                 confidence_threshold=_value_or_raise(match, pipeline_config, &#39;postprocessing_score_threshold&#39;),</div><div class="line">                 top_k=_value_or_raise(match, pipeline_config, &#39;postprocessing_max_detections_per_class&#39;),</div><div class="line">                 keep_top_k=_value_or_raise(match, pipeline_config, &#39;postprocessing_max_total_detections&#39;),</div><div class="line">                 nms_threshold=_value_or_raise(match, pipeline_config, &#39;postprocessing_iou_threshold&#39;)))</div><div class="line"></div><div class="line">        return {&#39;detection_output_node&#39;: detection_output_node}</div></div><!-- fragment --><p>The <code>run_before</code> and <code>run_after</code> functions define lists of replacers that this replacer should be run before and after respectively.</p>
<p>The <code>input_edges_match</code> and <code>output_edges_match</code> functions generate dictionaries describing how the input/output nodes matched with the replacer should be connected with new nodes generated in the <code>generate_sub_graph</code> function. Refer to <a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Subgraph_Replacement_Model_Optimizer.html">sub-graph replacements</a> documentation for more information.</p>
<p>The <code>generate_sub_graph</code> function performs the following actions:</p>
<ul>
<li>Lines 19-23: Reads the <code>pipeline.config</code> configuration file to get the model hyper-parameters and other attributes.</li>
<li>Lines 25-32: Makes tensor with confidences 4D and apply correct activation function (read from the <code>pipeline.config</code> file) to it.</li>
<li>Line 33: Disables permutation of <code>expand_dims_node</code>'s attributes because they are already in the NCHW layout.</li>
<li>Lines 35-39: Makes tensor with bounding boxes 2D, where the first dimension corresponds to a batch size.</li>
<li>Lines 49-52: Makes tensor with confidences 2D, where the first dimension corresponds to a batch size.</li>
<li>Lines 41-44: Creates a node with <code>DetectionOutput</code> layer with a number of layer attributes from the <code>pipeline.config</code> file. Also the inference function (<code>infer</code> attribute) is updated with a custom inference function <code>__class__.do_infer</code>. The latter change is described below.</li>
<li>Lines 46-59: Creates several <code>PriorBoxClustered</code> layers which generate prior boxes depending on the type of the grid anchor generator defined in the <code>pipeline.config</code> file. If the grid anchor type is not known then initialize <code>priors_node</code> as a node matched by the sub-graph replacement. In the latter case it is a constant node with prior boxes calculated for a particular input image shape.</li>
<li>Lines 61-72: Creates <code>DetectionOutput</code> layer with attributes from the <code>pipeline.config</code> file.</li>
<li>Line 74: Returns dictionary with mapping of nodes that is used in the <code>input_edges_match</code> and <code>output_edges_match</code> functions.</li>
</ul>
<p>The paragraphs below explains why the inference function for the Detection Output layer is modified. Before doing that it is necessary to make acquaintance with selected high-level steps of the Model Optimize model conversion pipeline. Note, that only selected steps are required for understanding the change are mentioned:</p>
<ol type="1">
<li>Model Optimizer creates calculation graph from the initial topology where each nodes corresponds to a operation from the initial model.</li>
<li>Model Optimizer performs "Front replacers" (including the one being described now).</li>
<li>Model Optimizer adds data nodes between operation nodes to the graph.</li>
<li>Model Optimizer performs "Middle replacers".</li>
<li>Model Optimizer performs "shape inference" phase. During this phase the shape of all data nodes is being calculated. Model Optimizer also calculates value for data tensors which are constant, i.e. do not depend on input. For example, tensor with prior boxes (generated with <code>MultipleGridAnchorGenerator</code> or similar scopes) doesn't depend on input and is evaluated by Model Optimizer during shape inference. Model Optimizer uses inference function stored in the 'infer' attribute of operation nodes.</li>
<li>Model Optimizer performs "Back replacers".</li>
<li>Model Optimizer generates IR.</li>
</ol>
<p>The <code>do_infer</code> function is needed to perform some adjustments to the tensor with prior boxes (anchors) that is known only after the shape inference phase and to perform additional transformations described below. This change is performed only if the tensor with prior boxes is not constant (so it is produced by <code>PriorBoxClustered</code> layers during inference). It is possible to make the <code>Postprocessor</code> block replacement as a Middle replacer (so the prior boxes tensor would be evaluated by the time the replacer is called), but in this case it will be necessary to correctly handle data nodes which are created between each pair of initially adjacent operation nodes. In order to inject required modification to the inference function of the <code>DetectionOutput</code> node, a new function is created to perform modifications and to call the initial inference function. The code of a new inference function is the following:</p>
<div class="fragment"><div class="line">@staticmethod</div><div class="line">def do_infer(node: Node):</div><div class="line">    prior_boxes = node.in_node(2).value</div><div class="line">    if prior_boxes is not None:</div><div class="line">        argv = node.graph.graph[&#39;cmd_params&#39;]</div><div class="line">        if argv.tensorflow_object_detection_api_pipeline_config is None:</div><div class="line">            raise Error(missing_param_error)</div><div class="line">        pipeline_config = PipelineConfig(argv.tensorflow_object_detection_api_pipeline_config)</div><div class="line">        variance = _variance_from_pipeline_config(pipeline_config)</div><div class="line">        # replicating the variance values for all prior-boxes</div><div class="line">        variances = np.tile(variance, [prior_boxes.shape[-2], 1])</div><div class="line">        # DetectionOutput Inference Engine expects the prior-boxes in the following layout: (values, variances)</div><div class="line">        prior_boxes = prior_boxes.reshape([-1, 4])</div><div class="line">        prior_boxes = np.concatenate((prior_boxes, variances), 0)</div><div class="line">        # compared to the IE&#39;s DetectionOutput, the TF keeps the prior-boxes in YXYX, need to get back to the XYXY</div><div class="line">        prior_boxes = np.concatenate((prior_boxes[:, 1:2], prior_boxes[:, 0:1],</div><div class="line">                                      prior_boxes[:, 3:4], prior_boxes[:, 2:3]), 1)</div><div class="line">        #  adding another dimensions, as the prior-boxes are expected as 3d tensors</div><div class="line">        prior_boxes = prior_boxes.reshape((1, 2, -1))</div><div class="line">        node.in_node(2).shape = int64_array(prior_boxes.shape)</div><div class="line">        node.in_node(2).value = prior_boxes</div><div class="line"></div><div class="line">    node.old_infer(node)</div><div class="line">    # compared to the IE&#39;s DetectionOutput, the TF keeps the locations in YXYX, need to get back to the XYXY</div><div class="line">    # for last convolutions that operate the locations need to swap the X and Y for output feature weights &amp; biases</div><div class="line">    conv_nodes = backward_bfs_for_operation(node.in_node(0), [&#39;Conv2D&#39;])</div><div class="line">    swap_weights_xy(conv_nodes)</div><div class="line">    squeeze_reshape_and_concat(conv_nodes)</div><div class="line"></div><div class="line">    for node_name in node.graph.nodes():</div><div class="line">        node = Node(node.graph, node_name)</div><div class="line">        if node.has_and_set(&#39;swap_xy_count&#39;) and len(node.out_nodes()) != node[&#39;swap_xy_count&#39;]:</div><div class="line">            raise Error(&#39;The weights were swapped for node &quot;{}&quot;, but this weight was used in other nodes.&#39;.format(</div><div class="line">                node.name))</div></div><!-- fragment --><ul>
<li>Lines 3-18: Updates the value of the tensor with prior boxes by appending variance values if the prior boxes are pre-calculated. Inference Engine implementation of the <code>DetectionOutput</code> layer expects these values located within the tensor with bounding boxes, but in TensorFlow they are applied in different way.</li>
<li>Line 23: Executes initial inference function to calculate the output shape of this node.</li>
<li>Lines 26-27: Finds predecessor node of type "Conv2D" of the node with bounding boxes (which is <code>node.in_node(0)</code>) and modifies convolution weights so "X" and "Y" coordinates are swapped. In TensorFlow bounding boxes are stored in the tensors in "YXYX" order, while in the Inference Engine it is "XYXY".</li>
<li>Line 28: Executes function looking for <code>Reshape</code> operations after the <code>Conv2D</code> nodes found above with 4D output and remove the dimension with index 2 which should be equal to 1. This is a workaround to make tensor 3D so its shape will not be transposed during the IR generation. The problem arises when bounding boxes predictions are reshaped from [1, 1, 1, X] to [1, X / 4, 1, 4]. The result tensor should not be transposed because after transpose it will have shape [1, 4, X / 4, 1] and the concatenation over dimension with index 2 will produce incorrect tensor. Also the function looks for <code>Concat</code> operations and changes the concatenation dimension from 2 to 1.</li>
</ul>
<h3>Faster R-CNN Topologies</h3>
<p>The Faster R-CNN models contain several building blocks similar to building blocks from SSD models so it is highly recommended to read the section about converting them first. Detailed information about Faster R-CNN topologies is provided <a href="https://arxiv.org/abs/1506.01497">in the abstract</a>.</p>
<h4>Preprocessor Block</h4>
<p>Faster R-CNN topologies contain similar <code>Preprocessor</code> block as SSD topologies. The same <code>ObjectDetectionAPIPreprocessorReplacement</code> sub-graph replacer is used to cut it off.</p>
<h4>Proposal Layer</h4>
<p>The <code>Proposal</code> layer is implemented with dozens of primitive operations in TensorFlow, meanwhile, it is a single layer in the Inference Engine. The <code>ObjectDetectionAPIProposalReplacement</code> sub-graph replacer identifies nodes corresponding to the layer and replaces them with required new nodes.</p>
<div class="fragment"><div class="line">class ObjectDetectionAPIProposalReplacement(FrontReplacementFromConfigFileSubGraph):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    This class replaces sub-graph of operations with Proposal layer and additional layers transforming</div><div class="line">    tensors from layout of TensorFlow to layout required by Inference Engine.</div><div class="line">    Refer to comments inside the function for more information about performed actions.</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    replacement_id = &#39;ObjectDetectionAPIProposalReplacement&#39;</div><div class="line"></div><div class="line">    def run_after(self):</div><div class="line">        return [ObjectDetectionAPIPreprocessorReplacement]</div><div class="line"></div><div class="line">    def run_before(self):</div><div class="line">        return [Sub, CropAndResizeReplacement]</div><div class="line"></div><div class="line">    def output_edges_match(self, graph: Graph, match: SubgraphMatch, new_sub_graph: dict):</div><div class="line">        return {match.output_node(0)[0].id: new_sub_graph[&#39;proposal_node&#39;].id}</div><div class="line"></div><div class="line">    def nodes_to_remove(self, graph: Graph, match: SubgraphMatch):</div><div class="line">        new_list = match.matched_nodes_names().copy()</div><div class="line">        # do not remove nodes that produce box predictions and class predictions</div><div class="line">        new_list.remove(match.single_input_node(0)[0].id)</div><div class="line">        new_list.remove(match.single_input_node(1)[0].id)</div><div class="line">        return new_list</div><div class="line"></div><div class="line">    def generate_sub_graph(self, graph: Graph, match: SubgraphMatch):</div><div class="line">        argv = graph.graph[&#39;cmd_params&#39;]</div><div class="line">        if argv.tensorflow_object_detection_api_pipeline_config is None:</div><div class="line">            raise Error(missing_param_error)</div><div class="line">        pipeline_config = PipelineConfig(argv.tensorflow_object_detection_api_pipeline_config)</div><div class="line"></div><div class="line">        max_proposals = _value_or_raise(match, pipeline_config, &#39;first_stage_max_proposals&#39;)</div><div class="line">        proposal_ratios = _value_or_raise(match, pipeline_config, &#39;anchor_generator_aspect_ratios&#39;)</div><div class="line">        proposal_scales = _value_or_raise(match, pipeline_config, &#39;anchor_generator_scales&#39;)</div><div class="line">        anchors_count = len(proposal_ratios) * len(proposal_scales)</div><div class="line"></div><div class="line">        # Convolution/matmul node that produces classes predictions</div><div class="line">        # Permute result of the tensor with classes permissions so it will be in a correct layout for Softmax</div><div class="line">        predictions_node = backward_bfs_for_operation(match.single_input_node(1)[0], [&#39;Add&#39;])[0]</div><div class="line"></div><div class="line">        reshape_classes_op = Reshape(graph, dict(dim=int64_array([0, anchors_count, 2, -1])))</div><div class="line">        reshape_classes_node = reshape_classes_op.create_node([], dict(name=&#39;predictions/Reshape&#39;, nchw_layout=True))</div><div class="line">        predictions_node.insert_node_after(reshape_classes_node, 0)</div><div class="line"></div><div class="line">        softmax_conf_op = Softmax(graph, dict(axis=2, nchw_layout=True, name=reshape_classes_node.id + &#39;/Softmax&#39;))</div><div class="line">        softmax_conf_node = softmax_conf_op.create_node([reshape_classes_node])</div><div class="line">        permute_reshape_softmax_op = Permute(graph, dict(order=int64_array([0, 2, 1, 3]), nchw_layout=True))</div><div class="line">        permute_reshape_softmax_node = permute_reshape_softmax_op.create_node([softmax_conf_node], dict(</div><div class="line">            name=softmax_conf_node.name + &#39;/Permute&#39;))</div><div class="line"></div><div class="line">        initial_shape_op = Shape(graph, dict(name=predictions_node.id + &#39;/Shape&#39;))</div><div class="line">        initial_shape_node = initial_shape_op.create_node([predictions_node])</div><div class="line"></div><div class="line">        # implement custom reshape infer function because we need to know the input convolution node output dimension</div><div class="line">        # sizes but we can know it only after partial infer</div><div class="line">        reshape_permute_op = Reshape(graph, dict())</div><div class="line">        reshape_permute_node = reshape_permute_op.create_node([permute_reshape_softmax_node, initial_shape_node],</div><div class="line">                                                              dict(name=&#39;Reshape_Permute_Class&#39;))</div><div class="line"></div><div class="line">        variance_height = pipeline_config.get_param(&#39;frcnn_variance_height&#39;)</div><div class="line">        variance_width = pipeline_config.get_param(&#39;frcnn_variance_width&#39;)</div><div class="line">        variance_x = pipeline_config.get_param(&#39;frcnn_variance_x&#39;)</div><div class="line">        variance_y = pipeline_config.get_param(&#39;frcnn_variance_y&#39;)</div><div class="line">        anchor_generator_height_stride = pipeline_config.get_param(&#39;anchor_generator_height_stride&#39;)</div><div class="line">        anchor_generator_width_stride = pipeline_config.get_param(&#39;anchor_generator_width_stride&#39;)</div><div class="line">        anchor_generator_height = pipeline_config.get_param(&#39;anchor_generator_height&#39;)</div><div class="line">        anchor_generator_width = pipeline_config.get_param(&#39;anchor_generator_width&#39;)</div><div class="line"></div><div class="line">        if variance_height != variance_width:</div><div class="line">            log.error(&#39;The values for variance for height &quot;{}&quot; is not equal to variance for width &quot;{}&quot;. The detection &#39;</div><div class="line">                      &#39;results will be inaccurate.&#39;.format(variance_height, variance_width))</div><div class="line">        if variance_x != variance_y:</div><div class="line">            log.error(&#39;The values for variance for x &quot;{}&quot; is not equal to variance for y &quot;{}&quot;. The detection &#39;</div><div class="line">                      &#39;results will be inaccurate.&#39;.format(variance_x, variance_y))</div><div class="line">        if anchor_generator_height_stride != anchor_generator_width_stride:</div><div class="line">            log.error(&#39;The values for the anchor generator height stride &quot;{}&quot; is not equal to the anchor generator &#39;</div><div class="line">                      &#39;width stride &quot;{}&quot;. The detection results will be inaccurate.&#39;.format(</div><div class="line">                anchor_generator_height_stride, anchor_generator_width_stride))</div><div class="line">        if anchor_generator_height != anchor_generator_width:</div><div class="line">            log.error(&#39;The values for the anchor generator height &quot;{}&quot; is not equal to the anchor generator width &#39;</div><div class="line">                      &#39;stride &quot;{}&quot;. The detection results will be inaccurate.&#39;.format(anchor_generator_height,</div><div class="line">                                                                                      anchor_generator_width))</div><div class="line"></div><div class="line">        proposal_op = ProposalOp(graph, dict(min_size=1,</div><div class="line">                                             framework=&#39;tensorflow&#39;,</div><div class="line">                                             pre_nms_topn=2 ** 31 - 1,</div><div class="line">                                             box_size_scale=variance_height,</div><div class="line">                                             box_coordinate_scale=variance_x,</div><div class="line">                                             post_nms_topn=max_proposals,</div><div class="line">                                             feat_stride=anchor_generator_height_stride,</div><div class="line">                                             ratio=proposal_ratios,</div><div class="line">                                             scale=proposal_scales,</div><div class="line">                                             normalize=1,</div><div class="line">                                             base_size=anchor_generator_height,</div><div class="line">                                             nms_thresh=_value_or_raise(match, pipeline_config,</div><div class="line">                                                                        &#39;first_stage_nms_iou_threshold&#39;)))</div><div class="line">        for key in (&#39;clip_before_nms&#39;, &#39;clip_after_nms&#39;):</div><div class="line">            if key in match.custom_replacement_desc.custom_attributes:</div><div class="line">                proposal_op.attrs[key] = int(match.custom_replacement_desc.custom_attributes[key])</div><div class="line"></div><div class="line">        anchors_node = backward_bfs_for_operation(match.single_input_node(0)[0], [&#39;Add&#39;])[0]</div><div class="line"></div><div class="line">        # creates input to store input image height, width and scales (usually 1.0s)</div><div class="line">        # the batch size for this input is fixed because it is allowed to pass images of the same size only as input</div><div class="line">        input_op_with_image_size = Input(graph, dict(shape=int64_array([1, 3]), fixed_batch=True))</div><div class="line">        input_with_image_size_node = input_op_with_image_size.create_node([], dict(name=&#39;image_info&#39;))</div><div class="line"></div><div class="line">        proposal_node = proposal_op.create_node([reshape_permute_node, anchors_node, input_with_image_size_node],</div><div class="line">                                                dict(name=&#39;proposals&#39;))</div><div class="line"></div><div class="line">        if &#39;do_not_swap_proposals&#39; in match.custom_replacement_desc.custom_attributes and \</div><div class="line">                match.custom_replacement_desc.custom_attributes[&#39;do_not_swap_proposals&#39;]:</div><div class="line">            swapped_proposals_node = proposal_node</div><div class="line">        else:</div><div class="line">            swapped_proposals_node = add_convolution_to_swap_xy_coordinates(graph, proposal_node, 5)</div><div class="line"></div><div class="line">        proposal_reshape_2d_op = Reshape(graph, dict(dim=int64_array([-1, 5]), nchw_layout=True))</div><div class="line">        proposal_reshape_2d_node = proposal_reshape_2d_op.create_node([swapped_proposals_node],</div><div class="line">                                                                      dict(name=&quot;reshape_swap_proposals_2d&quot;))</div><div class="line"></div><div class="line">        # feed the CropAndResize node with a correct boxes information produced with the Proposal layer</div><div class="line">        # find the first CropAndResize node in the BFS order</div><div class="line">        crop_and_resize_nodes_ids = [node_id for node_id in bfs_search(graph, [match.single_input_node(0)[0].id]) if</div><div class="line">                                     graph.node[node_id][&#39;op&#39;] == &#39;CropAndResize&#39;]</div><div class="line">        assert len(crop_and_resize_nodes_ids) != 0, &quot;Didn&#39;t find any CropAndResize nodes in the graph.&quot;</div><div class="line">        if &#39;do_not_swap_proposals&#39; not in match.custom_replacement_desc.custom_attributes or not \</div><div class="line">                match.custom_replacement_desc.custom_attributes[&#39;do_not_swap_proposals&#39;]:</div><div class="line">            crop_and_resize_node = Node(graph, crop_and_resize_nodes_ids[0])</div><div class="line">            # set a marker that the input with box coordinates has been pre-processed so the CropAndResizeReplacement</div><div class="line">            # transform doesn&#39;t try to merge the second and the third inputs</div><div class="line">            crop_and_resize_node[&#39;inputs_preprocessed&#39;] = True</div><div class="line">            graph.remove_edge(crop_and_resize_node.in_node(1).id, crop_and_resize_node.id)</div><div class="line">            graph.create_edge(proposal_reshape_2d_node, crop_and_resize_node, out_port=0, in_port=1)</div><div class="line"></div><div class="line">        tf_proposal_reshape_4d_op = Reshape(graph, dict(dim=int64_array([-1, 1, max_proposals, 5]), nchw_layout=True))</div><div class="line">        tf_proposal_reshape_4d_node = tf_proposal_reshape_4d_op.create_node([swapped_proposals_node],</div><div class="line">                                                                            dict(name=&quot;reshape_proposal_4d&quot;))</div><div class="line"></div><div class="line">        crop_op = Crop(graph, dict(axis=int64_array([3]), offset=int64_array([1]), dim=int64_array([4]),</div><div class="line">                                   nchw_layout=True))</div><div class="line">        crop_node = crop_op.create_node([tf_proposal_reshape_4d_node], dict(name=&#39;crop_proposals&#39;))</div><div class="line"></div><div class="line">        tf_proposals_crop_reshape_3d_op = Reshape(graph, dict(dim=int64_array([0, -1, 4]), nchw_layout=True))</div><div class="line">        tf_proposals_crop_reshape_3d_node = tf_proposals_crop_reshape_3d_op.create_node([crop_node],</div><div class="line">                                                                                        dict(name=&quot;reshape_crop_3d&quot;))</div><div class="line"></div><div class="line">        return {&#39;proposal_node&#39;: tf_proposals_crop_reshape_3d_node}</div></div><!-- fragment --><p> The main interest of the implementation of this replacer is the <code>generate_sub_graph</code> function.</p>
<p>Lines 26-34: Parses the <code>pipeline.config</code> file and gets required parameters for the <code>Proposal</code> layer.</p>
<p>Lines 38-57: Performs the following manipulations with the tensor with class predictions. TensorFlow uses the NHWC layout, while the Inference Engine uses NCHW. Model Optimizer by default performs transformations with all nodes data in the inference graph to convert it to the NCHW layout. The size of 'C' dimension of the tensor with class predictions is equal to <img class="formulaInl" alt="$base\_anchors\_count \cdot 2$" src="form_84.png"/>, where 2 corresponds to a number of classes (background and foreground) and <img class="formulaInl" alt="$base\_anchors\_count$" src="form_85.png"/> is equal to number of anchors that are applied to each position of 'H' and 'W' dimensions. Therefore, there are <img class="formulaInl" alt="$H \cdot W \cdot base\_anchors\_count$" src="form_86.png"/> bounding boxes. Lines 44-45 apply the Softmax layer to this tensor to get class probabilities for each bounding box.</p>
<p>Lines 59-81: Reads topology parameters related to variances and anchors generation.</p>
<p>Lines 83-108: Adds the <code>Proposal</code> layer to the graph. This layer has one input (generated in lines 104-105) which should be filled with three values before inference: input image height, input image width, image scale factor.</p>
<p>Lines 110-132: Swaps output values of the <code>Proposal</code> layer if the parameter <code>do_not_swap_proposals</code> is not set to <code>True</code> in the sub-graph replacement configuration file for the replacer.</p>
<p>Lines 134-144: Crops the output from the <code>Proposal</code> node to remove the batch indices (the Inference Engine implementation of the <code>Proposal</code> layer generates tensor with shape <code>[num_proposals, 5]</code>). The final tensor contains just box coordinates as in the TensorFlow implementation.</p>
<h4>SecondStagePostprocessor Block</h4>
<p>The <code>SecondStagePostprocessor</code> block is similar to the <code>Postprocessor</code> block from the SSDs topologies. But there are a number of differences in conversion of the <code>SecondStagePostprocessor</code> block.</p>
<div class="fragment"><div class="line">class ObjectDetectionAPIDetectionOutputReplacement(FrontReplacementFromConfigFileSubGraph):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    Replaces the sub-graph that is equal to the DetectionOutput layer from Inference Engine. This replacer is used for</div><div class="line">    Faster R-CNN, R-FCN and Mask R-CNN topologies conversion.</div><div class="line">    The replacer uses a value of the custom attribute &#39;coordinates_swap_method&#39; from the sub-graph replacement</div><div class="line">    configuration file to choose how to swap box coordinates of the 0-th input of the generated DetectionOutput layer.</div><div class="line">    Refer to the code for more details.</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    replacement_id = &#39;ObjectDetectionAPIDetectionOutputReplacement&#39;</div><div class="line"></div><div class="line">    def run_before(self):</div><div class="line">        return [ObjectDetectionAPIMaskRCNNROIPoolingSecondReplacement, Unpack, Sub]</div><div class="line"></div><div class="line">    def run_after(self):</div><div class="line">        return [ObjectDetectionAPIProposalReplacement, CropAndResizeReplacement]</div><div class="line"></div><div class="line">    def nodes_to_remove(self, graph: Graph, match: SubgraphMatch):</div><div class="line">        new_nodes_to_remove = match.matched_nodes_names().copy()</div><div class="line">        outputs = [&#39;detection_boxes&#39;, &#39;detection_scores&#39;, &#39;num_detections&#39;]</div><div class="line">        for output in outputs:</div><div class="line">            children = Node(graph, output).out_nodes()</div><div class="line">            if len(children) != 1:</div><div class="line">                log.warning(&#39;Output {} has {} children. It should have only one output: with op==`OpOutput`&#39;</div><div class="line">                            &#39;&#39;.format(output, len(children)))</div><div class="line">            elif children[list(children.keys())[0]].op == &#39;OpOutput&#39;:</div><div class="line">                new_nodes_to_remove.append(children[list(children.keys())[0]].id)</div><div class="line">            else:</div><div class="line">                continue</div><div class="line">        new_nodes_to_remove.extend(outputs)</div><div class="line">        return new_nodes_to_remove</div><div class="line"></div><div class="line">    def output_edges_match(self, graph: Graph, match: SubgraphMatch, new_sub_graph: dict):</div><div class="line">        # the DetectionOutput in IE produces single tensor, but in TF it produces four tensors, so we need to create</div><div class="line">        # only one output edge match</div><div class="line">        return {match.output_node(0)[0].id: new_sub_graph[&#39;detection_output_node&#39;].id}</div><div class="line"></div><div class="line">    @staticmethod</div><div class="line">    def skip_nodes_by_condition(current_node: Node, condition: callable):</div><div class="line">        while condition(current_node):</div><div class="line">            current_node = current_node.in_node()</div><div class="line">        return current_node</div><div class="line"></div><div class="line">    def generate_sub_graph(self, graph: Graph, match: SubgraphMatch):</div><div class="line">        argv = graph.graph[&#39;cmd_params&#39;]</div><div class="line">        if argv.tensorflow_object_detection_api_pipeline_config is None:</div><div class="line">            raise Error(missing_param_error)</div><div class="line">        pipeline_config = PipelineConfig(argv.tensorflow_object_detection_api_pipeline_config)</div><div class="line"></div><div class="line">        num_classes = _value_or_raise(match, pipeline_config, &#39;num_classes&#39;)</div><div class="line">        max_proposals = _value_or_raise(match, pipeline_config, &#39;first_stage_max_proposals&#39;)</div><div class="line">        activation_function = _value_or_raise(match, pipeline_config, &#39;postprocessing_score_converter&#39;)</div><div class="line"></div><div class="line">        activation_conf_node = add_activation_function_after_node(graph, match.single_input_node(1)[0].in_node(0),</div><div class="line">                                                                  activation_function)</div><div class="line"></div><div class="line">        # IE DetectionOutput layer consumes flattened tensors so need add a Reshape layer.</div><div class="line">        # The batch value of the input tensor is not equal to the batch of the topology, so it is not possible to use</div><div class="line">        # &quot;0&quot; value in the Reshape layer attribute to refer to the batch size, but we know how to</div><div class="line">        # calculate the second dimension so the batch value will be deduced from it with help of &quot;-1&quot;.</div><div class="line">        reshape_conf_op = Reshape(graph, dict(dim=int64_array([-1, (num_classes + 1) * max_proposals])))</div><div class="line">        reshape_conf_node = reshape_conf_op.create_node([activation_conf_node], dict(name=&#39;do_reshape_conf&#39;))</div><div class="line"></div><div class="line">        # Workaround for PermuteForReshape pass.</div><div class="line">        # We looking for first not Reshape-typed node before match.single_input_node(0)[0].in_node(0).</div><div class="line">        # And add  reshape_loc node after this first not Reshape-typed node.</div><div class="line">        current_node = self.skip_nodes_by_condition(match.single_input_node(0)[0].in_node(0),</div><div class="line">                                                    lambda x: x[&#39;kind&#39;] == &#39;op&#39; and x.soft_get(&#39;type&#39;) == &#39;Reshape&#39;)</div><div class="line"></div><div class="line">        reshape_loc_op = Reshape(graph, dict(dim=int64_array([-1, num_classes, 1, 4])))</div><div class="line">        reshape_loc_node = reshape_loc_op.create_node([current_node], dict(name=&#39;reshape_loc&#39;, nchw_layout=True))</div><div class="line">        update_attrs(reshape_loc_node, &#39;shape_attrs&#39;, &#39;dim&#39;)</div><div class="line"></div><div class="line">        # constant node with variances</div><div class="line">        variances_const_op = Const(graph, dict(value=_variance_from_pipeline_config(pipeline_config)))</div><div class="line">        variances_const_node = variances_const_op.create_node([])</div><div class="line"></div><div class="line">        # TF produces locations tensor without boxes for background.</div><div class="line">        # Inference Engine DetectionOutput layer requires background boxes so we generate them</div><div class="line">        loc_node = add_fake_background_loc(graph, reshape_loc_node)</div><div class="line">        PermuteAttrs.set_permutation(reshape_loc_node, loc_node, None)</div><div class="line"></div><div class="line">        # reshape locations tensor to 2D so it could be passed to Eltwise which will be converted to ScaleShift</div><div class="line">        reshape_loc_2d_op = Reshape(graph, dict(dim=int64_array([-1, 4])))</div><div class="line">        reshape_loc_2d_node = reshape_loc_2d_op.create_node([loc_node], dict(name=&#39;reshape_locs_2d&#39;, nchw_layout=True))</div><div class="line">        PermuteAttrs.set_permutation(loc_node, reshape_loc_2d_node, None)</div><div class="line"></div><div class="line">        # element-wise multiply locations with variances</div><div class="line">        eltwise_locs_op = Eltwise(graph, dict(operation=&#39;mul&#39;))</div><div class="line">        eltwise_locs_node = eltwise_locs_op.create_node([reshape_loc_2d_node, variances_const_node],</div><div class="line">                                                        dict(name=&#39;scale_locs&#39;))</div><div class="line"></div><div class="line">        # IE DetectionOutput layer consumes flattened tensors so need add a Reshape layer.</div><div class="line">        # The batch value of the input tensor is not equal to the batch of the topology, so it is not possible to use</div><div class="line">        # &quot;0&quot; value in the Reshape layer attribute to refer to the batch size, but we know how to</div><div class="line">        # calculate the second dimension so the batch value will be deduced from it with help of &quot;-1&quot;.</div><div class="line">        reshape_loc_do_op = Reshape(graph, dict(dim=int64_array([-1, (num_classes + 1) * max_proposals * 4])))</div><div class="line"></div><div class="line">        custom_attributes = match.custom_replacement_desc.custom_attributes</div><div class="line">        coordinates_swap_method = &#39;add_convolution&#39;</div><div class="line">        if &#39;coordinates_swap_method&#39; not in custom_attributes:</div><div class="line">            log.error(&#39;The ObjectDetectionAPIDetectionOutputReplacement sub-graph replacement configuration file &#39;</div><div class="line">                      &#39;must contain &quot;coordinates_swap_method&quot; in the &quot;custom_attributes&quot; dictionary. Two values are &#39;</div><div class="line">                      &#39;supported: &quot;swap_weights&quot; and &quot;add_convolution&quot;. The first one should be used when there is &#39;</div><div class="line">                      &#39;a MatMul or Conv2D node before the &quot;SecondStagePostprocessor&quot; block in the topology. With this &#39;</div><div class="line">                      &#39;solution the weights of the MatMul or Conv2D nodes are permutted, simulating the swap of XY &#39;</div><div class="line">                      &#39;coordinates in the tensor. The second could be used in any other cases but it is worse in terms &#39;</div><div class="line">                      &#39;of performance because it adds the Conv2D node which performs permutting of data. Since the &#39;</div><div class="line">                      &#39;attribute is not defined the second approach is used by default.&#39;)</div><div class="line">        else:</div><div class="line">            coordinates_swap_method = custom_attributes[&#39;coordinates_swap_method&#39;]</div><div class="line">        supported_swap_methods = [&#39;swap_weights&#39;, &#39;add_convolution&#39;]</div><div class="line">        if coordinates_swap_method not in supported_swap_methods:</div><div class="line">            raise Error(&#39;Unsupported &quot;coordinates_swap_method&quot; defined in the sub-graph replacement configuration &#39;</div><div class="line">                        &#39;file. Supported methods are: {}&#39;.format(&#39;, &#39;.join(supported_swap_methods)))</div><div class="line"></div><div class="line">        if coordinates_swap_method == &#39;add_convolution&#39;:</div><div class="line">            swapped_locs_node = add_convolution_to_swap_xy_coordinates(graph, eltwise_locs_node, 4)</div><div class="line">            reshape_loc_do_node = reshape_loc_do_op.create_node([swapped_locs_node], dict(name=&#39;do_reshape_locs&#39;))</div><div class="line">        else:</div><div class="line">            reshape_loc_do_node = reshape_loc_do_op.create_node([eltwise_locs_node], dict(name=&#39;do_reshape_locs&#39;))</div><div class="line"></div><div class="line">        # find Proposal output which has the data layout as in TF: YXYX coordinates without batch indices.</div><div class="line">        proposal_nodes_ids = [node_id for node_id, attrs in graph.nodes(data=True)</div><div class="line">                              if &#39;name&#39; in attrs and attrs[&#39;name&#39;] == &#39;crop_proposals&#39;]</div><div class="line">        if len(proposal_nodes_ids) != 1:</div><div class="line">            raise Error(&quot;Found the following nodes &#39;{}&#39; with name &#39;crop_proposals&#39; but there should be exactly 1. &quot;</div><div class="line">                        &quot;Looks like ObjectDetectionAPIProposalReplacement replacement didn&#39;t work.&quot;.</div><div class="line">                        format(proposal_nodes_ids))</div><div class="line">        proposal_node = Node(graph, proposal_nodes_ids[0])</div><div class="line"></div><div class="line">        # check whether it is necessary to permute proposals coordinates before passing them to the DetectionOutput</div><div class="line">        # currently this parameter is set for the RFCN topologies</div><div class="line">        if &#39;swap_proposals&#39; in custom_attributes and custom_attributes[&#39;swap_proposals&#39;]:</div><div class="line">            proposal_node = add_convolution_to_swap_xy_coordinates(graph, proposal_node, 4)</div><div class="line"></div><div class="line">        # reshape priors boxes as Detection Output expects</div><div class="line">        reshape_priors_op = Reshape(graph, dict(dim=int64_array([-1, 1, max_proposals * 4])))</div><div class="line">        reshape_priors_node = reshape_priors_op.create_node([proposal_node],</div><div class="line">                                                            dict(name=&#39;DetectionOutput_reshape_priors_&#39;))</div><div class="line"></div><div class="line">        detection_output_op = DetectionOutput(graph, {})</div><div class="line">        if coordinates_swap_method == &#39;swap_weights&#39;:</div><div class="line">            # update infer function to re-pack weights</div><div class="line">            detection_output_op.attrs[&#39;old_infer&#39;] = detection_output_op.attrs[&#39;infer&#39;]</div><div class="line">            detection_output_op.attrs[&#39;infer&#39;] = __class__.do_infer</div><div class="line">        for key in (&#39;clip_before_nms&#39;, &#39;clip_after_nms&#39;):</div><div class="line">            if key in match.custom_replacement_desc.custom_attributes:</div><div class="line">                detection_output_op.attrs[key] = int(match.custom_replacement_desc.custom_attributes[key])</div><div class="line"></div><div class="line">        detection_output_node = detection_output_op.create_node(</div><div class="line">            [reshape_loc_do_node, reshape_conf_node, reshape_priors_node],</div><div class="line">            dict(name=detection_output_op.attrs[&#39;type&#39;], share_location=0, variance_encoded_in_target=1,</div><div class="line">                 code_type=&#39;caffe.PriorBoxParameter.CENTER_SIZE&#39;, pad_mode=&#39;caffe.ResizeParameter.CONSTANT&#39;,</div><div class="line">                 resize_mode=&#39;caffe.ResizeParameter.WARP&#39;,</div><div class="line">                 num_classes=num_classes,</div><div class="line">                 confidence_threshold=_value_or_raise(match, pipeline_config, &#39;postprocessing_score_threshold&#39;),</div><div class="line">                 top_k=_value_or_raise(match, pipeline_config, &#39;postprocessing_max_detections_per_class&#39;),</div><div class="line">                 keep_top_k=_value_or_raise(match, pipeline_config, &#39;postprocessing_max_total_detections&#39;),</div><div class="line">                 nms_threshold=_value_or_raise(match, pipeline_config, &#39;postprocessing_iou_threshold&#39;)))</div><div class="line">        # sets specific name to the node so we can find it in other replacers</div><div class="line">        detection_output_node.name = &#39;detection_output&#39;</div><div class="line"></div><div class="line">        output_op = Output(graph, dict(name=&#39;do_OutputOp&#39;))</div><div class="line">        output_op.create_node([detection_output_node])</div><div class="line"></div><div class="line">        print(&#39;The graph output nodes &quot;num_detections&quot;, &quot;detection_boxes&quot;, &quot;detection_classes&quot;, &quot;detection_scores&quot; &#39;</div><div class="line">              &#39;have been replaced with a single layer of type &quot;Detection Output&quot;. Refer to IR catalogue in the &#39;</div><div class="line">              &#39;documentation for information about this layer.&#39;)</div><div class="line"></div><div class="line">        return {&#39;detection_output_node&#39;: detection_output_node}</div><div class="line"></div><div class="line">    @staticmethod</div><div class="line">    def do_infer(node):</div><div class="line">        node.old_infer(node)</div><div class="line">        # compared to the IE&#39;s DetectionOutput, the TF keeps the locations in YXYX, need to get back to the XYXY</div><div class="line">        # for last matmul/Conv2D that operate the locations need to swap the X and Y for output feature weights &amp; biases</div><div class="line">        swap_weights_xy(backward_bfs_for_operation(node.in_node(0), [&#39;MatMul&#39;, &#39;Conv2D&#39;]))</div></div><!-- fragment --><p>The differences in conversion are the following:</p>
<ul>
<li>The locations tensor does not contain information about class 0 (background), but Inference Engine <code>DetectionOutput</code> layer expects it. Line 79 append dummy tensor with fake coordinates.</li>
<li>The prior boxes tensor are not constant like in SSDs models, so it is not possible to apply the same solution. Instead, the element-wise multiplication is added to scale prior boxes tensor values with the variances values. The attribute <code>variance_encoded_in_target=1</code> is set to the <code>DetectionOutput</code> layer (lines 141-159).</li>
<li>The X and Y coordinates in the tensor with bounding boxes locations adjustments should be swapped. For some topologies it could be done by updating preceding convolution weights, but if there is no preceding convolutional node, the Model Optimizer inserts convolution node with specific kernel and weights that performs coordinates swap during topology inference.</li>
<li>Added marker node of type <code>OpOutput</code> that is used by the Model Optimizer to determine output nodes of the topology. It is used in the dead nodes elimination pass.</li>
</ul>
<h4>Cutting Off Part of the Topology</h4>
<p>There is an ability to cut-off part of the topology using the <code>--output</code> command line parameter. Detailed information on why it could be useful is provided in the <a class="el" href="_docs_MO_DG_prepare_model_convert_model_Cutting_Model.html">Cutting Off Parts of a Model </a>. The Faster R-CNN models are cut at the end using the sub-graph replacer <code>ObjectDetectionAPIOutputReplacement</code>.</p>
<div class="fragment"><div class="line">class ObjectDetectionAPIOutputReplacement(FrontReplacementFromConfigFileGeneral):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    This replacer is used to cut-off the network by specified nodes for models generated with Object Detection API.</div><div class="line">    The custom attribute for the replacer contains one value for key &quot;outputs&quot;. This string is a comma separated list</div><div class="line">    of outputs alternatives. Each output alternative is a &#39;|&#39; separated list of node name which could be outputs. The</div><div class="line">    first node from each alternative that exits in the graph is chosen. Others are ignored.</div><div class="line">    For example, if the &quot;outputs&quot; is equal to the following string:</div><div class="line"></div><div class="line">        &quot;Reshape_16,SecondStageBoxPredictor_1/Conv_3/BiasAdd|SecondStageBoxPredictor_1/Conv_1/BiasAdd&quot;</div><div class="line"></div><div class="line">    then the &quot;Reshape_16&quot; will be an output if it exists in the graph. The second output will be</div><div class="line">    SecondStageBoxPredictor_1/Conv_3/BiasAdd if it exist in the graph, if not then</div><div class="line">    SecondStageBoxPredictor_1/Conv_1/BiasAdd will be output if it exists in the graph.</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    replacement_id = &#39;ObjectDetectionAPIOutputReplacement&#39;</div><div class="line"></div><div class="line">    def run_before(self):</div><div class="line">        return [ObjectDetectionAPIPreprocessorReplacement]</div><div class="line"></div><div class="line">    def transform_graph(self, graph: Graph, replacement_descriptions: dict):</div><div class="line">        if graph.graph[&#39;cmd_params&#39;].output is not None:</div><div class="line">            log.warning(&#39;User defined output nodes are specified. Skip the graph cut-off by the &#39;</div><div class="line">                        &#39;ObjectDetectionAPIOutputReplacement.&#39;)</div><div class="line">            return</div><div class="line">        outputs = []</div><div class="line">        outputs_string = replacement_descriptions[&#39;outputs&#39;]</div><div class="line">        for alternatives in outputs_string.split(&#39;,&#39;):</div><div class="line">            for out_node_name in alternatives.split(&#39;|&#39;):</div><div class="line">                if graph.has_node(out_node_name):</div><div class="line">                    outputs.append(out_node_name)</div><div class="line">                    break</div><div class="line">                else:</div><div class="line">                    log.debug(&#39;A node &quot;{}&quot; does not exist in the graph. Do not add it as output&#39;.format(out_node_name))</div><div class="line">        _outputs = output_user_data_repack(graph, outputs)</div><div class="line">        add_output_ops(graph, _outputs, graph.graph[&#39;inputs&#39;])</div></div><!-- fragment --><p>This is a replacer of type "general" which is called just once in comparison with other Front-replacers ("scope" and "points") which are called for each matched instance. The replacer reads node names that should become new output nodes, like specifying <code>--output &lt;node_names&gt;</code>. The only difference is that the string containing node names could contain '|' character specifying output node names alternatives. Detailed explanation is provided in the class description in the code.</p>
<p>The <code>detection_boxes</code>, <code>detection_scores</code>, <code>num_detections</code> nodes are specified as outputs in the <code>faster_rcnn_support.json</code> file. These nodes are used to remove part of the graph that is not be needed to calculate value of specified output nodes.</p>
<h3>R-FCN topologies</h3>
<p>The R-FCN models are based on Faster R-CNN models so it is highly recommended to read the section about converting them first. Detailed information about R-FCN topologies is provided <a href="https://arxiv.org/abs/1605.06409">in the abstract</a>.</p>
<h4>Preprocessor Block</h4>
<p>R-FCN topologies contain similar <code>Preprocessor</code> block as SSD and Faster R-CNN topologies. The same <code>ObjectDetectionAPIPreprocessorReplacement</code> sub-graph replacer is used to cut it off.</p>
<h4>Proposal Layer</h4>
<p>Similar to Faster R-CNNs, R-FCN topologies contain implementation of Proposal layer before the <code>SecondStageBoxPredictor</code> block, so <code>ObjectDetectionAPIProposalReplacement</code> replacement is used in the sub-graph replacement configuration file.</p>
<h4>SecondStageBoxPredictor block</h4>
<p>The <code>SecondStageBoxPredictor</code> block differs from the self-titled block from Faster R-CNN topologies. It contains a number of <code>CropAndResize</code> operations consuming variously scaled boxes generated with a Proposal layer. The combination of <code>CropAndResize</code> layers located in the <code>while</code> loop forms a single position-sensitive ROI pooling (PSROIPooling) layer with bilinear interpolation. The <code>ObjectDetectionAPIPSROIPoolingReplacement</code> replacement matches two <code>while</code> loops with PSROIPooling layers applied to the blobs with box coordinates and classes predictions.</p>
<div class="fragment"><div class="line">class ObjectDetectionAPIPSROIPoolingReplacement(FrontReplacementFromConfigFileSubGraph):</div><div class="line">    replacement_id = &#39;ObjectDetectionAPIPSROIPoolingReplacement&#39;</div><div class="line"></div><div class="line">    def run_after(self):</div><div class="line">        return [ObjectDetectionAPIProposalReplacement]</div><div class="line"></div><div class="line">    def output_edges_match(self, graph: Graph, match: SubgraphMatch, new_sub_graph: dict):</div><div class="line">        return {match.output_node(0)[0].id: new_sub_graph[&#39;output_node&#39;].id}</div><div class="line"></div><div class="line">    def generate_sub_graph(self, graph: Graph, match: SubgraphMatch):</div><div class="line">        argv = graph.graph[&#39;cmd_params&#39;]</div><div class="line">        if argv.tensorflow_object_detection_api_pipeline_config is None:</div><div class="line">            raise Error(missing_param_error)</div><div class="line">        pipeline_config = PipelineConfig(argv.tensorflow_object_detection_api_pipeline_config)</div><div class="line">        num_classes = _value_or_raise(match, pipeline_config, &#39;num_classes&#39;)</div><div class="line"></div><div class="line">        input_node = match.input_nodes(0)[0][0].in_node(0)</div><div class="line">        if &#39;class_predictions&#39; in input_node.id:</div><div class="line">            psroipooling_output_dim = num_classes + 1</div><div class="line">        else:</div><div class="line">            psroipooling_output_dim = num_classes * 4</div><div class="line"></div><div class="line">        num_spatial_bins_height = pipeline_config.get_param(&#39;num_spatial_bins_height&#39;)</div><div class="line">        num_spatial_bins_width = pipeline_config.get_param(&#39;num_spatial_bins_width&#39;)</div><div class="line">        crop_height = pipeline_config.get_param(&#39;crop_height&#39;)</div><div class="line">        crop_width = pipeline_config.get_param(&#39;crop_width&#39;)</div><div class="line">        if crop_height != crop_width:</div><div class="line">            raise Error(&#39;Different &quot;crop_height&quot; and &quot;crop_width&quot; parameters from the pipeline config are not &#39;</div><div class="line">                        &#39;supported: {} vs {}&#39;.format(crop_height, crop_width))</div><div class="line">        psroipooling_op = PSROIPoolingOp(graph, {&#39;name&#39;: input_node.soft_get(&#39;name&#39;) + &#39;/PSROIPooling&#39;,</div><div class="line">                                                 &#39;output_dim&#39;: psroipooling_output_dim,</div><div class="line">                                                 &#39;group_size&#39;: crop_width / num_spatial_bins_width,</div><div class="line">                                                 &#39;spatial_bins_x&#39;: num_spatial_bins_width,</div><div class="line">                                                 &#39;spatial_bins_y&#39;: num_spatial_bins_height,</div><div class="line">                                                 &#39;mode&#39;: &#39;bilinear&#39;,</div><div class="line">                                                 &#39;spatial_scale&#39;: 1,</div><div class="line">                                                 })</div><div class="line"></div><div class="line">        if &#39;reshape_swap_proposals_2d&#39; in graph.nodes():</div><div class="line">            reshape_swap_proposals_node = Node(graph, &#39;reshape_swap_proposals_2d&#39;)</div><div class="line">        else:</div><div class="line">            swap_proposals_node = add_convolution_to_swap_xy_coordinates(graph, Node(graph, &#39;proposals&#39;), 5)</div><div class="line">            reshape_swap_proposals_node = Reshape(graph, {&#39;dim&#39;: [-1, 5], &#39;nchw_layout&#39;: True,</div><div class="line">                                                          &#39;name&#39;: &#39;reshape_swap_proposals_2d&#39;}).create_node(</div><div class="line">                [swap_proposals_node])</div><div class="line">        psroipooling_node = psroipooling_op.create_node([input_node, reshape_swap_proposals_node])</div><div class="line"></div><div class="line">        reduce_op = Reduce(graph, {&#39;name&#39;: &#39;mean&#39;,</div><div class="line">                                   &#39;reduce_type&#39;: &#39;mean&#39;,</div><div class="line">                                   &#39;axis&#39;: int64_array([1, 2]),</div><div class="line">                                   &#39;keep_dims&#39;: True</div><div class="line">                                   })</div><div class="line">        reduce_node = reduce_op.create_node([psroipooling_node])</div><div class="line"></div><div class="line">        graph.erase_node(match.output_node(0)[0].out_node())</div><div class="line"></div><div class="line">        return {&#39;output_node&#39;: reduce_node}</div></div><!-- fragment --><p>The main interest of the implementation of this replacer is the <code>generate_sub_graph</code> function.</p>
<p>Lines 12-15: Parses the <code>pipeline.config</code> file and gets required parameters for the <code>PSROIPooling</code> layer. Lines 17-21: Determines number of output channels for the <code>PSROIPooling</code> layer for box coordinates and classes predictions. Lines 23-46: Create <code>PSROIPooling</code> layer based on model parameters determined from the pipeline configuration file. Lines 48-53: Add Reduce layer which is the output of the <code>while</code> loops being replaced.</p>
<h4>SecondStagePostprocessor block</h4>
<p>The <code>SecondStagePostprocessor</code> block implements functionality of the <code>DetectionOutput</code> layer from the Inference Engine. The <code>ObjectDetectionAPIDetectionOutputReplacement</code> sub-graph replacement is used to replace the block. For this type of topologies the replacer adds convolution node to swap coordinates of boxes in of the 0-th input tensor to the <code>DetectionOutput</code> layer. The custom attribute <code>coordinates_swap_method</code> is set to value <code>add_convolution</code> in the sub-graph replacement configuration file to enable that behaviour. A method (<code>swap_weights</code>) is not suitable for this type of topologies because there are no <code>Mul</code> or <code>Conv2D</code> operations before the 0-th input of the <code>DetectionOutput</code> layer.</p>
<h4>Cutting Off Part of the Topology</h4>
<p>The R-FCN models are cut at the end with the sub-graph replacer <code>ObjectDetectionAPIOutputReplacement</code> as Faster R-CNNs topologies using the following output node names: <code>detection_boxes</code>.</p>
<h3>Mask R-CNN Topologies</h3>
<p>The Mask R-CNN models are based on Faster R-CNN models so it is highly recommended to read the section about converting them first. Detailed information about Mask R-CNN topologies is provided <a href="https://arxiv.org/abs/1703.06870">in the abstract</a>.</p>
<h4>Preprocessor Block</h4>
<p>Mask R-CNN topologies contain similar <code>Preprocessor</code> block as SSD and Faster R-CNN topologies. The same <code>ObjectDetectionAPIPreprocessorReplacement</code> sub-graph replacer is used to cut it off.</p>
<h4>Proposal and ROI (Region of Interest) Pooling</h4>
<p>Proposal and ROI Pooling layers are added to Mask R-CNN topologies like in Faster R-CNNs.</p>
<h4>DetectionOutput Layer</h4>
<p>Unlike in SSDs and Faster R-CNNs, the implementation of the <code>DetectionOutput</code> layer in Mask R-CNNs topologies is not separated in a dedicated scope. But the matcher is defined with start/end points defined in the <code>mask_rcnn_support.json</code> so the replacer correctly adds the <code>DetectionOutput</code> layer.</p>
<h4>One More ROIPooling</h4>
<p>There is the second <code>CropAndResize</code> (equivalent of <code>ROIPooling</code> layer) that uses boxes produced with the <code>DetectionOutput</code> layer. The <code>ObjectDetectionAPIMaskRCNNROIPoolingSecondReplacement</code> replacer is used to replace this node.</p>
<div class="fragment"><div class="line">class ObjectDetectionAPIMaskRCNNROIPoolingSecondReplacement(FrontReplacementFromConfigFileSubGraph):</div><div class="line">    replacement_id = &#39;ObjectDetectionAPIMaskRCNNROIPoolingSecondReplacement&#39;</div><div class="line"></div><div class="line">    def run_after(self):</div><div class="line">        return [ObjectDetectionAPIProposalReplacement]</div><div class="line"></div><div class="line">    def output_edges_match(self, graph: Graph, match: SubgraphMatch, new_sub_graph: dict):</div><div class="line">        return {match.output_node(0)[0].id: new_sub_graph[&#39;roi_pooling_node&#39;].id}</div><div class="line"></div><div class="line">    def generate_sub_graph(self, graph: Graph, match: SubgraphMatch):</div><div class="line">        argv = graph.graph[&#39;cmd_params&#39;]</div><div class="line">        if argv.tensorflow_object_detection_api_pipeline_config is None:</div><div class="line">            raise Error(missing_param_error)</div><div class="line">        pipeline_config = PipelineConfig(argv.tensorflow_object_detection_api_pipeline_config)</div><div class="line">        roi_pool_size = _value_or_raise(match, pipeline_config, &#39;initial_crop_size&#39;)</div><div class="line"></div><div class="line">        detection_output_nodes_ids = [node_id for node_id, attrs in graph.nodes(data=True)</div><div class="line">                                      if &#39;name&#39; in attrs and attrs[&#39;name&#39;] == &#39;detection_output&#39;]</div><div class="line">        if len(detection_output_nodes_ids) != 1:</div><div class="line">            raise Error(&quot;Found the following nodes &#39;{}&#39; with &#39;detection_output&#39; but there should be exactly 1.&quot;.</div><div class="line">                        format(detection_output_nodes_ids))</div><div class="line">        detection_output_node = Node(graph, detection_output_nodes_ids[0])</div><div class="line"></div><div class="line">        # add reshape of Detection Output so it can be an output of the topology</div><div class="line">        reshape_detection_output_2d_op = Reshape(graph, dict(dim=int64_array([-1, 7])))</div><div class="line">        reshape_detection_output_2d_node = reshape_detection_output_2d_op.create_node(</div><div class="line">            [detection_output_node], dict(name=&#39;reshape_do_2d&#39;))</div><div class="line"></div><div class="line">        # adds special node of type &quot;Output&quot; that is a marker for the output nodes of the topology</div><div class="line">        output_op = Output(graph, dict(name=&#39;do_reshaped_OutputOp&#39;))</div><div class="line">        output_node = output_op.create_node([reshape_detection_output_2d_node])</div><div class="line"></div><div class="line">        # add attribute &#39;output_sort_order&#39; so it will be used as a key to sort output nodes before generation of IR</div><div class="line">        output_node.in_edge()[&#39;data_attrs&#39;].append(&#39;output_sort_order&#39;)</div><div class="line">        output_node.in_edge()[&#39;output_sort_order&#39;] = [(&#39;detection_boxes&#39;, 0)]</div><div class="line"></div><div class="line">        # creates two Crop operations which get input from the DetectionOutput layer, cuts of slices of data with class</div><div class="line">        # ids and probabilities and produce a tensor with batch ids and bounding boxes only (as it is expected by the</div><div class="line">        # ROIPooling layer)</div><div class="line">        crop_batch_op = Crop(graph, dict(axis=int64_array([3]), offset=int64_array([0]), dim=int64_array([1]),</div><div class="line">                                         nchw_layout=True))</div><div class="line">        crop_batch_node = crop_batch_op.create_node([detection_output_node], dict(name=&#39;crop_do_batch_ids&#39;))</div><div class="line"></div><div class="line">        crop_coordinates_op = Crop(graph, dict(axis=int64_array([3]), offset=int64_array([3]), dim=int64_array([4]),</div><div class="line">                                               nchw_layout=True))</div><div class="line">        crop_coordinates_node = crop_coordinates_op.create_node([detection_output_node], dict(name=&#39;crop_do_coords&#39;))</div><div class="line"></div><div class="line">        concat_op = Concat(graph, dict(axis=3))</div><div class="line">        concat_node = concat_op.create_node([crop_batch_node, crop_coordinates_node], dict(name=&#39;batch_and_coords&#39;,</div><div class="line">                                                                                           nchw_layout=True))</div><div class="line"></div><div class="line">        # reshape bounding boxes as required by ROIPooling</div><div class="line">        reshape_do_op = Reshape(graph, dict(dim=int64_array([-1, 5])))</div><div class="line">        reshape_do_node = reshape_do_op.create_node([concat_node], dict(name=&#39;reshape_do&#39;))</div><div class="line"></div><div class="line">        roi_pooling_op = ROIPooling(graph, dict(method=&quot;bilinear&quot;, spatial_scale=1,</div><div class="line">                                                pooled_h=roi_pool_size, pooled_w=roi_pool_size))</div><div class="line">        roi_pooling_node = roi_pooling_op.create_node([match.single_input_node(0)[0].in_node(), reshape_do_node],</div><div class="line">                                                      dict(name=&#39;ROI_pooling_2&#39;))</div><div class="line">        return {&#39;roi_pooling_node&#39;: roi_pooling_node}</div></div><!-- fragment --><p> The Inference Engine <code>DetectionOutput</code> layer implementation produces one tensor with seven numbers for each actual detection:</p>
<ul>
<li>batch index</li>
<li>class label</li>
<li>class probability</li>
<li>x_1 box coordinate</li>
<li>y_1 box coordinate</li>
<li>x_2 box coordinate</li>
<li>y_2 box coordinate.</li>
</ul>
<p>The boxes coordinates must be fed to the <code>ROIPooling</code> layer, so the <code>Crop</code> layer is added to remove unnecessary part (lines 37-50).</p>
<p>Then the result tensor is reshaped (lines 53-54) and <code>ROIPooling</code> layer is created (lines 56-59).</p>
<h4>Mask Tensors Processing</h4>
<p>The post-processing part of Mask R-CNN topologies filters out bounding boxes with low probabilities and applies activation function to the rest one. This post-processing is implemented using the <code>Gather</code> operation, which is not supported by the Inference Engine. Special Front-replacer removes this post-processing and just inserts the activation layer to the end. The filtering of bounding boxes is done in the dedicated demo <code>mask_rcnn_demo</code>. The code of the replacer is the following:</p>
<div class="fragment"><div class="line">class ObjectDetectionAPIMaskRCNNSigmoidReplacement(FrontReplacementFromConfigFileGeneral):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    This replacer is used to convert Mask R-CNN topologies only.</div><div class="line">    Adds activation with sigmoid function to the end of the network producing masks tensors.</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    replacement_id = &#39;ObjectDetectionAPIMaskRCNNSigmoidReplacement&#39;</div><div class="line"></div><div class="line">    def run_after(self):</div><div class="line">        return [ObjectDetectionAPIMaskRCNNROIPoolingSecondReplacement]</div><div class="line"></div><div class="line">    def transform_graph(self, graph: Graph, replacement_descriptions):</div><div class="line">        output_node = None</div><div class="line">        op_outputs = [n for n, d in graph.nodes(data=True) if &#39;op&#39; in d and d[&#39;op&#39;] == &#39;OpOutput&#39;]</div><div class="line">        for op_output in op_outputs:</div><div class="line">            last_node = Node(graph, op_output).in_node(0)</div><div class="line">            if last_node.name.startswith(&#39;SecondStageBoxPredictor&#39;):</div><div class="line">                sigmoid_op = Activation(graph, dict(operation=&#39;sigmoid&#39;))</div><div class="line">                sigmoid_node = sigmoid_op.create_node([last_node], dict(name=last_node.id + &#39;/sigmoid&#39;))</div><div class="line">                sigmoid_node.name = &#39;masks&#39;</div><div class="line"></div><div class="line">                if output_node is not None:</div><div class="line">                    raise Error(&#39;Identified two possible outputs from the topology. Cannot proceed.&#39;)</div><div class="line">                # add special node of type &quot;Output&quot; that is a marker for the output nodes of the topology</div><div class="line">                output_op = Output(graph, dict(name=sigmoid_node.name + &#39;/OutputOp&#39;))</div><div class="line">                output_node = output_op.create_node([sigmoid_node])</div><div class="line"></div><div class="line">        print(&#39;The predicted masks are produced by the &quot;masks&quot; layer for each bounding box generated with a &#39;</div><div class="line">              &#39;&quot;detection_output&quot; layer.\n Refer to IR catalogue in the documentation for information &#39;</div><div class="line">              &#39;about the DetectionOutput layer and Inference Engine documentation about output data interpretation.\n&#39;</div><div class="line">              &#39;The topology can be inferred using dedicated demo &quot;mask_rcnn_demo&quot;.&#39;)</div></div><!-- fragment --><p> The replacer looks for the output node which name starts with 'SecondStageBoxPredictor' (the another node of type 'OpOutput' is located after the <code>DetectionOutput</code> node). This node contains the generated masks. The replacer adds activation layer 'Sigmoid' after this node as it is done in the initial TensorFlow* model.</p>
<h4>Cutting Off Part of the Topology</h4>
<p>The Mask R-CNN models are cut at the end with the sub-graph replacer <code>ObjectDetectionAPIOutputReplacement</code> using the following output node names:</p>
<p><code>SecondStageBoxPredictor_1/Conv_3/BiasAdd|SecondStageBoxPredictor_1/Conv_1/BiasAdd</code></p>
<p>One of these two nodes produces output mask tensors. The child nodes of these nodes are related to post-processing which is implemented in the <a class="el" href="_demos_mask_rcnn_demo_README.html">Mask R-CNN demo</a> and should be cut off. </p>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>