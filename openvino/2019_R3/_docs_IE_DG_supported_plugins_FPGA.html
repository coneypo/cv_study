<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>FPGA Plugin - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">FPGA Plugin </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Introducing FPGA Plugin</h2>
<p>The FPGA plugin provides an opportunity for high performance scoring of neural networks on Intel&reg; FPGA devices.</p>
<blockquote class="doxtable">
<p><b>NOTE</b>: Before using the FPGA plugin, ensure that you have installed and configured either the Intel® Vision Accelerator Design with an Intel® Arria 10 FPGA (Speed Grade 1), Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2) or the Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA. For installation and configuration details, see <a class="el" href="_docs_install_guides_installing_openvino_linux_fpga.html">FPGA installation</a>. </p>
</blockquote>
<h2>Heterogeneous Execution</h2>
<p>When your topology contains layers that are not supported by the Intel&reg; FPGA plugin, use <a class="el" href="_docs_IE_DG_supported_plugins_HETERO.html">Heterogeneous plugin</a> with dedicated fallback device.</p>
<p>If a network has layers that are not supported in the Intel&reg; FPGA plugin or in a fallback plugin, you can implement a custom layer on the CPU/GPU and use the extensibility mechanism described in <a class="el" href="_docs_IE_DG_Integrate_your_kernels_into_IE.html">Inference Engine Kernels Extensibility</a>. In addition to adding custom kernels, you must still point to the CPU plugin or the GPU plugin as fallback devices for heterogeneous plugin.</p>
<h2>Supported Networks</h2>
<p>The following network topologies are supported in heterogeneous mode, running on FPGA with fallback to CPU or GPU devices.</p>
<blockquote class="doxtable">
<p><b>IMPORTANT</b>: Use only bitstreams from the current version of the OpenVINO toolkit. Bitstreams from older versions of the OpenVINO toolkit are incompatible with later versions of the OpenVINO toolkit. For example, you cannot use the <code>1-0-1_A10DK_FP16_Generic</code> bitstream, when the OpenVINO toolkit supports the <code>2019R2_PL2_FP16_InceptionV1_SqueezeNet_VGG_YoloV3.aocx</code> bitstream. </p>
</blockquote>
<table class="doxtable">
<tr>
<th align="left">Network </th><th align="left">Bitstreams (Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 1)) </th><th align="left">Bitstreams (Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2)) </th><th align="left">Bitstreams (Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA)  </th></tr>
<tr>
<td align="left">AlexNet </td><td align="left">2019R3_PV_PL1_FP16_AlexNet_GoogleNet_InceptionV1_SSD300_Generic, 2019R3_PV_PL1_FP11_AlexNet_GoogleNet_Generic </td><td align="left">2019R3_PV_PL2_FP16_AlexNet_GoogleNet_InceptionV1_SSD300_Generic, 2019R3_PV_PL2_FP11_AlexNet_GoogleNet_Generic </td><td align="left">2019R3_PV_RC_FP16_AlexNet_GoogleNet_Generic, 2019R3_PV_RC_FP11_AlexNet_GoogleNet_Generic </td></tr>
<tr>
<td align="left">GoogleNet v1 </td><td align="left">2019R3_PV_PL1_FP16_AlexNet_GoogleNet_InceptionV1_SSD300_Generic, 2019R3_PV_PL1_FP11_AlexNet_GoogleNet_Generic </td><td align="left">2019R3_PV_PL2_FP16_AlexNet_GoogleNet_InceptionV1_SSD300_Generic, 2019R3_PV_PL2_FP11_AlexNet_GoogleNet_Generic </td><td align="left">2019R3_PV_RC_FP16_AlexNet_GoogleNet_Generic, 2019R3_PV_RC_FP11_AlexNet_GoogleNet_Generic </td></tr>
<tr>
<td align="left">VGG-16 </td><td align="left">2019R3_PV_PL1_FP16_SqueezeNet_VGG, 2019R3_PV_PL1_FP11_ResNet_VGG </td><td align="left">2019R3_PV_PL2_FP16_SqueezeNet_VGG, 2019R3_PV_PL2_FP11_ResNet_VGG </td><td align="left">2019R3_PV_RC_FP16_InceptionV1_VGG, 2019R3_PV_RC_FP11_InceptionV1_ResNet_SqueezeNet_TinyYolo_VGG </td></tr>
<tr>
<td align="left">VGG-19 </td><td align="left">2019R3_PV_PL1_FP16_SqueezeNet_VGG, 2019R3_PV_PL1_FP11_ResNet_VGG </td><td align="left">2019R3_PV_PL2_FP16_SqueezeNet_VGG, 2019R3_PV_PL2_FP11_ResNet_VGG </td><td align="left">2019R3_PV_RC_FP16_InceptionV1_VGG, 2019R3_PV_RC_FP11_InceptionV1_ResNet_SqueezeNet_TinyYolo_VGG </td></tr>
<tr>
<td align="left">SqueezeNet v 1.0 </td><td align="left">2019R3_PV_PL1_FP16_SqueezeNet_VGG, 2019R3_PV_PL1_FP11_InceptionV1_SqueezeNet </td><td align="left">2019R3_PV_PL2_FP16_SqueezeNet_VGG, 2019R3_PV_PL2_FP11_InceptionV1_SqueezeNet </td><td align="left">2019R3_PV_RC_FP16_SqueezeNet_TinyYolo, 2019R3_PV_RC_FP11_InceptionV1_ResNet_SqueezeNet_TinyYolo_VGG </td></tr>
<tr>
<td align="left">SqueezeNet v 1.1 </td><td align="left">2019R3_PV_PL1_FP16_SqueezeNet_VGG, 2019R3_PV_PL1_FP11_InceptionV1_SqueezeNet </td><td align="left">2019R3_PV_PL2_FP16_SqueezeNet_VGG, 2019R3_PV_PL2_FP11_InceptionV1_SqueezeNet </td><td align="left">2019R3_PV_RC_FP16_SqueezeNet_TinyYolo, 2019R3_PV_RC_FP11_InceptionV1_ResNet_SqueezeNet_TinyYolo_VGG </td></tr>
<tr>
<td align="left">ResNet-18 </td><td align="left">2019R3_PV_PL1_FP16_ResNet_TinyYolo_YoloV3, 2019R3_PV_PL1_FP11_ResNet_VGG </td><td align="left">2019R3_PV_PL2_FP16_ResNet_TinyYolo_YoloV3, 2019R3_PV_PL2_FP11_ResNet_VGG </td><td align="left">2019R3_PV_RC_FP16_ResNet_YoloV3, 2019R3_PV_RC_FP11_InceptionV1_ResNet_SqueezeNet_TinyYolo_VGG </td></tr>
<tr>
<td align="left">ResNet-50 </td><td align="left">2019R3_PV_PL1_FP16_ResNet_TinyYolo_YoloV3, 2019R3_PV_PL1_FP11_ResNet_VGG </td><td align="left">2019R3_PV_PL2_FP16_ResNet_TinyYolo_YoloV3, 2019R3_PV_PL2_FP11_ResNet_VGG </td><td align="left">2019R3_PV_RC_FP16_ResNet_YoloV3, 2019R3_PV_RC_FP11_InceptionV1_ResNet_SqueezeNet_TinyYolo_VGG </td></tr>
<tr>
<td align="left">ResNet-101 </td><td align="left">2019R3_PV_PL1_FP16_ResNet_TinyYolo_YoloV3, 2019R3_PV_PL1_FP11_ResNet_VGG </td><td align="left">2019R3_PV_PL2_FP16_ResNet_TinyYolo_YoloV3, 2019R3_PV_PL2_FP11_ResNet_VGG </td><td align="left">2019R3_PV_RC_FP16_ResNet_YoloV3, 2019R3_PV_RC_FP11_InceptionV1_ResNet_SqueezeNet_TinyYolo_VGG </td></tr>
<tr>
<td align="left">ResNet-152 </td><td align="left">2019R3_PV_PL1_FP16_ResNet_TinyYolo_YoloV3, 2019R3_PV_PL1_FP11_ResNet_VGG </td><td align="left">2019R3_PV_PL2_FP16_ResNet_TinyYolo_YoloV3, 2019R3_PV_PL2_FP11_ResNet_VGG </td><td align="left">2019R3_PV_RC_FP16_ResNet_YoloV3, 2019R3_PV_RC_FP11_InceptionV1_ResNet_SqueezeNet_TinyYolo_VGG </td></tr>
<tr>
<td align="left">MobileNet (Caffe) </td><td align="left">2019R3_PV_PL1_FP16_AlexNet_GoogleNet_InceptionV1_SSD300_Generic, 2019R3_PV_PL1_FP11_MobileNet_TinyYolo_Clamp </td><td align="left">2019R3_PV_PL2_FP16_AlexNet_GoogleNet_InceptionV1_SSD300_Generic, 2019R3_PV_PL2_FP11_ResNet_VGG </td><td align="left">2019R3_PV_RC_FP16_AlexNet_GoogleNet_Generic, 2019R3_PV_RC_FP11_AlexNet_GoogleNet_Generic </td></tr>
<tr>
<td align="left">MobileNet (TensorFlow) </td><td align="left">2019R3_PV_PL1_FP16_MobileNet_Clamp, 2019R3_PV_PL1_FP11_MobileNet_TinyYolo_Clamp </td><td align="left">2019R3_PV_PL2_FP16_MobileNet_Clamp, 2019R3_PV_PL2_FP11_MobileNet_TinyYolo_Clamp </td><td align="left">2019R3_PV_RC_FP16_MobileNet_Clamp, 2019R3_PV_RC_FP11_MobileNet_Clamp </td></tr>
<tr>
<td align="left">SqueezeNet-based variant of the SSD* </td><td align="left">2019R3_PV_PL1_FP16_SqueezeNet_VGG, 2019R3_PV_PL1_FP11_InceptionV1_SqueezeNet </td><td align="left">2019R3_PV_PL2_FP16_SqueezeNet_VGG, 2019R3_PV_PL2_FP11_InceptionV1_SqueezeNet </td><td align="left">2019R3_PV_RC_FP16_SqueezeNet_TinyYolo, 2019R3_PV_RC_FP11_InceptionV1_ResNet_SqueezeNet_TinyYolo_VGG </td></tr>
<tr>
<td align="left">GoogleNet-based variant of SSD </td><td align="left">2019R3_PV_PL1_FP16_AlexNet_GoogleNet_InceptionV1_SSD300_Generic, 2019R3_PV_PL1_FP11_AlexNet_GoogleNet_Generic </td><td align="left">2019R3_PV_PL2_FP16_AlexNet_GoogleNet_InceptionV1_SSD300_Generic, 2019R3_PV_PL2_FP11_AlexNet_GoogleNet_Generic </td><td align="left">2019R3_PV_RC_FP16_AlexNet_GoogleNet_Generic, 2019R3_PV_RC_FP11_AlexNet_GoogleNet_Generic </td></tr>
<tr>
<td align="left">ResNet-based variant of SSD </td><td align="left">2019R3_PV_PL1_FP16_ResNet_TinyYolo_YoloV3, 2019R3_PV_PL1_FP11_ResNet_VGG </td><td align="left">2019R3_PV_PL2_FP16_ResNet_TinyYolo_YoloV3, 2019R3_PV_PL2_FP11_ResNet_VGG </td><td align="left">2019R3_PV_RC_FP16_ResNet_YoloV3, 2019R3_PV_RC_FP11_InceptionV1_ResNet_SqueezeNet_TinyYolo_VGG </td></tr>
<tr>
<td align="left">RMNet </td><td align="left">2019R3_PV_PL1_FP16_RMNet, 2019R3_PV_PL1_FP11_RMNet </td><td align="left">2019R3_PV_PL2_FP16_RMNet, 2019R3_PV_PL2_FP11_RMNet </td><td align="left">2019R3_PV_RC_FP16_RMNet, 2019R3_PV_RC_FP11_RMNet </td></tr>
<tr>
<td align="left">Yolo v3 </td><td align="left">2019R3_PV_PL1_FP16_ResNet_TinyYolo_YoloV3, 2019R3_PV_PL1_FP11_YoloV3_ELU </td><td align="left">2019R3_PV_PL2_FP16_ResNet_TinyYolo_YoloV3, 2019R3_PV_PL2_FP11_YoloV3_ELU </td><td align="left">2019R3_PV_RC_FP16_ResNet_YoloV3, 2019R3_PV_RC_FP11_YoloV3_ELU </td></tr>
</table>
<p>In addition to the list above, arbitrary topologies having big continues subgraphs consisting of layers supported by FPGA plugin are recommended to be executed on FPGA plugin.</p>
<h2><a class="anchor" id="TranslatingArchtoBitstream"></a>Translate from Architecture to FPGA Bitstream Files</h2>
<p>Various FPGA bitstreams that support CNN are available in the OpenVINO&trade; toolkit package for FPGA.</p>
<p>To select the correct bitstream (<code>.aocx</code>) file for an architecture, select a network (for example, Resnet-18) from the table above for either the Intel® Vision Accelerator Design with an Intel® Arria 10 FPGA (Speed Grade 1), Intel® Vision Accelerator Design with an Intel® Arria 10 FPGA (Speed Grade 2) or the Intel&reg; Programmable Acceleration Card (PAC) with Intel&reg; Arria&reg; 10 GX FPGA and note the corresponding architecture.</p>
<p>The following table describes several parameters that might help you to select the proper bitstream for your needs:</p>
<table class="doxtable">
<tr>
<th align="left">Name </th><th align="left">Board </th><th align="left">Precision </th><th align="left">LRN Support </th><th align="left">Leaky ReLU Support </th><th align="left">PReLU Support </th><th align="left">Clamp Support </th><th align="left">ELU Support  </th></tr>
<tr>
<td align="left">2019R3_PV_PL1_FP11_AlexNet_GoogleNet_Generic </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 1) </td><td align="left">FP11 </td><td align="left">true </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL1_FP11_InceptionV1_SqueezeNet </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 1) </td><td align="left">FP11 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL1_FP11_MobileNet_TinyYolo_Clamp </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 1) </td><td align="left">FP11 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">true </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL1_FP11_ResNet_VGG </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 1) </td><td align="left">FP11 </td><td align="left">false </td><td align="left">false </td><td align="left">false </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL1_FP11_RMNet </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 1) </td><td align="left">FP11 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">true </td></tr>
<tr>
<td align="left">2019R3_PV_PL1_FP11_SSD300 </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 1) </td><td align="left">FP11 </td><td align="left">true </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL1_FP11_YoloV3_ELU </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 1) </td><td align="left">FP11 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">true </td></tr>
<tr>
<td align="left">2019R3_PV_PL1_FP16_AlexNet_GoogleNet_InceptionV1_SSD300_Generic </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 1) </td><td align="left">FP16 </td><td align="left">true </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL1_FP16_ELU </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 1) </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">true </td></tr>
<tr>
<td align="left">2019R3_PV_PL1_FP16_MobileNet_Clamp </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 1) </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">true </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL1_FP16_ResNet_TinyYolo_YoloV3 </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 1) </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL1_FP16_RMNet </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 1) </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">true </td></tr>
<tr>
<td align="left">2019R3_PV_PL1_FP16_SqueezeNet_VGG </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 1) </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL2_FP11_AlexNet_GoogleNet_Generic </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2) </td><td align="left">FP11 </td><td align="left">true </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL2_FP11_InceptionV1_SqueezeNet </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2) </td><td align="left">FP11 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL2_FP11_MobileNet_TinyYolo_Clamp </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2) </td><td align="left">FP11 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">true </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL2_FP11_ResNet_VGG </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2) </td><td align="left">FP11 </td><td align="left">false </td><td align="left">false </td><td align="left">false </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL2_FP11_RMNet </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2) </td><td align="left">FP11 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">true </td></tr>
<tr>
<td align="left">2019R3_PV_PL2_FP11_SSD300 </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2) </td><td align="left">FP11 </td><td align="left">true </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL2_FP11_YoloV3_ELU </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2) </td><td align="left">FP11 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">true </td></tr>
<tr>
<td align="left">2019R3_PV_PL2_FP16_AlexNet_GoogleNet_InceptionV1_SSD300_Generic </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2) </td><td align="left">FP16 </td><td align="left">true </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL2_FP16_ELU </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2) </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">true </td></tr>
<tr>
<td align="left">2019R3_PV_PL2_FP16_MobileNet_Clamp </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2) </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">true </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL2_FP16_ResNet_TinyYolo_YoloV3 </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2) </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_PL2_FP16_RMNet </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2) </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">true </td></tr>
<tr>
<td align="left">2019R3_PV_PL2_FP16_SqueezeNet_VGG </td><td align="left">Intel&reg; Vision Accelerator Design with an Intel&reg; Arria&reg; 10 FPGA (Speed Grade 2) </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_RC_FP11_AlexNet_GoogleNet_Generic </td><td align="left">Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA </td><td align="left">FP11 </td><td align="left">true </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_RC_FP11_InceptionV1_ResNet_SqueezeNet_TinyYolo_VGG </td><td align="left">Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA </td><td align="left">FP11 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_RC_FP11_MobileNet_Clamp </td><td align="left">Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA </td><td align="left">FP11 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">true </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_RC_FP11_RMNet </td><td align="left">Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA </td><td align="left">FP11 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">true </td></tr>
<tr>
<td align="left">2019R3_PV_RC_FP11_Streaming </td><td align="left">Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA </td><td align="left">FP11 </td><td align="left">true </td><td align="left">false </td><td align="left">false </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_RC_FP11_Streaming_Slicing </td><td align="left">Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA </td><td align="left">FP11 </td><td align="left">true </td><td align="left">false </td><td align="left">false </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_RC_FP11_YoloV3_ELU </td><td align="left">Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA </td><td align="left">FP11 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">true </td></tr>
<tr>
<td align="left">2019R3_PV_RC_FP16_AlexNet_GoogleNet_Generic </td><td align="left">Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA </td><td align="left">FP16 </td><td align="left">true </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_RC_FP16_ELU </td><td align="left">Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">true </td></tr>
<tr>
<td align="left">2019R3_PV_RC_FP16_InceptionV1_VGG </td><td align="left">Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_RC_FP16_MobileNet_Clamp </td><td align="left">Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">true </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_RC_FP16_ResNet_YoloV3 </td><td align="left">Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
<tr>
<td align="left">2019R3_PV_RC_FP16_RMNet </td><td align="left">Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">true </td></tr>
<tr>
<td align="left">2019R3_PV_RC_FP16_SqueezeNet_TinyYolo </td><td align="left">Intel&reg; Programmable Acceleration Card with Intel&reg; Arria&reg; 10 GX FPGA </td><td align="left">FP16 </td><td align="left">false </td><td align="left">true </td><td align="left">true </td><td align="left">false </td><td align="left">false </td></tr>
</table>
<h2>Set Environment for Running the FPGA Plugin</h2>
<p>To make the FPGA plugin run directly or through the heterogeneous plugin, set up the environment:</p><ol type="1">
<li>Set up environment to access Intel&reg; FPGA RTE for OpenCL: <div class="fragment"><div class="line">source /opt/altera/aocl-pro-rte/aclrte-linux64/init_opencl.sh</div></div><!-- fragment --></li>
<li>Set additional environment variables for the FPGA plugin from the following table:</li>
</ol>
<table class="doxtable">
<tr>
<th align="left">Variable </th><th align="left">Setting  </th></tr>
<tr>
<td align="left">DLA_AOCX </td><td align="left">Path to the bitstream to the bitstream which can be programmed to the card. See section <a href="#TranslatingArchtoBitstream">Translation from Architecture to FPGA Bitstream Files</a> for choosing a bitstream for your chosen network and board.<br />
<br />
Try to avoid programming the bit stream during run time. Program the FPGA before.<br />
<br />
If you want to program the bitstream during run time, set <code>CL_CONTEXT_COMPILER_MODE_INTELFPGA=1</code> </td></tr>
<tr>
<td align="left">CL_CONTEXT_COMPILER_MODE_INTELFPGA </td><td align="left">To prevent the host application from programming the FPGA, set this variable to a value of 3.<br />
<br />
Program the bitstream in advance.<br />
<br />
Refer to the Program a Bitstream section in the <a class="el" href="_docs_install_guides_installing_openvino_linux_fpga.html">Installation Guide</a> </td></tr>
<tr>
<td align="left">ACL_PCIE_USE_JTAG_PROGRAMMING </td><td align="left">Set this variable to a value of 1 to force FPGA reprogramming using JTAG </td></tr>
</table>
<h2>Analyzing Heterogeneous Execution</h2>
<p>Besides generation of .dot files, you can use the error listening mechanism:</p>
<div class="fragment"><div class="line"><span class="keyword">class </span>FPGA_ErrorListener : <span class="keyword">public</span> <a class="code" href="classInferenceEngine_1_1IErrorListener.html">InferenceEngine::IErrorListener</a></div><div class="line">{</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="keyword">virtual</span> <span class="keywordtype">void</span> <a class="code" href="classInferenceEngine_1_1IErrorListener.html#a1ca803ff3a45ad8862e83a1b21ffa1e7">onError</a>(<span class="keyword">const</span> <span class="keywordtype">char</span> *msg) noexcept <span class="keyword">override</span> {</div><div class="line">        std::cout &lt;&lt; msg;</div><div class="line">    }</div><div class="line">};</div><div class="line">...</div><div class="line">FPGA_ErrorListener err_listener;</div><div class="line">core.<a class="code" href="classInferenceEngine_1_1Core.html#a0a109aae9705393b227f05ae3a408b12">SetLogCallback</a>(err_listener); <span class="comment">// will be used for FPGA device as well</span></div></div><!-- fragment --><p> If during network loading some layers are decided to be executed on a fallback plugin, the following message is printed:</p>
<div class="fragment"><div class="line">Layer (Name: detection_out, Type: DetectionOutput) is not supported:</div><div class="line">    custom or unknown.</div><div class="line">    Has (3) sets of inputs, must be 1, or 2.</div><div class="line">    Input dimensions (2) should be 4.</div></div><!-- fragment --><h2>Multiple FPGA Devices Support</h2>
<p>The Inference Engine FPGA plugin provides an ability to load different networks on multiple FPGA devices. For example, to load two networks AlexNet and MobileNet v2 on two different FPGA devices, follow the steps below:</p>
<ol type="1">
<li>Program each FGPA device with a corresponding bitstream: <div class="fragment"><div class="line">aocl program acl0 2019R3_PV_PL1_FP16_AlexNet_GoogleNet_InceptionV1_SSD300_Generic.aocx</div></div><!-- fragment --> <div class="fragment"><div class="line">aocl program acl1 2019R3_PV_PL1_FP16_MobileNet_Clamp.aocx</div></div><!-- fragment --> For more information about bitstream programming instructions, refer to <a class="el" href="_docs_install_guides_installing_openvino_linux_fpga.html">Installation Guide for Linux* with Support for FPGA</a></li>
<li>All FPGA devices are enumerated with unique ID starting from <code>0</code>. By default, all networks are loaded to the default device with ID <code>0</code>. If you want to load a network on a particular non-default device, specify the <code>KEY_DEVICE_ID</code> parameter for C++ and <code>DEVICE_ID</code> parameter for Python*. The following code snippets demonstrates how to load the AlexNet network on the FPGA device with ID <code>0</code> and the MobileNet v2 network on the device with ID <code>1</code>:<ul>
<li>With C++: <div class="fragment"><div class="line"><span class="comment">// Load AlexNet network on the first FPGA device programmed with bitstream supporting AlexNet</span></div><div class="line"><a class="code" href="classInferenceEngine_1_1Core.html">InferenceEngine::Core</a> core;</div><div class="line"></div><div class="line">CNNNetReader reader1;</div><div class="line">reader1.ReadNetwork(<span class="stringliteral">&quot;alexnet.xml&quot;</span>);</div><div class="line">reader1.ReadWeights(<span class="stringliteral">&quot;alexnet.bin&quot;</span>);</div><div class="line"></div><div class="line"><span class="keyword">auto</span> exeNetwork1 = core.<a class="code" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff">LoadNetwork</a>(reader1.getNetwork(), <span class="stringliteral">&quot;FPGA.0&quot;</span>);</div><div class="line"></div><div class="line"><span class="comment">// Load MobileNet network on the second FPGA device programmed with MobileNet bitstream</span></div><div class="line"></div><div class="line">CNNNetReader reader2;</div><div class="line">reader2.ReadNetwork(<span class="stringliteral">&quot;mobilenet_v2.xml&quot;</span>);</div><div class="line">reader2.ReadWeights(<span class="stringliteral">&quot;mobilenet_v2.bin&quot;</span>);</div><div class="line"></div><div class="line"><span class="keyword">auto</span> exeNetwork2 = core.<a class="code" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff">LoadNetwork</a>(reader2.getNetwork(), <span class="stringliteral">&quot;FPGA&quot;</span>, { { <a class="code" href="namespaceInferenceEngine_1_1PluginConfigParams.html#a5f4163dbc2c805b6adcadb4b90033d0b">KEY_DEVICE_ID</a>, <span class="stringliteral">&quot;1&quot;</span> } });</div></div><!-- fragment --></li>
<li>With Python: <div class="fragment"><div class="line"># Load AlexNet network on the first FPGA device programmed with bitstream supporting AlexNet</div><div class="line">net1 = IENetwork(model=&quot;alexnet.xml&quot;, weights=&quot;alexnet.bin&quot;)</div><div class="line">plugin.load(network=net1, config={&quot;DEVICE_ID&quot;: &quot;0&quot;})</div><div class="line"></div><div class="line"># Load MobileNet network on the second FPGA device programmed with MobileNet bitstream</div><div class="line">net2 = IENetwork(model=&quot;mobilenet_v2.xml&quot;, weights=&quot;mobilenet_v2.bin&quot;)</div><div class="line">plugin.load(network=net2, config={&quot;DEVICE_ID&quot;: &quot;1&quot;})</div></div><!-- fragment --> Note that you have to use asynchronous infer requests to utilize several FPGA devices, otherwise the execution on devices is performed sequentially.</li>
</ul>
</li>
</ol>
<h2>Input Streaming on FPGA</h2>
<p>FPGA device can get input data for inference from inner channels if it is programmed with bitstream supporing so called "Input Streaming Feature". Input streaming can be enabled for networks that are loaded to such devices:</p>
<div class="fragment"><div class="line"><span class="comment">// Load network on the FPGA device programmed with bitstream that supports input streaming feature</span></div><div class="line"><a class="code" href="classInferenceEngine_1_1Core.html">InferenceEngine::Core</a> core;</div><div class="line"></div><div class="line">CNNNetReader reader;</div><div class="line">reader.ReadNetwork(<span class="stringliteral">&quot;alexnet.xml&quot;</span>);</div><div class="line">reader.ReadWeights(<span class="stringliteral">&quot;alexnet.bin&quot;</span>);</div><div class="line"></div><div class="line"><span class="keyword">auto</span> exeNetwork = core.<a class="code" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff">LoadNetwork</a>(reader.getNetwork(), <span class="stringliteral">&quot;FPGA&quot;</span>, { { DLIA_KEY_ENABLE_STREAMING, <span class="stringliteral">&quot;YES&quot;</span> } });</div></div><!-- fragment --><p>The exception is raised if you try to load network with enabled streaming on device without input streaming capability. You can get device capability using get <a class="el" href="classInferenceEngine_1_1Core.html#ada42cbc50479ccab13bd16d3c6eba885" title="Gets general runtime metric for dedicated hardware. The method is needed to request common device pro...">InferenceEngine::Core::GetMetric</a> API functions: </p><div class="fragment"><div class="line"><span class="comment">// Load network on the FPGA device programmed with bitstream that supports input streaming feature</span></div><div class="line"><a class="code" href="classInferenceEngine_1_1Core.html">InferenceEngine::Core</a> core;</div><div class="line"></div><div class="line">std::vector&lt;std::string&gt; capabilities = ie.<a class="code" href="classInferenceEngine_1_1Core.html#ada42cbc50479ccab13bd16d3c6eba885">GetMetric</a>(<span class="stringliteral">&quot;FPGA&quot;</span>, <a class="code" href="ie__plugin__config_8hpp.html#a69d0efa20c5b2bec020a706279f0c7be">METRIC_KEY</a>(OPTIMIZATION_CAPABILITIES));</div><div class="line"></div><div class="line"><span class="keywordtype">bool</span> deviceWithStreaming = (capabilities.end() != std::find(std::begin(capabilities), std::end(capabilities), <a class="code" href="dlia__config_8hpp.html#a984ffb2c10cb65116bbfea0f672b7e7f">DLIA_METRIC_VALUE</a>(<a class="code" href="namespaceInferenceEngine_1_1DliaMetrics.html#a26aa1d57e1a717517149aeb5f7029680">INPUT_STREAMING</a>)));</div><div class="line"></div><div class="line"><span class="keywordflow">if</span> (deviceWithStreaming) {</div><div class="line">    <span class="keyword">auto</span> exeNetwork = core.<a class="code" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff">LoadNetwork</a>(reader.getNetwork(), <span class="stringliteral">&quot;FPGA&quot;</span>, { { DLIA_KEY_ENABLE_STREAMING, <span class="stringliteral">&quot;YES&quot;</span> } });</div><div class="line">}</div></div><!-- fragment --><p>The exception is raised if you try to set input blob using <a class="el" href="classInferenceEngine_1_1InferRequest.html#a27fb179e3bae652d76076965fd2a5653" title="Sets input/output data to infer. ">InferenceEngine::InferRequest::SetBlob</a> for infer request for executable network loaded to the device with input streaming enabled:</p>
<div class="fragment"><div class="line"><span class="comment">// Create Infer request</span></div><div class="line"><span class="keyword">auto</span> inferRequest = exeNetwork.CreateInferRequest();</div><div class="line"></div><div class="line">inferRequest.SetBlob(<span class="stringliteral">&quot;Input Blob Name&quot;</span>, blob);  <span class="comment">//  Exception is raised here!</span></div></div><!-- fragment --><p>The content of the blob memory is ignored if you use the blob that is got from inference request using <a class="el" href="classInferenceEngine_1_1InferRequest.html#a9601a4cda3f309181af34feedf1b914c" title="Wraps original method IInferRequest::GetBlob. ">InferenceEngine::InferRequest::GetBlob</a>:</p>
<div class="fragment"><div class="line"><span class="comment">// Get input blob from infer request</span></div><div class="line"><span class="keyword">auto</span> blob = inferRequest.GetBlob(<span class="stringliteral">&quot;Input Blob Name&quot;</span>);</div><div class="line"></div><div class="line">fill_blob(blob);       <span class="comment">//  New content of the blob memory</span></div><div class="line">inferRequest.Infer();  <span class="comment">//  won&#39;t be used here</span></div></div><!-- fragment --><blockquote class="doxtable">
<p><b>NOTE</b>: Only networks with one input can be infered using bitstream with input streaming feature </p>
</blockquote>
<blockquote class="doxtable">
<p><b>NOTE</b>: Input streaming feature is compatible with HETERO:FPGA,CPU </p>
</blockquote>
<h2>How to Interpret Performance Counters</h2>
<p>As a result of collecting performance counters using <code><a class="el" href="classInferenceEngine_1_1InferRequest.html#a1aa33a2377a38ecf708abf352079154d" title="Wraps original method IInferRequest::GetPerformanceCounts. ">InferenceEngine::InferRequest::GetPerformanceCounts</a></code> you can find out performance data about execution on FPGA, pre-processing and post-processing data and data transferring from/to FPGA card.</p>
<p>If network is sliced to two parts that are executed on CPU, you can find performance data about Intel&reg; MKL-DNN kernels, their types, and other useful information.</p>
<h2>Limitations of the FPGA Support for CNN</h2>
<p>The Inference Engine FPGA plugin has limitations on network topologies, kernel parameters, and batch size.</p>
<ul>
<li>Depending on the bitstream loaded on the target device, the FPGA performs calculations with precision rates ranging from FP11 to FP16. This might have accuracy implications. Use the <a class="el" href="_tools_accuracy_checker_README.html">Accuracy Checker</a> to verify the network accuracy on the validation data set.</li>
<li>Networks that have many CNN layers that are not supported on FPGA stayed in topologies between supported layers might lead to dividing of graph to many subgraphs that might lead to <code>CL_OUT_OF_HOST_MEMORY</code> error. These topologies are not FPGA friendly for this release.</li>
<li>When you use the heterogeneous plugin, the affinity and distribution of nodes by devices depends on the FPGA bitstream that you use. Some layers might not be supported by a bitstream or parameters of the layer are not supported by the bitstream.</li>
</ul>
<h2>See Also</h2>
<ul>
<li><a class="el" href="_docs_IE_DG_supported_plugins_Supported_Devices.html">Supported Devices</a> </li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>