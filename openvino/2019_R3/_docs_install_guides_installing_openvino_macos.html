<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Install Intel® Distribution of OpenVINO™ toolkit for macOS* - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Install Intel® Distribution of OpenVINO™ toolkit for macOS* </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><blockquote class="doxtable">
<p><b>NOTES</b>:</p><ul>
<li>The Intel® Distribution of OpenVINO™ is supported on macOS* 10.14.4 or higher versions.</li>
<li>This installation has been validated on macOS 10.14.4.</li>
<li>An internet connection is required to follow the steps in this guide. If you have access to the Internet through the proxy server only, please make sure that it is configured in your OS environment. </li>
</ul>
</blockquote>
<h2>Introduction</h2>
<p>The Intel® Distribution of OpenVINO™ toolkit quickly deploys applications and solutions that emulate human vision. Based on Convolutional Neural Networks (CNN), the toolkit extends computer vision (CV) workloads across Intel® hardware, maximizing performance.</p>
<p>The Intel® Distribution of OpenVINO™ toolkit for macOS* includes the Intel® Deep Learning Deployment Toolkit (Intel® DLDT) and OpenCV* to deploy applications for accelerated inference on Intel® CPUs.</p>
<p>The Intel® Distribution of OpenVINO™ 2019 R3 toolkit for macOS*:</p>
<ul>
<li>Enables CNN-based deep learning inference on the edge</li>
<li>Supports heterogeneous execution across Intel® CPU and Intel® Neural Compute Stick 2 with Intel® Movidius™ VPUs</li>
<li>Speeds time-to-market via an easy-to-use library of computer vision functions and pre-optimized kernels</li>
<li>Includes optimized calls for computer vision standards including OpenCV*</li>
</ul>
<p><b>Included with the Installation</b></p>
<p>The following components are installed by default:</p>
<table class="doxtable">
<tr>
<th align="left">Component </th><th align="left">Description  </th></tr>
<tr>
<td align="left"><a class="el" href="_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html">Model Optimizer</a> </td><td align="left">This tool imports, converts, and optimizes models, which were trained in popular frameworks, to a format usable by Intel tools, especially the Inference Engine. <br />
 Popular frameworks include Caffe*, TensorFlow*, MXNet*, and ONNX*. </td></tr>
<tr>
<td align="left"><a class="el" href="_docs_IE_DG_inference_engine_intro.html">Inference Engine</a> </td><td align="left">This is the engine that runs a deep learning model. It includes a set of libraries for an easy inference integration into your applications. </td></tr>
<tr>
<td align="left"><a href="https://docs.opencv.org/master/">OpenCV*</a> </td><td align="left">OpenCV* community version compiled for Intel® hardware </td></tr>
<tr>
<td align="left"><a class="el" href="_docs_IE_DG_Samples_Overview.html">Sample Applications</a> </td><td align="left">A set of simple console applications demonstrating how to use the Inference Engine in your applications. </td></tr>
<tr>
<td align="left"><a class="el" href="_demos_README.html">Demos</a> </td><td align="left">A set of console applications that demonstrate how you can use the Inference Engine in your applications to solve specific use-cases </td></tr>
<tr>
<td align="left"><a class="el" href="_docs_IE_DG_Tools_Overview.html">Additional Tools</a> </td><td align="left">A set of tools to work with your models </td></tr>
<tr>
<td align="left"><a class="el" href="_models_intel_index.html">Documentation for Pre-Trained Models </a> </td><td align="left">Documentation for the pre-trained models available in the <a href="https://github.com/opencv/open_model_zoo">Open Model Zoo repo</a> </td></tr>
</table>
<h2>Development and Target Platform</h2>
<p>The development and target platforms have the same requirements, but you can select different components during the installation, based on your intended use.</p>
<p><b>Hardware</b></p>
<blockquote class="doxtable">
<p><b>NOTE</b>: The current version of the Intel® Distribution of OpenVINO™ toolkit for macOS* supports inference on Intel CPUs and Intel® Neural Compute Sticks 2 only. </p>
</blockquote>
<ul>
<li>6th-10th Generation Intel® Core™</li>
<li>Intel® Xeon® v5 family</li>
<li>Intel® Xeon® v6 family</li>
<li>Intel® Neural Compute Stick 2</li>
</ul>
<p><b>Software Requirements</b></p>
<ul>
<li>CMake 3.4 or higher</li>
<li>Python 3.5 or higher</li>
<li>Apple Xcode* Command Line Tools</li>
<li>(Optional) Apple Xcode* IDE (not required for OpenVINO, but useful for development)</li>
<li>(Optional) Homebrew* (required for installation of a library for Intel® Neural Compute Stick 2)</li>
</ul>
<p><b>Operating Systems</b></p>
<ul>
<li>macOS* 10.14.4</li>
</ul>
<h2>Overview</h2>
<p>This guide provides step-by-step instructions on how to install the Intel® Distribution of OpenVINO™ 2019 R1.1 toolkit for macOS*.</p>
<p>The following steps will be covered:</p>
<ol type="1">
<li><a href="#Install-Core">Install the Intel® Distribution of OpenVINO™ Toolkit </a>.</li>
<li><a href="#set-the-environment-variables">Set the OpenVINO environment variables and (optional) Update to <code>.bash_profile</code></a>.</li>
<li><a href="#configure-the-model-optimizer">Configure the Model Optimizer</a>.</li>
<li><a href="#Run-Demos">Run verification scripts to verify installation and compile samples</a>.</li>
</ol>
<h2><a class="anchor" id="Install-Core"></a>Install the Intel® Distribution of OpenVINO™ toolkit Core Components</h2>
<p>If you have a previous version of the Intel® Distribution of OpenVINO™ toolkit installed, rename or delete these two directories:</p>
<ul>
<li><code>/home/&lt;user&gt;/inference_engine_samples</code></li>
<li><code>/home/&lt;user&gt;/openvino_models</code></li>
</ul>
<p><a href="https://software.intel.com/en-us/openvino-toolkit/choose-download/free-download-macos">Download the latest version of OpenVINO toolkit for macOS*</a> then return to this guide to proceed with the installation.</p>
<p>Install the OpenVINO toolkit core components:</p>
<ol type="1">
<li>Go to the directory in which you downloaded the Intel® Distribution of OpenVINO™ toolkit. This document assumes this is your <code>Downloads</code> directory. By default, the disk image file is saved as <code>m_openvino_toolkit_p_&lt;version&gt;.dmg</code>.</li>
<li>Double-click the <code>m_openvino_toolkit_p_&lt;version&gt;.dmg</code> file to mount. The disk image is mounted to <code>/Volumes/m_openvino_toolkit_p_&lt;version&gt;</code> and automatically opened in a separate window.</li>
<li>Run the installation wizard application <code>m_openvino_toolkit_p_&lt;version&gt;.app</code></li>
<li><p class="startli">On the <b>User Selection</b> screen, choose a user account for the installation:</p><ul>
<li>Root</li>
<li>Administrator</li>
<li>Current user</li>
</ul>
<div class="image">
<img src="openvino-install-macos-01.png" alt="openvino-install-macos-01.png"/>
</div>
<p class="startli">The default installation directory path depends on the privileges you choose for the installation.</p>
</li>
<li>Click <b>Next</b> and follow the instructions on your screen.</li>
<li>If you are missing external dependencies, you will see a warning screen. Take note of any dependencies you are missing. After installing the Intel® Distribution of OpenVINO™ toolkit core components, you will need to install the missing dependencies. For example, the screen example below indicates you are missing two dependencies: <div class="image">
<img src="openvino-install-macos-02.png" alt="openvino-install-macos-02.png"/>
</div>
</li>
<li>Click <b>Next</b>.</li>
<li>The <b>Installation summary</b> screen shows you the default component set to install: <div class="image">
<img src="openvino-install-macos-03.png" alt="openvino-install-macos-03.png"/>
</div>
<ul>
<li><p class="startli">If you used <b>root</b> or <b>administrator</b> privileges to run the installer, it installs the OpenVINO toolkit to <code>/opt/intel/openvino_&lt;version&gt;/</code></p>
<p class="startli">For simplicity, a symbolic link to the latest installation is also created: <code>/opt/intel/openvino/</code></p>
</li>
<li><p class="startli">If you used <b>regular user</b> privileges to run the installer, it installs the OpenVINO toolkit to <code>/home/&lt;user&gt;/intel/openvino_&lt;version&gt;/</code></p>
<p class="startli">For simplicity, a symbolic link to the latest installation is also created: <code>/home/&lt;user&gt;/intel/openvino/</code></p>
</li>
</ul>
</li>
<li><p class="startli">If needed, click <b>Customize</b> to change the installation directory or the components you want to install: </p><div class="image">
<img src="openvino-install-macos-04.png" alt="openvino-install-macos-04.png"/>
</div>
<p class="startli">Click <b>Next</b> to save the installation options and show the Installation summary screen.</p>
</li>
<li>On the <b>Installation summary</b> screen, press <b>Install</b> to begin the installation.</li>
<li>When the first part of installation is complete, the final screen informs you that the core components have been installed and additional steps still required: <div class="image">
<img src="openvino-install-macos-05.png" alt="openvino-install-macos-05.png"/>
</div>
</li>
<li>Click <b>Finish</b> to close the installation wizard. A new browser window opens to the next section of the Installation Guide to set the environment variables. If the installation did not indicate you must install dependencies, you can move ahead to <a href="#set-the-environment-variables">Set the Environment Variables</a>. If you received a message that you were missing external software dependencies, listed under <b>Software Requirements</b> at the top of this guide, you need to install them now before continuing on to the next section.</li>
</ol>
<h2><a class="anchor" id="set-the-environment-variables"></a>Set the Environment Variables</h2>
<p>You need to update several environment variables before you can compile and run OpenVINO™ applications. Open the macOS Terminal* or a command-line interface shell you prefer and run the following script to temporarily set your environment variables:</p>
<div class="fragment"><div class="line">source /opt/intel/openvino/bin/setupvars.sh</div></div><!-- fragment --><p><b>Optional</b>: The OpenVINO environment variables are removed when you close the shell. You can permanently set the environment variables as follows:</p>
<ol type="1">
<li>Open the <code>.bash_profile</code> file in the current user home directory: <div class="fragment"><div class="line">vi ~/.bash_profile</div></div><!-- fragment --></li>
<li>Press the <b>i</b> key to switch to the insert mode.</li>
<li>Add this line to the end of the file: <div class="fragment"><div class="line">source /opt/intel/openvino/bin/setupvars.sh</div></div><!-- fragment --></li>
</ol>
<ol type="1">
<li>Save and close the file: press the <b>Esc</b> key, type <code>:wq</code> and press the <b>Enter</b> key.</li>
<li>To verify your change, open a new terminal. You will see <code>[setupvars.sh] OpenVINO environment initialized</code>.</li>
</ol>
<p>The environment variables are set. Continue to the next section to configure the Model Optimizer.</p>
<h2><a class="anchor" id="configure-the-model-optimizer"></a>Configure the Model Optimizer</h2>
<p>The Model Optimizer is a Python*-based command line tool for importing trained models from popular deep learning frameworks such as Caffe*, TensorFlow*, Apache MXNet*, ONNX* and Kaldi*.</p>
<p>The Model Optimizer is a key component of the OpenVINO toolkit. You cannot perform inference on your trained model without running the model through the Model Optimizer. When you run a pre-trained model through the Model Optimizer, your output is an Intermediate Representation (IR) of the network. The IR is a pair of files that describe the whole model:</p>
<ul>
<li><code>.xml</code>: Describes the network topology</li>
<li><code>.bin</code>: Contains the weights and biases binary data</li>
</ul>
<p>The Inference Engine reads, loads, and infers the IR files, using a common API on the CPU hardware.</p>
<p>For more information about the Model Optimizer, see the <a class="el" href="_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html">Model Optimizer Developer Guide</a>.</p>
<h3>Model Optimizer Configuration Steps</h3>
<p>You can choose to either configure the Model Optimizer for all supported frameworks at once, <b>OR</b> for one framework at a time. Choose the option that best suits your needs. If you see error messages, verify that you installed all dependencies listed under <b>Software Requirements</b> at the top of this guide.</p>
<blockquote class="doxtable">
<p><b>NOTE</b>: If you installed OpenVINO to a non-default installation directory, replace <code>/opt/intel/</code> with the directory where you installed the software. </p>
</blockquote>
<p><b>Option 1: Configure the Model Optimizer for all supported frameworks at the same time:</b></p>
<ol type="1">
<li>Go to the Model Optimizer prerequisites directory: <div class="fragment"><div class="line">cd /opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites</div></div><!-- fragment --></li>
<li>Run the script to configure the Model Optimizer for Caffe, TensorFlow, MXNet, Kaldi*, and ONNX: <div class="fragment"><div class="line">sudo ./install_prerequisites.sh</div></div><!-- fragment --></li>
</ol>
<p><b>Option 2: Configure the Model Optimizer for each framework separately:</b></p>
<p>Configure individual frameworks separately <b>ONLY</b> if you did not select <b>Option 1</b> above.</p>
<ol type="1">
<li>Go to the Model Optimizer prerequisites directory: <div class="fragment"><div class="line">cd /opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites</div></div><!-- fragment --></li>
<li>Run the script for your model framework. You can run more than one script:<ul>
<li>For <b>Caffe</b>: <div class="fragment"><div class="line">sudo ./install_prerequisites_caffe.sh</div></div><!-- fragment --></li>
<li>For <b>TensorFlow</b>: <div class="fragment"><div class="line">sudo ./install_prerequisites_tf.sh</div></div><!-- fragment --></li>
<li>For <b>MXNet</b>: <div class="fragment"><div class="line">sudo ./install_prerequisites_mxnet.sh</div></div><!-- fragment --></li>
<li>For <b>ONNX</b>: <div class="fragment"><div class="line">sudo ./install_prerequisites_onnx.sh</div></div><!-- fragment --></li>
<li>For <b>Kaldi</b>: <div class="fragment"><div class="line">sudo ./install_prerequisites_kaldi.sh</div></div><!-- fragment --></li>
</ul>
</li>
</ol>
<p>The Model Optimizer is configured for one or more frameworks.</p>
<p>You are ready to verify the installation by <a href="#Run-Demos">running the verification scripts</a>.</p>
<h2><a class="anchor" id="Run-Demos"></a>Run the Verification Scripts to Verify Installation and Compile Samples</h2>
<blockquote class="doxtable">
<p><b>NOTES</b>:</p><ul>
<li>The steps shown here assume you used the default installation directory to install the OpenVINO toolkit. If you installed the software to a directory other than <code>/opt/intel/</code>, update the directory path with the location where you installed the toolkit.</li>
<li>If you installed the product as a root user, you must switch to the root mode before you continue: <code>sudo -i</code>. </li>
</ul>
</blockquote>
<p>To verify the installation and compile two Inference Engine samples, run the verification applications provided with the product on the CPU:</p>
<h3>Run the Image Classification Verification Script</h3>
<ol type="1">
<li>Go to the <b>Inference Engine demo</b> directory: <div class="fragment"><div class="line">cd /opt/intel/openvino/deployment_tools/demo</div></div><!-- fragment --></li>
<li>Run the <b>Image Classification verification script</b>: <div class="fragment"><div class="line">./demo_squeezenet_download_convert_run.sh</div></div><!-- fragment --></li>
</ol>
<p>The Image Classification verification script downloads a public SqueezeNet Caffe* model and runs the Model Optimizer to convert the model to <code>.bin</code> and <code>.xml</code> Intermediate Representation (IR) files. The Inference Engine requires this model conversion so it can use the IR as input and achieve optimum performance on Intel hardware.</p>
<p>This verification script creates the directory <code>/home/&lt;user&gt;/inference_engine_samples/</code>, builds the <a class="el" href="_inference_engine_samples_classification_sample_async_README.html">Image Classification Sample</a> application and runs with the model IR and <code>car.png</code> image located in the <code>demo</code> directory. When the verification script completes, you will have the label and confidence for the top-10 categories:</p>
<div class="image">
<img src="image_classification_script_output_lnx.png" alt="image_classification_script_output_lnx.png"/>
</div>
<p>For a brief description of the Intermediate Representation <code>.bin</code> and <code>.xml</code> files, see <a href="#configure-the-model-optimizer">Configuring the Model Optimizer</a>.</p>
<p>This script is complete. Continue to the next section to run the Inference Pipeline verification script.</p>
<h3>Run the Inference Pipeline Verification Script</h3>
<ol type="1">
<li>While still in <code>/opt/intel/openvino/deployment_tools/demo/</code>, run the Inference Pipeline verification script: <div class="fragment"><div class="line">./demo_security_barrier_camera.sh</div></div><!-- fragment --> This verification script downloads three pre-trained model IRs, builds the <a class="el" href="_demos_security_barrier_camera_demo_README.html">Security Barrier Camera Demo</a> application and runs it with the downloaded models and the <code>car_1.bmp</code> image from the <code>demo</code> directory to show an inference pipeline. The verification script uses vehicle recognition in which vehicle attributes build on each other to narrow in on a specific attribute.</li>
</ol>
<p>First, an object is identified as a vehicle. This identification is used as input to the next model, which identifies specific vehicle attributes, including the license plate. Finally, the attributes identified as the license plate are used as input to the third model, which recognizes specific characters in the license plate.</p>
<p>When the verification script completes, you will see an image that displays the resulting frame with detections rendered as bounding boxes, and text: </p><div class="image">
<img src="inference_pipeline_script_mac.png" alt="inference_pipeline_script_mac.png"/>
</div>
<ol type="1">
<li>Close the image viewer screen to end the demo.</li>
</ol>
<p><b>Congratulations</b>, you have completed the Intel® Distribution of OpenVINO™ 2019 R1.1 installation for macOS. To learn more about what you can do with the Intel® Distribution of OpenVINO™ toolkit, see the additional resources provided below.</p>
<h2><a class="anchor" id="additional-NCS2-steps"></a>Steps for Intel® Neural Compute Stick 2</h2>
<p>These steps are only required if you want to perform inference on Intel® Neural Compute Stick 2 powered by the Intel® Movidius™ Myriad™ X VPU. See also the <a href="https://software.intel.com/en-us/neural-compute-stick/get-started">Get Started page for Intel® Neural Compute Stick 2</a>.</p>
<p>To perform inference on Intel® Neural Compute Stick 2, install the <code>libusb</code> library by running the following command in the Homebrew* command manager: </p><div class="fragment"><div class="line">brew install libusb</div></div><!-- fragment --><blockquote class="doxtable">
<p><b>NOTE</b>: If you do not have Homebrew on your machine, install it by running the following command: <code>/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"</code></p>
<p>For details, refer to the <a href="https://brew.sh/">Homebrew documentation</a>. </p>
</blockquote>
<h2><a class="anchor" id="Hello-World-Tutorial"></a>Hello World Tutorials</h2>
<p>Visit the Intel Distribution of OpenVINO Toolkit <a href="https://github.com/intel-iot-devkit/inference-tutorials-generic/tree/openvino_toolkit_r3_0">Inference Tutorials for Face Detection and Car Detection Exercises</a></p>
<h2>Additional Resources</h2>
<ul>
<li>To learn more about the verification applications, see <code>README.txt</code> in <code>/opt/intel/openvino/deployment_tools/demo/</code>.</li>
<li>For detailed description of the pre-trained models, go to the <a class="el" href="_models_intel_index.html">Overview of OpenVINO toolkit Pre-Trained Models</a> page.</li>
<li>More information on <a class="el" href="_docs_IE_DG_Samples_Overview.html">sample applications</a>.</li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html">Convert Your Caffe* Model</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html">Convert Your TensorFlow* Model</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_MxNet.html">Convert Your MXNet* Model</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html">Convert Your ONNX* Model</a></li>
<li><a href="https://software.intel.com/en-us/openvino-toolkit">Intel Distribution of OpenVINO Toolkit home page</a></li>
<li><a href="https://docs.openvinotoolkit.org">Intel Distribution of OpenVINO Toolkit documentation</a> </li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>