<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Offloading Sub-Graph Inference to TensorFlow* - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Offloading Sub-Graph Inference to TensorFlow* </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Model Optimizer cannot generate an Intermediate Representation from unsupported TensorFlow* operations, as is the case with some custom layers. However, you can still successfully create an Intermediate Representation if you offload the unsupported operations to TensorFlow for computation. To support this scenario, you must build Inference Engine custom layer with the TensorFlow C++ runtime. The layer that offloads computations to TensorFlow has type <code>TFCustomSubgraphCall</code> in the Inference Engine Intermediate Representation and contains a TensorFlow graph protobuf string describing nodes being offloaded.</p>
<p><b>Limitations:</b></p>
<ul>
<li>You can only offload operations to TensorFlow from a Linux* OS computer.</li>
<li>The custom layer supports inference only on a CPU, not on Intel&reg; Integrated Graphics or on Intel&reg; FPGA.</li>
<li>The Inference Engine uses NCHW layout for tensors, but TensorFlow uses usually NHWC. Model Optimizer performs conversion between these layouts to correctly infer the model. Model Optimizer adds transpose operations to convert sub-graph 4D input tensors from NCHW layout to NHWC and vice versa for the output nodes. These operations are embedded in the protobuf string that describes the TensorFlow sub-graph in the Intermediate Representation <code>.xml</code> file. Sometimes this approach fails. For example, the offload convolution to TensorFlow fails, because the convolution layout weights in TensorFlow do not correspond to the layout weights in the Inference Engine. However, offloading convolution nodes plus nodes with weights succeeds, because the node with weights is a part of offloaded sub-graph, so there are no transposes for the weights tensor. The successful nodes are usually of type <code>Const</code>.</li>
</ul>
<h2>Model Optimizer CLI Options</h2>
<p>Two command-line options are available to offload part of the inference to TensorFlow.</p>
<blockquote class="doxtable">
<p><b>NOTE</b>: Use the command-line options on the line with the command: </p><div class="fragment"><div class="line">python3 mo.py --input_model model-file.pb</div></div><!-- fragment --> </blockquote>
<ul>
<li>Use node name patterns to offload a sub-graph of operations, using the command-line option: <div class="fragment"><div class="line">-- tensorflow_subgraph_patterns</div></div><!-- fragment --> This option uses a comma-separated list of regular expressions to match node names. This offload has two primary characteristics:<ul>
<li>All nodes that match a specific regular expression are merged into a single Inference Engine node that TensorFlow* executes.</li>
<li>All patterns are applied independently, which means two nodes that match two different patterns are not merged into one node.â€‹ For example, the option <code>--tensorflow_subgraph_patterns "Scope_1/.*,Scope_2.*"</code> is merged with all nodes whose names start from <code>Scope_1/</code> to a new node, and all nodes whose names start from <code>Scope_2</code> are merged to a different node.</li>
</ul>
</li>
<li>Offload specific types of operations, using the command-line option: <div class="fragment"><div class="line">--tensorflow_operation_patterns</div></div><!-- fragment --> This option specifies a comma-separated list of regular expressions to match node types. This offload has a primary characteristic: all nodes that match a specific regular expression are merged into a single Inference Engine node that TensorFlow executes. For example, the following command offloads all operations of type <code>Concat</code>, <code>ConcatV2</code>, <code>Add</code>, and <code>BiasAdd</code> to Tensorflow: <div class="fragment"><div class="line">--tensorflow_operation_patterns &quot;Concat.*,.*Add&quot;</div></div><!-- fragment --></li>
<li><b>[ Unavailable ]</b> Offload all unsupported operations automatically. The option <code>--tensorflow_operation_patterns</code> is <b>not available anymore</b>. Instead, use the command-line parameter <code>--tensorflow_operation_patterns</code> and specify types of unsupported operations separated by commas. To determine the operations, run the Model Optimizer with the input model file and input/output node names (if necessary). If the Model Optimizer detects unsupported operations, it prints the following error message with the list of unsupported operations: <div class="fragment"><div class="line">[ ERROR ]  List of operations that cannot be converted to Inference Engine IR:</div><div class="line">[ ERROR ]      &lt;type_of_unsupported_operation&gt; (1)</div><div class="line">[ ERROR ]          &lt;name_of_unsupported_operation&gt;</div><div class="line">[ ERROR ]  Part of the nodes was not converted to IR. Stopped.</div></div><!-- fragment --> To offload unsupported operations to TensorFlow, run the following command-line option with the type of an unsupported operation specified: <div class="fragment"><div class="line">--tensorflow_operation_patterns &lt;type_of_unsupported_operation&gt;</div></div><!-- fragment --></li>
</ul>
<p>You can use all two options by issuing the commands in this order: </p><div class="fragment"><div class="line">python3 mo.py --input_model model-file.pb --tensorflow_subgraph_patterns</div><div class="line">python3 mo.py --input_model model-file.pb --tensorflow_operation_patterns</div></div><!-- fragment --><h2>How to Build a Custom Layer to Offload Computations to TensorFlow* <a class="anchor" id="how-to-build-customlayer-offload-computations-tensorflow"></a></h2>
<p>Your system should meet the requirements needed to build the TensorFlow from the sources described in the following <a href="https://www.tensorflow.org/install/install_sources">page</a>. The Tensorflow version 1.2, 1.3.1, 1.4, 1.5, 1.6 and 1.7 are supported.</p>
<blockquote class="doxtable">
<p><b>NOTE</b>: You need to perform this step only once. </p>
</blockquote>
<ol type="1">
<li>Clone the TensorFlow Git repository.</li>
<li>Set the environment variable <code>TF_ROOT_DIR</code> to point to the cloned directory: <div class="fragment"><div class="line">export TF_ROOT_DIR=&lt;TENSORFLOW_DIR&gt;</div></div><!-- fragment --></li>
<li>Set the OpenVINO toolkit environment variables running the <code>setupvars.sh</code> script: <div class="fragment"><div class="line">source &lt;INSTALL_DIR&gt;/bin/setupvars.sh</div></div><!-- fragment --></li>
<li>Build an Inference Engine layer with TensorFlow runtime. This might take about 20 minutes: <div class="fragment"><div class="line">./tf_call_ie_layer/build.sh.</div></div><!-- fragment --></li>
<li>A shared library is generated: <div class="fragment"><div class="line">$&lt;TF_ROOT_DIR&gt;/bazel-bin/tensorflow/cc/inference_engine_layer/libtensorflow_call_layer.so</div></div><!-- fragment --> This library is the Inference Engine custom layer, which is used to offload inference to TensorFlow.</li>
</ol>
<h2>How to Run a Model with Operations Offloaded to TensorFlow*</h2>
<ol type="1">
<li>Compile <code>extensibility_sample</code></li>
<li>Run <code>extensibility_sample</code>: <div class="fragment"><div class="line">./extensibility_sample -i &lt;path_to_image_file&gt; -m &lt;path_to_IR.xml&gt; -d CPU -l &lt;path_to_libtensorflow_call_layer.so&gt;</div></div><!-- fragment --> </li>
</ol>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>