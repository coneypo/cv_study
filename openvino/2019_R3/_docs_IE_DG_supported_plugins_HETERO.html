<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Heterogeneous Plugin - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Heterogeneous Plugin </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Introducing Heterogeneous Plugin</h2>
<p>The heterogeneous plugin enables computing for inference on one network on several devices. Purposes to execute networks in heterogeneous mode</p><ul>
<li>To utilize accelerators power and calculate heaviest parts of network on accelerator and execute not supported layers on fallback devices like CPU</li>
<li>To utilize all available hardware more efficiently during one inference</li>
</ul>
<p>The execution through heterogeneous plugin can be divided to two independent steps:</p><ul>
<li>Setting of affinity to layers (binding them to devices in <code><a class="el" href="classInferenceEngine_1_1ICNNNetwork.html" title="This is the main interface to describe the NN topology. ">InferenceEngine::ICNNNetwork</a></code>)</li>
<li>Loading a network to the Heterogeneous plugin, splitting the network to parts, and executing them through the plugin</li>
</ul>
<p>These steps are decoupled. The setting of affinity can be done automatically using fallback policy or in manual mode.</p>
<p>The fallback automatic policy means greedy behavior and assigns all layers which can be executed on certain device on that device follow priorities.</p>
<p>Some of the topologies are not friendly to heterogeneous execution on some devices or cannot be executed in such mode at all. Example of such networks might be networks having activation layers which are not supported on primary device. If transmitting of data from one part of network to another part in heterogeneous mode takes relatively much time, then it is not much sense to execute them in heterogeneous mode on these devices. In this case you can define heaviest part manually and set affinity thus way to avoid sending of data back and forth many times during one inference.</p>
<h2>Annotation of Layers per Device and Default Fallback Policy</h2>
<p>Default fallback policy decides which layer goes to which device automatically according to the support in dedicated plugins (FPGA,GPU,CPU,MYRIAD).</p>
<p>Another way to annotate a network is setting affinity manually using <code>CNNLayer::affinity</code> field. This field accepts string values of devices like "CPU" or "FPGA".</p>
<p>The fallback policy does not work if even one layer has an initialized affinity. The sequence should be calling of automating affinity settings and then fix manually. </p><div class="fragment"><div class="line"><a class="code" href="classInferenceEngine_1_1CNNNetReader.html">InferenceEngine::CNNNetReader</a> reader;</div><div class="line">reader.<a class="code" href="classInferenceEngine_1_1CNNNetReader.html#a7ef33e831fd01cd773a6e335cd6e57ea">ReadNetwork</a>(<span class="stringliteral">&quot;Model.xml&quot;</span>);</div><div class="line">reader.<a class="code" href="classInferenceEngine_1_1CNNNetReader.html#af6839d68366c11eda4d0fab23be3391c">ReadWeights</a>(<span class="stringliteral">&quot;Model.bin&quot;</span>);</div><div class="line"><span class="keyword">auto</span> network = reader.<a class="code" href="classInferenceEngine_1_1CNNNetReader.html#a1e3e65597e0426a672ce85d2bcffdb41">getNetwork</a>();</div><div class="line"></div><div class="line"><span class="comment">// This example demonstrates how to perform default affinity initialization and then</span></div><div class="line"><span class="comment">// correct affinity manually for some layers</span></div><div class="line"><a class="code" href="classInferenceEngine_1_1Core.html">InferenceEngine::Core</a> core;</div><div class="line"><span class="keyword">const</span> std::string device = <span class="stringliteral">&quot;HETERO:FPGA,CPU&quot;</span>;</div><div class="line"></div><div class="line"><span class="comment">// QueryNetworkResult object contains map layer -&gt; device</span></div><div class="line"><a class="code" href="structInferenceEngine_1_1QueryNetworkResult.html">InferenceEngine::QueryNetworkResult</a> res = core.<a class="code" href="classInferenceEngine_1_1Core.html#a4cf4df5766f45bdabd5d9dc85cc82e4b">QueryNetwork</a>(network, device, { });</div><div class="line"></div><div class="line"><span class="comment">// update default affinities</span></div><div class="line">res.<a class="code" href="structInferenceEngine_1_1QueryNetworkResult.html#aff431e5d7451f364dee1c1c54ca78333">supportedLayersMap</a>[<span class="stringliteral">&quot;layerName&quot;</span>] = <span class="stringliteral">&quot;CPU&quot;</span>;</div><div class="line"></div><div class="line"><span class="comment">// set affinities to network</span></div><div class="line"><span class="keywordflow">for</span> (<span class="keyword">auto</span> &amp;&amp; layer : res.<a class="code" href="structInferenceEngine_1_1QueryNetworkResult.html#aff431e5d7451f364dee1c1c54ca78333">supportedLayersMap</a>) {</div><div class="line">    network.getLayerByName(layer-&gt;first)-&gt;affinity = layer-&gt;second;</div><div class="line">}</div><div class="line"></div><div class="line"><span class="comment">// load network with affinities set before</span></div><div class="line"><span class="keyword">auto</span> executable_network = core.<a class="code" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff">LoadNetwork</a>(network, device);</div></div><!-- fragment --><p>If you rely on the default affinity distribution, you can avoid calling <code><a class="el" href="classInferenceEngine_1_1Core.html#a4cf4df5766f45bdabd5d9dc85cc82e4b" title="Query device if it supports specified network with specified configuration. ">InferenceEngine::Core::QueryNetwork</a></code> and just call <code><a class="el" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff" title="Creates an executable network from a network object. Users can create as many networks as they need a...">InferenceEngine::Core::LoadNetwork</a></code> instead: </p><div class="fragment"><div class="line"><a class="code" href="classInferenceEngine_1_1CNNNetReader.html">InferenceEngine::CNNNetReader</a> reader;</div><div class="line">reader.<a class="code" href="classInferenceEngine_1_1CNNNetReader.html#a7ef33e831fd01cd773a6e335cd6e57ea">ReadNetwork</a>(<span class="stringliteral">&quot;Model.xml&quot;</span>);</div><div class="line">reader.<a class="code" href="classInferenceEngine_1_1CNNNetReader.html#af6839d68366c11eda4d0fab23be3391c">ReadWeights</a>(<span class="stringliteral">&quot;Model.bin&quot;</span>);</div><div class="line"></div><div class="line"><a class="code" href="classInferenceEngine_1_1Core.html">InferenceEngine::Core</a> core;</div><div class="line"><span class="keyword">auto</span> executable_network = core.<a class="code" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff">LoadNetwork</a>(reader.<a class="code" href="classInferenceEngine_1_1CNNNetReader.html#a1e3e65597e0426a672ce85d2bcffdb41">getNetwork</a>(), <span class="stringliteral">&quot;HETERO:FPGA,CPU&quot;</span>);</div></div><!-- fragment --><h2>Details of Splitting Network and Execution</h2>
<p>During loading of the network to heterogeneous plugin, network is divided to separate parts and loaded to dedicated plugins. Intermediate blobs between these sub graphs are allocated automatically in the most efficient way.</p>
<h2>Execution Precision</h2>
<p>Precision for inference in heterogeneous plugin is defined by</p><ul>
<li>Precision of IR.</li>
<li>Ability of final plugins to execute in precision defined in IR</li>
</ul>
<p>Examples:</p><ul>
<li>If you want to execute GPU with CPU fallback with FP16 on GPU, you need to use only FP16 IR. Weight are converted from FP16 to FP32 automatically for execution on CPU by heterogeneous plugin automatically.</li>
<li>If you want to execute on FPGA with CPU fallback, you can use any precision for IR. The execution on FPGA is defined by bitstream, the execution on CPU happens in FP32.</li>
</ul>
<p>Samples can be used with the following command:</p>
<div class="fragment"><div class="line">./object_detection_sample_ssd -m  &lt;path_to_model&gt;/ModelSSD.xml -i &lt;path_to_pictures&gt;/picture.jpg -d HETERO:FPGA,CPU</div></div><!-- fragment --><p> where:</p><ul>
<li><code>HETERO</code> stands for heterogeneous plugin</li>
<li><code>FPGA,CPU</code> points to fallback policy with priority on FPGA and fallback to CPU</li>
</ul>
<p>You can point more than two devices: <code>-d HETERO:FPGA,GPU,CPU</code></p>
<h2>Analyzing Heterogeneous Execution</h2>
<p>After enabling of <code>KEY_HETERO_DUMP_GRAPH_DOT</code> config key, you can dump GraphViz* <code>.dot</code> files with annotations of devices per layer.</p>
<p>Heterogeneous plugin can generate two files:</p><ul>
<li><code>hetero_affinity_&lt;network name&gt;.dot</code> - annotation of affinities per layer. This file is written to the disk only if default fallback policy was executed</li>
<li><code>hetero_subgraphs_&lt;network name&gt;.dot</code> - annotation of affinities per graph. This file is written to the disk during execution of <code>ICNNNetwork::LoadNetwork()</code> for heterogeneous plugin</li>
</ul>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;<a class="code" href="ie__plugin__config_8hpp.html">ie_plugin_config.hpp</a>&quot;</span></div><div class="line"><span class="preprocessor">#include &quot;<a class="code" href="hetero__plugin__config_8hpp.html">hetero/hetero_plugin_config.hpp</a>&quot;</span></div><div class="line"><span class="keyword">using namespace </span><a class="code" href="namespaceInferenceEngine_1_1PluginConfigParams.html">InferenceEngine::PluginConfigParams</a>;</div><div class="line"><span class="keyword">using namespace </span><a class="code" href="namespaceInferenceEngine_1_1HeteroConfigParams.html">InferenceEngine::HeteroConfigParams</a>;</div><div class="line"></div><div class="line">...</div><div class="line">InferenceEngine::Core core;</div><div class="line">core.<a class="code" href="classInferenceEngine_1_1Core.html#a268e2d24595061e9d804460cc6ca9ad3">SetConfig</a>({ { KEY_HETERO_DUMP_GRAPH_DOT, <a class="code" href="namespaceInferenceEngine_1_1PluginConfigParams.html#a42d48631fa3332ded8c776513e897bf3">YES</a> } }, <span class="stringliteral">&quot;HETERO&quot;</span>);</div></div><!-- fragment --><p>You can use GraphViz* utility or converters to <code>.png</code> formats. On Ubuntu* operating system, you can use the following utilities:</p><ul>
<li><code>sudo apt-get install xdot</code></li>
<li><code>xdot hetero_subgraphs.dot</code></li>
</ul>
<p>You can use performance data (in samples, it is an option <code>-pc</code>) to get performance data on each subgraph.</p>
<p>Here is an example of the output: for Googlenet v1 running on FPGA with fallback to CPU: </p><div class="fragment"><div class="line">subgraph1: 1. input preprocessing (mean data/FPGA):EXECUTED       layerType:                    realTime: 129        cpu: 129            execType:</div><div class="line">subgraph1: 2. input transfer to DDR:EXECUTED       layerType:                    realTime: 201        cpu: 0              execType:</div><div class="line">subgraph1: 3. FPGA execute time:EXECUTED       layerType:                    realTime: 3808       cpu: 0              execType:</div><div class="line">subgraph1: 4. output transfer from DDR:EXECUTED       layerType:                    realTime: 55         cpu: 0              execType:</div><div class="line">subgraph1: 5. FPGA output postprocessing:EXECUTED       layerType:                    realTime: 7          cpu: 7              execType:</div><div class="line">subgraph1: 6. copy to IE blob:EXECUTED       layerType:                    realTime: 2          cpu: 2              execType:</div><div class="line">subgraph2: out_prob:          NOT_RUN        layerType: Output             realTime: 0          cpu: 0              execType: unknown</div><div class="line">subgraph2: prob:              EXECUTED       layerType: SoftMax            realTime: 10         cpu: 10             execType: ref</div><div class="line">Total time: 4212     microseconds</div></div><!-- fragment --> <h2>See Also</h2>
<ul>
<li><a class="el" href="_docs_IE_DG_supported_plugins_Supported_Devices.html">Supported Devices</a> </li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>