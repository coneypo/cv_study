<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>How to configure OpenVINO™ launcher - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">How to configure OpenVINO™ launcher </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>For enabling OpenVINO™ launcher you need to add <code>framework: dlsdk</code> in launchers section of your configuration file and provide following parameters:</p>
<ul>
<li><code>device</code> - specifies which device will be used for infer. Supported: <code>CPU</code>, <code>GPU</code>, <code>FPGA</code>, <code>MYRIAD</code>, Heterogeneous plugin as <code>HETERO:target_device,fallback_device</code> and Multi device plugin as <code>MULTI:target_device1,target_device2</code>. You are able to not specify device intently and provide one or several devices via <code>-td, --target devices</code> command line argument. Target device will be selected from command line (in turn when several devices provided, evaluations will be run one by one with all specified devices).</li>
<li><code>model</code> - path to xml file with Caffe model for your topology.</li>
<li><code>weights</code> - path to bin file with weights for your topology.</li>
</ul>
<p>launcher may optionally provide model parameters in source framework format which will be converted to Inference Engine IR using Model Optimizer. If you want to use Model Optimizer for model conversion, please view <a href="https://software.intel.com/en-us/articles/OpenVINO-ModelOptimizer">Model Optimizer Developer Guide</a>. You can provide:</p>
<ul>
<li><code>caffe_model</code> and <code>caffe_weights</code> for Caffe model and weights (*.prototxt and *.caffemodel).</li>
<li><code>tf_model</code> for TensorFlow model (*.pb, *.pb.frozen, *.pbtxt).</li>
<li><code>tf_meta</code> for TensorFlow MetaGraph (*.meta).</li>
<li><code>mxnet_weights</code> for MXNet params (*.params).</li>
<li><code>onnx_model</code> for ONNX model (*.onnx).</li>
<li><code>kaldi_model</code> for Kaldi model (*.nnet).</li>
</ul>
<p>In case when you want to determine additional parameters for model conversion (data_type, input_shape and so on), you can use <code>mo_params</code> for arguments with values and <code>mo_flags</code> for positional arguments like <code>legacy_mxnet_model</code> . Full list of supported parameters you can find in Model Optimizer Developer Guide.</p>
<p>Model will be converted before every evaluation. You can provide <code>converted_model_dir</code> for saving converted model in specific folder, otherwise, converted models will be saved in path provided via <code>-C</code> command line argument or source model directory.</p>
<ul>
<li><code>adapter</code> - approach how raw output will be converted to representation of dataset problem, some adapters can be specific to framework. You can find detailed instruction how to use adapters <a class="el" href="_tools_accuracy_checker_accuracy_checker_adapters_README.html">here</a>.</li>
</ul>
<p>Launcher understands which batch size will be used from model intermediate representation (IR). If you want to use batch for infer, please, provide model with required batch or convert it using specific parameter in <code>mo_params</code>.</p>
<ul>
<li><code>allow_reshape_input</code> - parameter, which allows to reshape input layer to data shape (default value is False).</li>
</ul>
<p>Additionally you can provide device specific parameters:</p>
<ul>
<li><code>cpu_extensions</code> (path to extension file with custom layers for cpu). You can also use special key <code>AUTO</code> for automatic search cpu extensions library in the provided as command line argument directory (option <code>-e, --extensions</code>)</li>
<li><code>gpu_extensions</code> (path to extension *.xml file with OpenCL kernel description for gpu).</li>
<li><code>bitstream</code> for running on FPGA.</li>
</ul>
<p>Beside that, you can launch model in <code>async_mode</code>, enable this option and provide the number of infer requests (<code>num_requests</code>), which will be used in evaluation process. For multi device configuration async mode used automatically. You can provide number requests for each device as part device specification: <code>MULTI:device_1(num_req_1),device_2(num_req_2)</code> or in <code>num_requests</code> config section (for this case comma-separated list of integer numbers or one value if number requests for all devices equal can be used).</p>
<h2>Specifying model inputs in config.</h2>
<p>In case when you model has several inputs you should provide list of input layers in launcher config section using key <code>inputs</code>. Each input description should has following info:</p><ul>
<li><code>name</code> - input layer name in network</li>
<li><code>type</code> - type of input values, it has impact on filling policy. Available options:<ul>
<li><code>CONST_INPUT</code> - input will be filled using constant provided in config. It also requires to provide <code>value</code>.</li>
<li><code>IMAGE_INFO</code> - specific key for setting information about input shape to layer (used in Faster RCNN based topologies). You do not need provide <code>value</code>, because it will be calculated in runtime. Format value is <code>Nx[H, W, S]</code>, where <code>N</code> is batch size, <code>H</code> - original image height, <code>W</code> - original image width, <code>S</code> - scale of original image (default 1).</li>
<li><code>INPUT</code> - network input for main data stream (e. g. images). If you have several data inputs, you should provide regular expression for identifier as <code>value</code> for specifying which one data should be provided in specific input. Optionally you can determine <code>shape</code> of input (actually does not used, DLSDK launcher uses info given from network) and <code>layout</code> in case when your model was trained with non-standard data layout (For DLSDK default layout is <code>NCHW</code>).</li>
</ul>
</li>
</ul>
<p>OpenVINO™ launcher config example:</p>
<div class="fragment"><div class="line">launchers:</div><div class="line">  - framework: dlsdk</div><div class="line">    device: HETERO:FPGA,CPU</div><div class="line">    caffe_model: path_to_model/alexnet.prototxt</div><div class="line">    caffe_weights: path_to_weights/alexnet.caffemodel</div><div class="line">    adapter: classification</div><div class="line">    mo_params:</div><div class="line">      batch: 4</div><div class="line">    mo_flags:</div><div class="line">      - reverse_input_channels</div><div class="line">    cpu_extensions: cpu_extentions_avx512.so</div></div><!-- fragment --> </div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>