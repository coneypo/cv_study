<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Introduction to Intel® Deep Learning Deployment Toolkit - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Introduction to Intel® Deep Learning Deployment Toolkit </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Deployment Challenges</h2>
<p>Deploying deep learning networks from the training environment to embedded platforms for inference might be a complex task that introduces a number of technical challenges that must be addressed:</p>
<ul>
<li>There are a number of deep learning frameworks widely used in the industry, such as Caffe*, TensorFlow*, MXNet*, Kaldi* etc.</li>
<li>Typically the training of the deep learning networks is performed in data centers or server farms while the inference might take place on embedded platforms, optimized for performance and power consumption. Such platforms are typically limited both from software perspective (programming languages, third party dependencies, memory consumption, supported operating systems), and from hardware perspective (different data types, limited power envelope), so usually it is not recommended (and sometimes just impossible) to use original training framework for inference. An alternative solution would be to use dedicated inference APIs that are well optimized for specific hardware platforms.</li>
<li>Additional complications of the deployment process include supporting various layer types and networks that are getting more and more complex. Obviously, ensuring the accuracy of the transforms networks is not trivial.</li>
</ul>
<h2>Deployment Workflow</h2>
<p>The process assumes that you have a network model trained using one of the <a href="#SupportedFW">supported frameworks</a>. The scheme below illustrates the typical workflow for deploying a trained deep learning model: </p><div class="image">
<img src="workflow_steps.png" alt="workflow_steps.png"/>
</div>
<p>The steps are:</p>
<ol type="1">
<li><a class="el" href="_docs_MO_DG_prepare_model_Config_Model_Optimizer.html">Configure Model Optimizer</a> for the specific framework (used to train your model).</li>
<li>Run <a href="#MO">Model Optimizer</a> to produce an optimized <a class="el" href="_docs_MO_DG_prepare_model_convert_model_IRLayersCatalogSpec.html">Intermediate Representation (IR)</a> of the model based on the trained network topology, weights and biases values, and other optional parameters.</li>
<li>Test the model in the IR format using the <a href="#IE">Inference Engine</a> in the target environment with provided <a class="el" href="_docs_IE_DG_Samples_Overview.html">Inference Engine sample applications</a>.</li>
<li><a class="el" href="_docs_IE_DG_Integrate_with_customer_application_new_API.html">Integrate Inference Engine</a> in your application to deploy the model in the target environment.</li>
</ol>
<h2>Model Optimizer <a class="anchor" id="MO"></a></h2>
<p>Model Optimizer is a cross-platform command line tool that facilitates the transition between the training and deployment environment, performs static model analysis and automatically adjusts deep learning models for optimal execution on end-point target devices.</p>
<p>Model Optimizer is designed to support multiple deep learning <a href="#SupportedFW">supported frameworks and formats</a>.</p>
<p>While running Model Optimizer you do not need to consider what target device you wish to use, the same output of the MO can be used in all targets.</p>
<h3>Model Optimizer Workflow</h3>
<p>The process assumes that you have a network model trained using one of the <a href="#SupportedFW">supported frameworks</a>. The Model Optimizer workflow can be described as following:</p>
<ul>
<li><a class="el" href="_docs_MO_DG_prepare_model_Config_Model_Optimizer.html">Configure Model Optimizer</a> for one of the supported deep learning framework that was used to train the model.</li>
<li>Provide as input a trained network that contains a certain network topology, and the adjusted weights and biases (with some optional parameters).</li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Converting_Model.html">Run Model Optimizer</a> to perform specific model optimizations (for example, horizontal fusion of certain network layers). Exact optimizations are framework-specific, refer to appropriate documentation pages: <a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html">Converting a Caffe Model</a>, <a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html">Converting a TensorFlow Model</a>, <a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_MxNet.html">Converting a MXNet Model</a>, <a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Kaldi.html">Converting a Kaldi Model</a>, <a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html">Converting an ONNX Model</a>.</li>
<li>Model Optimizer produces as output an <a class="el" href="_docs_MO_DG_prepare_model_convert_model_IRLayersCatalogSpec.html">Intermediate Representation (IR)</a> of the network which is used as an input for the Inference Engine on all targets. The IR is a pair of files that describe the whole model:<ul>
<li><code>.xml</code>: The topology file - an XML file that describes the network topology</li>
<li><code>.bin</code>: The trained data file - a .bin file that contains the weights and biases binary data</li>
</ul>
</li>
</ul>
<p>The Intermediate Representation (IR) files can be read, loaded and inferred with <a href="#IE">Inference Engine</a>. The Inference Engine API offers a unified API across a number of <a href="#SupportedTargets">supported Intel® platforms</a>.</p>
<h3>Supported Frameworks and Formats <a class="anchor" id="SupportedFW"></a></h3>
<ul>
<li>Caffe* (most public branches)</li>
<li>TensorFlow*</li>
<li>MXNet*</li>
<li>Kaldi*</li>
<li>ONNX*</li>
</ul>
<h3>Supported Models</h3>
<p>For the list of supported models refer to the framework or format specific page:</p><ul>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html">Supported Caffe* models</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html">Supported TensorFlow* models</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_MxNet.html">Supported MXNet* models</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html">Supported ONNX* models</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Kaldi.html">Supported Kaldi* models</a></li>
</ul>
<h2>Inference Engine <a class="anchor" id="IE"></a></h2>
<p>Inference Engine is a runtime that delivers a unified API to integrate the inference with application logic:</p>
<ul>
<li>Takes as input the model. The model presented in the specific form of <a class="el" href="_docs_MO_DG_prepare_model_convert_model_IRLayersCatalogSpec.html">Intermediate Representation (IR)</a> produced by Model Optimizer.</li>
<li>Optimizes inference execution for target hardware.</li>
<li>Delivers inference solution with reduced footprint on embedded inference platforms.</li>
</ul>
<p>The Inference Engine supports inference of multiple image classification networks, including AlexNet, GoogLeNet, VGG and ResNet families of networks, fully convolutional networks like FCN8 used for image segmentation, and object detection networks like Faster R-CNN.</p>
<p>For the full list of supported hardware, refer to the <a class="el" href="_docs_IE_DG_supported_plugins_Supported_Devices.html">Supported Devices</a> section.</p>
<p>The Inference Engine package contains <a href="files.html">headers</a>, runtime libraries, and <a class="el" href="_docs_IE_DG_Samples_Overview.html">sample console applications</a> demonstrating how you can use the Inference Engine in your applications.</p>
<h2>See Also</h2>
<ul>
<li><a class="el" href="_docs_IE_DG_Samples_Overview.html">Inference Engine Samples</a></li>
<li><a href="https://software.intel.com/en-us/computer-vision-sdk">Intel&reg; Deep Learning Deployment Toolkit Web Page</a></li>
</ul>
<h4>Optimization Notice</h4>
<p><sup>For complete information about compiler optimizations, see our <a href="https://software.intel.com/en-us/articles/optimization-notice#opt-en">Optimization Notice</a>.</sup> </p>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>