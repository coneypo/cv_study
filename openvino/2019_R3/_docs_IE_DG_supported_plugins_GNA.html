<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>GNA Plugin - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">GNA Plugin </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Introducing the GNA Plugin</h2>
<p>The GNA plugin was developed for low power scoring of neural networks on the Intel&reg; Speech Enabling Developer Kit, the Amazon Alexa* Premium Far-Field Developer Kit, Intel&reg; Pentium&reg; Silver processor J5005, Intel&reg; Celeron&reg; processor J4005, Intel&reg; Core&trade; i3-8121U processor, and others.</p>
<h2>Supported Frameworks <a class="anchor" id="supported-frameworks"></a></h2>
<p>The following frameworks have been tested in this release:</p>
<ul>
<li><b>Kaldi* framework Nnet recipes</b></li>
<li><b>Kaldi* framework Nnet2 recipes</b></li>
<li><b>TensorFlow* framework</b></li>
</ul>
<p>Refer to <a class="el" href="_docs_MO_DG_prepare_model_Supported_Frameworks_Layers.html">Supported Framework Layers </a> for the list of supported standard layers.</p>
<h2>BIOS, Library, and Drivers</h2>
<p>This release was tested on Intel&reg; NUC7CJYH with BIOS Update [JYGLKCPX.86A] Version: 0037, GNA library 01.00.00.1317 and driver 01.00.00.1310 (for Windows* and Linux*).</p>
<h2>Supported Configuration Parameters</h2>
<p>The plugin supports the configuration parameters listed below. The parameters are passed as <code>std::map&lt;std::string, std::string&gt;</code> on <code><a class="el" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff" title="Creates an executable network from a network object. Users can create as many networks as they need a...">InferenceEngine::Core::LoadNetwork</a></code>.</p>
<table class="doxtable">
<tr>
<th align="left">Parameter Name </th><th align="left">Parameter Values </th><th align="left">Default </th><th align="left">Description  </th></tr>
<tr>
<td align="left"><code>GNA_COMPACT_MODE</code> </td><td align="left"><code>YES</code>/<code>NO</code> </td><td align="left"><code>YES</code> </td><td align="left">Reuse I/O buffers to save space (makes debugging harder) </td></tr>
<tr>
<td align="left"><code>GNA_SCALE_FACTOR</code> </td><td align="left">FP32 number </td><td align="left">1.0 </td><td align="left">Scale factor to use for input quantization </td></tr>
<tr>
<td align="left"><code>KEY_GNA_DEVICE_MODE</code> </td><td align="left"><code>GNA_AUTO</code>/<code>GNA_HW</code>/<code>GNA_SW</code>/<code>GNA_SW_EXACT</code>/<code>GNA_SW_FP32</code> </td><td align="left"><code>GNA_AUTO</code> </td><td align="left">Execution mode (GNA and emulation modes) </td></tr>
<tr>
<td align="left"><code>KEY_GNA_FIRMWARE_MODEL_IMAGE</code> </td><td align="left">string </td><td align="left"><code>""</code> </td><td align="left">Name for embedded model binary dump file </td></tr>
<tr>
<td align="left"><code>KEY_GNA_PRECISION</code> </td><td align="left"><code>I16</code>/<code>I8</code> </td><td align="left"><code>I16</code> </td><td align="left">Hint to GNA plugin: preferred integer weight resolution for quantization </td></tr>
<tr>
<td align="left"><code>KEY_PERF_COUNT</code> </td><td align="left"><code>YES</code>/<code>NO</code> </td><td align="left"><code>NO</code> </td><td align="left">Turn on performance counters reporting </td></tr>
<tr>
<td align="left"><code>KEY_GNA_LIB_N_THREADS</code> </td><td align="left">1-127 integer number </td><td align="left">1 </td><td align="left">Sets the number of GNA accelerator library worker threads used for inference computation in software modes </td></tr>
</table>
<h2>How to Interpret Performance Counters</h2>
<p>As a result of collecting performance counters using <code><a class="el" href="classInferenceEngine_1_1IInferencePlugin.html#ae49088f17b7a3c48342ded0590f447d2" title="Queries performance measures per layer to get feedback of what is the most time consuming layer Note:...">InferenceEngine::IInferencePlugin::GetPerformanceCounts</a></code>, you can find various performance data about execution on GNA. Returned map stores a counter description as a key, counter value is stored in the field <code>realTime_uSec</code> of <code>InferenceEngineProfileInfo</code> structure. Current GNA implementation calculates counters for whole utterance scoring and does not provide "per layer" information. API allows to retrieve counter units in cycles, but they can be converted to seconds as follows:</p>
<div class="fragment"><div class="line">seconds = cycles/GNA frequency</div></div><!-- fragment --><p>Intel Core i3-8121U processor includes GNA with frequency 400MHz, and Intel Pentium Silver J5005 andÂ Intel Celeron J4005 processors - 200MHz.</p>
<p>Performance counters provided for the time being:</p>
<ul>
<li>Scoring request performance results<ul>
<li>number of total cycles spent on scoring in hardware (including compute and memory stall cycles)</li>
<li>number of stall cycles spent in hardware</li>
</ul>
</li>
</ul>
<h2>Multithreading Support in GNA Plugin</h2>
<p>The GNA plugin supports the following configuration parameters for multithreading management:</p>
<ul>
<li><p class="startli"><code>KEY_GNA_LIB_N_THREADS</code></p>
<p class="startli">By default, the GNA plugin uses one worker thread for inference computations. This parameter allows you to create up to 127 threads for software modes.</p>
</li>
</ul>
<blockquote class="doxtable">
<p><b>NOTE:</b> Multithreading mode does not guarantee the same computation order as the order of issuing. Additionally, in this case, software modes do not implement any serializations. </p>
</blockquote>
<h2>Network Batch Size</h2>
<p>The GNA plugin supports the processing of context-windowed speech frames in batches of 1-8 frames in one input blob using <code><a class="el" href="classInferenceEngine_1_1ICNNNetwork.html#abe649d332d99d5c6abda004cfe659ad1" title="Changes the inference batch size. ">InferenceEngine::ICNNNetwork::setBatchSize</a></code>. A big batch size value increases the speed of utterance processing. It is strongly recommended to use this network option if possible.</p>
<blockquote class="doxtable">
<p><b>NOTE</b>: For networks with convolutional and RNN/LSTM layers supported batch size equal to 1. </p>
</blockquote>
<h2>Compatibility with Heterogeneous Plugin</h2>
<p>Heterogeneous plugin was tested with GNA as the primary device and CPU as a secondary. For running inference of networks with layers unsupported by the GNA plugin (for example, Softmax), you can use the Heterogeneous plugin with the following configuration <code>HETERO:GNA,CPU</code>. For the list of supported networks, see the <a href="#supported-frameworks">Supported Frameworks</a>.</p>
<blockquote class="doxtable">
<p><b>NOTE:</b> Due to limitation of GNA backend library, heterogenous support limited to cases where in resulted sliced graph there only one subgraph scheduled to run on GNA_HW or GNA_SW devices. </p>
</blockquote>
<h2>See Also</h2>
<ul>
<li><a class="el" href="_docs_IE_DG_supported_plugins_Supported_Devices.html">Supported Devices</a> </li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>