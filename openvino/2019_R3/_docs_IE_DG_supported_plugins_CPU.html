<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>CPU Plugin - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">CPU Plugin </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Introducing CPU Plugin</h2>
<p>The CPU plugin was developed in order to provide opportunity for high performance scoring of neural networks on CPU, using the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN).</p>
<p>Currently, the CPU plugin uses Intel&reg; Threading Building Blocks (Intel&reg; TBB) in order to parallelize calculations. Please refer to the <a class="el" href="_docs_optimization_guide_dldt_optimization_guide.html">Optimization Guide</a> for associated performance considerations.</p>
<p>The set of supported layers can be expanded with <a class="el" href="_inference_engine_src_extension_README.html">the extensibility library</a>. To add a new layer in this library, you can use <a class="el" href="_docs_IE_DG_Integrate_your_kernels_into_IE.html">the extensibility mechanism</a>.</p>
<h2>Supported Platforms</h2>
<p>OpenVINO&trade; toolkit is officially supported and validated on the following platforms:</p>
<table class="doxtable">
<tr>
<th align="left">Host </th><th align="left">OS (64-bit)  </th></tr>
<tr>
<td align="left">Development </td><td align="left">Ubuntu* 16.04/CentOS* 7.4/MS Windows* 10 </td></tr>
<tr>
<td align="left">Target </td><td align="left">Ubuntu* 16.04/CentOS* 7.4/MS Windows* 10 </td></tr>
</table>
<p>The CPU Plugin supports inference on Intel&reg; Xeon&reg; with Intel&reg; AVX2 and AVX512, Intel&reg; Core&trade; Processors with Intel&reg; AVX2, Intel Atom&reg; Processors with Intel&reg; SSE.</p>
<p>You can use <code>-pc</code> the flag for samples if you would like to know which configuration is used by some layer. This flag shows execution statistics that you can use to get information about layer name, execution status, layer type, execution time and the type of the execution primitive.</p>
<h2>Internal CPU Plugin Optimizations</h2>
<p>CPU plugin supports several graph optimization algorithms:</p>
<ul>
<li><b>Merging of group convolutions.</b> It means that if a topology contains the following pipeline: <div class="image">
<img src="mkldnn_group_conv.png" alt="mkldnn_group_conv.png"/>
</div>
 CPU plugin will merge it into one Convolution with the group parameter (Convolutions should have the same parameters).</li>
<li><b>Fusing of Convolution with ReLU or ELU.</b> CPU plugin is fusing all Convolution with ReLU or ELU layers if these layers are located after the Convolution layer.</li>
<li><b>Removing power layer.</b> CPU plugin removes Power layer from topology if it has the following parameters: <b>power</b> = 1, <b>scale</b> = 1, <b>offset</b> = 0.</li>
<li><b>Fusing Convolution + Sum or Convolution + Sum + ReLU.</b> In order to improve performance, CPU plugin fuses the next structure: <div class="image">
<img src="mkldnn_conv_sum.png" alt="mkldnn_conv_sum.png"/>
</div>
 This fuse allows you to upgrade the graph to the following structure: <div class="image">
<img src="mkldnn_conv_sum_result.png" alt="mkldnn_conv_sum_result.png"/>
</div>
</li>
</ul>
<h2>Supported Configuration Parameters</h2>
<p>The plugin supports the configuration parameters listed below. All parameters must be set with the <code><a class="el" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff" title="Creates an executable network from a network object. Users can create as many networks as they need a...">InferenceEngine::Core::LoadNetwork()</a></code> method. Refer to the OpenVINO samples for usage examples: <a class="el" href="_inference_engine_samples_benchmark_app_README.html">Benchmark App</a>.</p>
<p>These are general options, also supported by other plugins:</p>
<table class="doxtable">
<tr>
<th align="left">Parameter name </th><th align="left">Parameter values </th><th align="left">Default </th><th align="left">Description  </th></tr>
<tr>
<td align="left">KEY_EXCLUSIVE_ASYNC_REQUESTS </td><td align="left">YES/NO </td><td align="left">NO </td><td align="left">Forces async requests (also from different executable networks) to execute serially. This prevents potential oversubscription </td></tr>
<tr>
<td align="left">KEY_PERF_COUNT </td><td align="left">YES/NO </td><td align="left">NO </td><td align="left">Enables gathering performance counters </td></tr>
</table>
<p>CPU-specific settings:</p>
<table class="doxtable">
<tr>
<th align="left">Parameter name </th><th align="left">Parameter values </th><th align="left">Default </th><th align="left">Description  </th></tr>
<tr>
<td align="left">KEY_CPU_THREADS_NUM </td><td align="left">positive integer values</td><td align="left">0 </td><td align="left">Specifies number of threads that CPU plugin should use for inference. Zero (default) means using all (logical) cores </td></tr>
<tr>
<td align="left">KEY_CPU_BIND_THREAD </td><td align="left">YES/NO </td><td align="left">YES </td><td align="left">Binds inference worker threads to CPU cores. The binding is usually performance friendly, especially in server scenarios. The option also limits number of OpenMP* or Intel(R) TBB threads to the number of hardware cores. </td></tr>
<tr>
<td align="left">KEY_CPU_THROUGHPUT_STREAMS </td><td align="left">KEY_CPU_THROUGHPUT_NUMA, KEY_CPU_THROUGHPUT_AUTO, or positive integer values</td><td align="left">1 </td><td align="left">Specifies number of CPU "execution" streams for the throughput mode. Upper bound for a number of inference requests that can be executed simulteneously. All available CPU cores are evenly distributed between the streams. The default value is 1, which implies latency-oriented behaviour with all available cores processing requests one by one.<br />
KEY_CPU_THROUGHPUT_NUMA creates as many streams as needed to accomodate NUMA and avoid associated penalties.<br />
KEY_CPU_THROUGHPUT_AUTO creates bare minimum of streams to improve the performance; this is the most portable option if you don't know how many cores your target machine has (and what would be the optimal number of streams). Notice that your application should provie enough parallel slack (e.g. run many inference requests) to leverage the throughput mode. <br />
 A positive integer value creates the requested number of streams. </td></tr>
</table>
<h2>See Also</h2>
<ul>
<li><a class="el" href="_docs_IE_DG_supported_plugins_Supported_Devices.html">Supported Devices</a> </li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>