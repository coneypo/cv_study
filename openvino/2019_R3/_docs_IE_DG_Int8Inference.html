<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Low-Precision 8-bit Integer Inference - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Low-Precision 8-bit Integer Inference </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Disclaimer</h2>
<p>Inference Engine with low-precision 8-bit integer inference is in a feature preview and requires the following prerequisites to be satisfied:</p><ul>
<li>Inference Engine <a class="el" href="_docs_IE_DG_supported_plugins_CPU.html">CPU Plugin</a> must be built with the Intel&reg; Math Kernel Library (Intel&reg; MKL) dependency. In the Intel Distribution of OpenVINO it is satisfied by default, this is mostly the requirement if you are using OpenVINO available in Open Source, because Open Source OpenVINO can be also built with OpenBLAS that is unacceptable if you want to use 8-bit integer inference.</li>
<li>Intel&reg; platforms that support at least one extension to x86 instruction set from the following list:<ul>
<li>Intel® Advanced Vector Extensions 512 (Intel® AVX-512)</li>
<li>Intel® Advanced Vector Extensions 2.0 (Intel® AVX2)</li>
<li>Intel® Streaming SIMD Extensions 4.2 (Intel® SSE4.2)</li>
</ul>
</li>
<li>A model must contain at least one activation layer of Rectified Linear Unit (ReLU) type. If this requirement is not satisfied, 8-bit inference is unavailable for this particular model in Inference Engine.</li>
</ul>
<p>The 8-bit inference feature was validated on the following topologies:</p><ul>
<li><b>Classification models:</b><ul>
<li>Caffe Inception v1, Inception v4</li>
<li>Caffe ResNet-50 v1, ResNet-101 v1</li>
<li>Caffe MobileNet</li>
<li>Caffe SqueezeNet v1.0, SqueezeNet v1.1</li>
<li>Caffe VGG16, VGG19</li>
<li>TensorFlow Inception v3, Inception v4, Inception ResNet v2</li>
<li>Caffe DenseNet-121, DenseNet-161, DenseNet-169, DenseNet-201</li>
</ul>
</li>
<li><b>Object detection models:</b><ul>
<li>Caffe SSD_SqueezeNet</li>
<li>Caffe SSD_MobileNet</li>
<li>Caffe SSD_Vgg16_300</li>
<li>TensorFlow SSD Mobilenet v1, SSD Mobilenet v2</li>
</ul>
</li>
<li><b>Semantic segmentation models:</b><ul>
<li>Unet2D</li>
</ul>
</li>
<li><b>Recommendation system models:</b><ul>
<li>NCF</li>
</ul>
</li>
</ul>
<h2>Introduction</h2>
<p>A lot of investigation was made in the field of deep learning with the idea of using low precision computations during inference in order to boost deep learning pipelines and gather higher performance. For example, one of the popular approaches is to shrink the precision of activations and weights values from <code>fp32</code> precision to smaller ones, for example, to <code>fp11</code> or <code>int8</code>. For more information about this approach, refer to <b>Brief History of Lower Precision in Deep Learning</b> section in <a href="https://software.intel.com/en-us/articles/lower-numerical-precision-deep-learning-inference-and-training">this whitepaper</a>.</p>
<p>8-bit computations (referred to as <code>int8</code>) offer better performance compared to the results of inference in higher precision (for example, <code>fp32</code>), because they allow to load more data into a single processor instruction. Usually the cost for significant boost is a reduced accuracy. However, it is proved that the drop in accuracy can be negligible and depends on task requirements, so that the application engineer can set up the maximum accuracy drop that is acceptable.</p>
<p>Current Inference Engine solution for low-precision inference uses Intel MKL-DNN, which supports inference of the following layers in 8-bit integer computation mode:</p><ul>
<li>Convolution</li>
<li>FullyConnected (AVX-512 only)</li>
<li>ReLU</li>
<li>ReLU6</li>
<li>Pooling</li>
<li>Eltwise</li>
<li>Concat</li>
<li>Resample</li>
</ul>
<p>This means that 8-bit inference can only be performed with the CPU plugin on the layers listed above. All other layers are executed in the format supported by the CPU plugin: 32-bit floating point format (<code>fp32</code>).</p>
<h2>Low-Precision 8-bit Integer Inference Workflow</h2>
<p>For 8-bit integer computations, the original model (or its Intermediate Representation) must be in the <code>fp32</code> format. In order to perform calculation of layers in the <code>int8</code> format, the input data (input blob) and weights of the given layer (also biases and/or other blobs of the layer) must be quantized - transitioned from <code>fp32</code> to <code>int8</code> format. The quantization process converts model input into a lower-precision format. The precision and accuracy factors are specified by the scale and rounding-mode respectively. Read more about mathematical computations under the hood in the <a href="https://intel.github.io/mkl-dnn/ex_int8_simplenet.html">white paper</a>.</p>
<p>8-bit inference pipeline includes two stages (also refer to the figure below):</p><ol type="1">
<li><em>Offline stage</em>, or <em>model calibration</em>. During this stage, scale factors and execution profiles are defined for each layer in a way that low-precision accuracy drop for 8-bit integer inference satisfies the specified threshold. The output of this stage is a calibrated model.</li>
<li><em>Run-time stage</em>. This stage is an internal procedure of the <a class="el" href="_docs_IE_DG_supported_plugins_CPU.html">CPU Plugin</a>. During this stage, the calibrated model is loaded to the plugin. For each layer that obtain the corresponding execution profile, the plugin normalizes the weights (and biases, if present). It also adds scale factors at the particular places of the model defined by the internal algorithm with regards to the maximum performance and minimum number of extra layout manipulations.</li>
</ol>
<div class="image">
<img src="cpu_int8_flow.png" alt="cpu_int8_flow.png"/>
</div>
<h3>Offline Stage: Model Calibration</h3>
<p>One of the vital components for successful data quantization is a set of scale factors for each layer that supports 8-bit computations. These scales are obtained from statistics of layers activations collected by the OpenVINO <a class="el" href="_inference_engine_tools_calibration_tool_README.html">Calibration Tool</a> on a calibration dataset. The calibration dataset contains images and can be a subset of the validation set. A small fraction of images from validation dataset (1-5%) is enough to create a calibration dataset. For more information on the dataset preparation, refer to the <a class="el" href="_tools_accuracy_checker_README.html">Accuracy Checker Tool</a>.</p>
<blockquote class="doxtable">
<p><b>NOTE</b>: OpenVINO 2019 R1 release introduced a <a class="el" href="_inference_engine_tools_calibration_tool_README.html">Python* version of the Calibration Tool</a>. The C++ version of the Calibration Tool is deprecated. </p>
</blockquote>
<p>To calibrate a model, the Calibration Tool preforms the following steps:</p>
<ol type="1">
<li><em>Collecting layer statistics</em> (minimum and maximum values of layers activations) and baseline of accuracy metric for <code>fp32</code> inference. Note that accuracy metric depends on the type of the calibrated model. For classification networks, <b>top-1</b> metric is used; for object detection models, <b>mAP</b> metric is used.</li>
<li><em>Collecting accuracy metric</em> for 8-bit inference. During this step, different filters are applied to the collected activations statistics to remove activation outliers (isolated values that are very different from the majority of known values). If the resulting accuracy satisfies the required level with respect to the accepted accuracy drop delta, the Calibration Tool stops the calibration process.</li>
<li><em>Collecting accuracy drop</em> information on the calibration dataset for each layer that supports 8-bit computations using the <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation#Normalized_root-mean-square_deviation">Normalized Root-Mean-Square Deviation</a> metric. This metric allows to put all layers in decreasing order so that it is clear which layers bring the biggest accuracy drop.</li>
<li><em>Eliminating layers with the largest accuracy drop</em> from 8-bit computation by switching them back to <code>fp32</code> mode. After eliminating one layer, the Calibration Tool computes the accuracy of this configuration. Until the resulting accuracy satisfies the required level with respect to the accepted accuracy drop delta (which equals 1% by default), the tool continues switching layers back to <code>fp32</code> computations in the order defined in the step 3. However, calibration of the model with all layers returned to <code>fp32</code> computations is meaningless, so that this plays a role of hard stop of the whole calibration process.</li>
</ol>
<p>When the calibration completes, the tool writes the resulting statistics and the modified Intermediate Representation (IR) to the <code>.xml</code> file. The tool does not change the IR structure, so the layers hierarchy is the same. However, the layers that are chosen to be executed in 8-bit format are marked with the appropriate profile attribute, and their statistics is stored at the end of the <code>.xml</code> file.</p>
<p>When you pass the calibrated IR to the <a class="el" href="_docs_IE_DG_supported_plugins_CPU.html">CPU plugin</a>, the plugin automatically recognizes it as calibrated and performs the 8-bit inference. At the same time, other plugins do not support 8-bit inference, so if you pass the calibrated model to them, statistics and additional attributes are ignored and the model is inferred in the precision that this plugin supports.</p>
<h3>Run-Time Stage: Quantization</h3>
<p>This is the second stage of the 8-bit integer inference. After you load the calibrated model IR to the CPU plugin, it performs quantization for 8-bit inference:</p><ul>
<li>Inserts the corresponding scale factors to transform layer inputs precision to unsigned <code>int8</code> data type and normalize output layers to unsigned 8-bit integer type, to signed 8-bit integer type, or to 32-bit floating data type</li>
<li>Normalizes the weights of convolution layers to fit the signed 8-bit integer data type</li>
<li>Normalizes the biases of convolution layers to fit the signed 32-bit integer data type</li>
</ul>
<h2>Performance Counters</h2>
<p>Information about layer precision is stored in the performance counters that are available from the Inference Engine API. The layers have the following marks:</p><ul>
<li>Suffix <code>I8</code> for layers that had 8-bit data type input and were computed in 8-bit precision</li>
<li>Suffix <code>FP32</code> for layers computed in 32-bit precision</li>
</ul>
<p>For example, the performance counters table for the Inception model can look as follows:</p>
<div class="fragment"><div class="line">inception_5b/5x5_reduce       EXECUTED       layerType: Convolution        realTime: 417        cpu: 417            execType: gemm_blas_I8</div><div class="line">inception_5b/output           EXECUTED       layerType: Concat             realTime: 34         cpu: 34             execType: ref_I8</div><div class="line">inception_5b/output_U8_nhw... EXECUTED       layerType: Reorder            realTime: 33092      cpu: 33092          execType: reorder_I8</div><div class="line">inception_5b/output_oScale... EXECUTED       layerType: ScaleShift         realTime: 1390       cpu: 1390           execType: jit_avx2_FP32</div><div class="line">inception_5b/output_oScale... EXECUTED       layerType: Reorder            realTime: 143        cpu: 143            execType: reorder_FP32</div><div class="line">inception_5b/pool             EXECUTED       layerType: Pooling            realTime: 59301      cpu: 59301          execType: ref_any_I8</div></div><!-- fragment --><p>The <code>execType</code> column of the table includes inference primitives with specific suffixes.</p>
<h2>See Also</h2>
<ul>
<li><a class="el" href="_inference_engine_tools_calibration_tool_README.html">Calibration Tool</a> </li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>