<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Introduction to the Performance Topics - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Introduction to the Performance Topics </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This section is a shorter version of the <a class="el" href="_docs_optimization_guide_dldt_optimization_guide.html">Optimization Guide</a> for the Intel Deep Learning Deployment Toolkit.</p>
<h2>Precision</h2>
<p>Inference precision directly affects the performance.</p>
<p>Model Optimizer can produce an IR with different precision. For example, float16 IR initially targets VPU and GPU devices, while for example the CPU can also able to execute regular float32. Also, further device-specific inference precision settings are available, e.g. <a class="el" href="_docs_IE_DG_Int8Inference.html">8-bit integer inference on the CPU</a>. Notice that for <a class="el" href="_docs_IE_DG_supported_plugins_MULTI.html">MULTI device</a> that supports automatic inference on multiple devices in parallel, you can use the FP16 IR. More information, such as preferred data types for specific devices can also be found in the <a class="el" href="_docs_IE_DG_supported_plugins_Supported_Devices.html">Supported Devices</a> section.</p>
<h2>Latency vs. Throughput</h2>
<p>One way to increase computational efficiency is batching, which combines many (potentially tens) of input images to achieve optimal throughput. However, high batch size also comes with a latency penalty. So, for more real-time oriented usages, lower batch sizes (as low as a single input) are used. Refer to the <a class="el" href="_inference_engine_samples_benchmark_app_README.html">Benchmark App</a> sample, which allows latency vs. throughput measuring.</p>
<h2>Using Async API</h2>
<p>To gain better performance on accelerators, such as VPU or FPGA, the Inference Engine uses the asynchronous approach (see <a class="el" href="_docs_IE_DG_Integrate_with_customer_application_new_API.html">Integrating Inference Engine in Your Application (current API)</a>). The point is amortizing the costs of data transfers, by pipe-lining, see <a class="el" href="_demos_object_detection_demo_ssd_async_README.html">Async API explained</a>. Since the pipe-lining relies on the availability of the parallel slack, running multiple inference requests in parallel is essential. Refer to the <a class="el" href="_inference_engine_samples_benchmark_app_README.html">Benchmark App</a> sample, which enables running a number of inference requests in parallel. Specifying different number of request produces different throughput measurements.</p>
<h2>Best Latency on the Multi-Socket CPUs</h2>
<p>Notice that when latency is of concern, there are additional tips for multi-socket systems. When input is limited to the single image, the only way to achieve the best latency, is to limit execution to the single socket. The reason is that single image is simply not enough to saturate more than one socket. Also NUMA overheads might dominate the execution time. Below is example command-line that limits the execution to the single socket using numactl for the best <em>latency</em> value (assuming the machine with 28 phys cores per socket): </p><div class="fragment"><div class="line">limited to the single socket).</div><div class="line">$ numactl -m 0 --physcpubind 0-27  benchmark_app -m &lt;model.xml&gt; -api sync -nthreads 28</div></div><!-- fragment --><p> Notice that if you have more than one input, running as many inference requests as you have NUMA nodes (or sockets), usually gives the same best latency as single request on the single socket, but much higher throughput. Assuming 2 NUMA nodes machine: </p><div class="fragment"><div class="line">$ benchmark_app -m &lt;model.xml&gt; -nstreams 2</div></div><!-- fragment --><p> Number of NUMA nodes on the machine can be queried via 'lscpu'.</p>
<h2>Throughput Mode for CPU</h2>
<p>Unlike most accelerators, CPU is perceived as an inherently latency-oriented device. Since 2018 R5 release, the Inference Engine introduced the "throughput" mode, which allows the Inference Engine to efficiently run multiple inference requests on the CPU simultaneously, greatly improving the throughput.</p>
<p>Internally, the execution resources are split/pinned into execution "streams". Using this feature gains much better performance for the networks that originally are not scaled well with a number of threads (for example, lightweight topologies). This is especially pronounced for the many-core server machines.</p>
<p>Run the <a class="el" href="_inference_engine_samples_benchmark_app_README.html">Benchmark App</a> and play with number of infer requests running in parallel. Try different values of the <code>-nstreams</code> argument from <code>1</code> to a number of CPU cores and find one that provides the best performance. For example, on a 8-core CPU, compare the <code>-nstreams 1</code> (which is a latency-oriented scenario) to the <code>2</code>, <code>4</code> and <code>8</code> requests.</p>
<p>In addition to the number of streams, it is also possible to play with the batch size to find the throughput sweet-spot.</p>
<p>The throughput mode relaxes the requirement to saturate the CPU by using a large batch: running multiple independent inference requests in parallel often gives much better performance, than using a batch only. This allows you to simplify the app-logic, as you don't need to combine multiple inputs into a batch to achieve good CPU performance. Instead, it is possible to keep a separate infer request per camera or another source of input and process the requests in parallel using Async API.</p>
<h2>Benchmark App</h2>
<p><a class="el" href="_inference_engine_samples_benchmark_app_README.html">Benchmark App</a> sample is the best performance reference. It has a lot of device-specific knobs, but the primary usage is as simple as: </p><div class="fragment"><div class="line">$ ./benchmark_app –d GPU –m &lt;model&gt; -i &lt;input&gt;</div></div><!-- fragment --><p> to measure the performance of the model on the GPU. Or </p><div class="fragment"><div class="line">$ ./benchmark_app –d CPU –m &lt;model&gt; -i &lt;input&gt;</div></div><!-- fragment --><p> to execute on the CPU instead.</p>
<p>For example, for the CPU throughput mode from the previous section, you can play with number of streams (<code>-nstreams</code> command-line param). Try different values of the <code>-nstreams</code> argument from <code>1</code> to a number of CPU cores and find one that provides the best performance. For example, on a 8-core CPU, compare the <code>-nstreams 1</code> (which is a latency-oriented scenario) to the <code>2</code>, <code>4</code> and <code>8</code> streams. Notice that <code>benchmark_app</code> automatically queries/creates/runs number of requests required to saturate the given number of streams.</p>
<p>Finally, notice that when you don't specify number of streams with <code>-nstreams</code>, "AUTO" value for the streams is used, e.g. for the CPU this is <a class="el" href="_docs_IE_DG_supported_plugins_CPU.html">CPU_THROUGHPUT_AUTO</a>. You can spot the actual value behind "AUTO" for your machine in the application output. Notice that the "AUTO" number is not necessarily most optimal, so it is generally recommended to play either with the benchmark_app's "-nstreams" as described above, or via <a class="el" href="_docs_Workbench_DG_Introduction.html">new Workbench tool</a>.This allows you to simplify the app-logic, as you don't need to combine multiple inputs into a batch to achieve good CPU performance. Instead, it is possible to keep a separate infer request per camera or another source of input and process the requests in parallel using Async API.</p>
<h2>Kernels Tuning for GPU</h2>
<p>GPU backend comes with a feature, that allows models tuning, so the workload is configured to fit better into hardware.</p>
<p>Tuning is time consuming process, which internally execute every layer several (or even hundreds) times to find most performant configuration.</p>
<p>This configuration is saved into json-formatted file, whose name can be passed as plugin param to network. GPU backend will process this data to configure kernels for the best performance.</p>
<p>For more details about Kernels Tuning and How-To please refer to <a class="el" href="_docs_IE_DG_GPU_Kernels_Tuning.html">GPU Kernels Tuning</a>. </p>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>