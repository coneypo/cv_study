<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>GPU Plugin - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">GPU Plugin </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The GPU plugin uses the Intel&reg; Compute Library for Deep Neural Networks (<a href="https://01.org/cldnn">clDNN</a>) to infer deep neural networks. clDNN is an open source performance library for Deep Learning (DL) applications intended for acceleration of Deep Learning Inference on Intel&reg; Processor Graphics including Intel&reg; HD Graphics and Intel&reg; Iris&reg; Graphics. For an in-depth description of clDNN, see: <a href="https://github.com/intel/clDNN">cLDNN sources</a> and <a href="https://software.intel.com/en-us/articles/accelerating-deep-learning-inference-with-intel-processor-graphics">Accelerate Deep Learning Inference with Intel&reg; Processor Graphics</a>.</p>
<h2>Optimizations</h2>
<p>The plugin supports the following optimizations:</p><ul>
<li>Fused layers:<ul>
<li>Convolution - Activation</li>
<li>Deconvolution - Activation</li>
<li>Eltwise - Activation</li>
<li>Fully Connected - Activation</li>
</ul>
</li>
<li>Layers optimized out when conditions allow:<ul>
<li>Crop</li>
<li>Concatenate</li>
<li>Reshape</li>
<li>Flatten</li>
<li>Split</li>
<li>Copy</li>
</ul>
</li>
<li>Layers executed during load time (not during inference)<ul>
<li>PriorBox</li>
</ul>
</li>
</ul>
<h2>CPU Executed Layers</h2>
<p>The following layers are not accelerated on the GPU and executed on the host CPU instead:</p><ul>
<li>Proposal</li>
<li>SimplerNMS</li>
<li>PriorBox</li>
<li>DetectionOutput</li>
</ul>
<h2>Known Layers Limitations</h2>
<ul>
<li>ROIPooling is supported for 'max' value of 'method' attribute.</li>
</ul>
<h2>Supported Configuration Parameters</h2>
<p>The plugin supports the configuration parameters listed below. All parameters must be set before calling <code><a class="el" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff" title="Creates an executable network from a network object. Users can create as many networks as they need a...">InferenceEngine::Core::LoadNetwork()</a></code> in order to take effect.</p>
<table class="doxtable">
<tr>
<th>Parameter Name </th><th>Parameter Values </th><th>Default </th><th>Description  </th></tr>
<tr>
<td><code>KEY_PERF_COUNT</code> </td><td><code>YES</code> / <code>NO</code> </td><td><code>NO</code> </td><td>Collect performance counters during inference </td></tr>
<tr>
<td><code>KEY_CONFIG_FILE</code> </td><td><code>"&lt;file1&gt; [&lt;file2&gt; ...]"</code> </td><td><code>""</code> </td><td>Load custom layer configuration files </td></tr>
<tr>
<td><code>KEY_DUMP_KERNELS</code> </td><td><code>YES</code> / <code>NO</code> </td><td><code>NO</code> </td><td>Dump the final kernels used for custom layers </td></tr>
<tr>
<td><code>KEY_TUNING_MODE</code> </td><td><code>TUNING_DISABLED</code> <br />
 <code>TUNING_CREATE</code> <br />
 <code>TUNING_USE_EXISTING</code> </td><td><code>TUNING_DISABLED</code> </td><td>Disable inference kernel tuning <br />
 Create tuning file (expect much longer runtime) <br />
 Use an existing tuning file </td></tr>
<tr>
<td><code>KEY_TUNING_FILE</code> </td><td><code>"&lt;filename&gt;"</code> </td><td><code>""</code> </td><td>Tuning file to create / use </td></tr>
<tr>
<td><code>KEY_CLDNN_PLUGIN_PRIORITY</code> </td><td><code>&lt;0-3&gt;</code> </td><td><code>0</code> </td><td>OpenCL queue priority (before usage, make sure your OpenCL driver supports appropriate extension)<br />
 Higher value means higher priority for clDNN OpenCL queue. 0 disables the setting. </td></tr>
<tr>
<td><code>KEY_CLDNN_PLUGIN_THROTTLE</code> </td><td><code>&lt;0-3&gt;</code> </td><td><code>0</code> </td><td>OpenCL queue throttling (before usage, make sure your OpenCL driver supports appropriate extension)<br />
 Lower value means lower driver thread priority and longer sleep time for it. 0 disables the setting. </td></tr>
<tr>
<td><code>KEY_CLDNN_GRAPH_DUMPS_DIR</code> </td><td><code>"&lt;dump_dir&gt;"</code> </td><td><code>""</code> </td><td>clDNN graph optimizer stages dump output directory (in GraphViz format) </td></tr>
<tr>
<td><code>KEY_CLDNN_SOURCES_DUMPS_DIR</code> </td><td><code>"&lt;dump_dir&gt;"</code> </td><td><code>""</code> </td><td>Final optimized clDNN OpenCL sources dump output directory </td></tr>
<tr>
<td><code>KEY_GPU_THROUGHPUT_STREAMS</code> </td><td><code>KEY_GPU_THROUGHPUT_AUTO</code>, or positive integer</td><td>1 </td><td>Specifies a number of GPU "execution" streams for the throughput mode (upper bound for a number of inference requests that can be executed simultaneously).<br />
This option is can be used to decrease GPU stall time by providing more effective load from several streams. Increasing the number of streams usually is more effective for smaller topologies or smaller input sizes. Note that your application should provide enough parallel slack (e.g. running many inference requests) to leverage full GPU bandwidth. Additional streams consume several times more GPU memory, so make sure the system has enough memory available to suit parallel stream execution. Multiple streams might also put additional load on CPU. If CPU load increases, it can be regulated by setting an appropriate <code>KEY_CLDNN_PLUGIN_THROTTLE</code> option value (see above). If your target system has relatively weak CPU, keep throttling low. <br />
The default value is 1, which implies latency-oriented behaviour.<br />
<code>KEY_GPU_THROUGHPUT_AUTO</code> creates bare minimum of streams to improve the performance; this is the most portable option if you are not sure how many resources your target machine has (and what would be the optimal number of streams). <br />
 A positive integer value creates the requested number of streams. </td></tr>
<tr>
<td><code>KEY_EXCLUSIVE_ASYNC_REQUESTS</code> </td><td><code>YES</code> / <code>NO</code> </td><td><code>NO</code> </td><td>Forces async requests (also from different executable networks) to execute serially. </td></tr>
</table>
<h2>Note on Debug Capabilities of the GPU Plugin</h2>
<p>Inference Engine GPU plugin provides possibility to dump the user custom OpenCL&trade; kernels to a file to allow you to properly debug compilation issues in your custom kernels.</p>
<p>The application can use the <code>SetConfig()</code> function with the key <code>PluginConfigParams::KEY_DUMP_KERNELS</code> and value: <code>PluginConfigParams::YES</code>. Then during network loading, all custom layers will print their OpenCL kernels with the JIT instrumentation added by the plugin. The kernels will be stored in the working directory under files named the following way: <code>clDNN_program0.cl</code>, <code>clDNN_program1.cl</code>.</p>
<p>This option is disabled by default. Additionally, the application can call the <code>SetConfig()</code> function with the key <code>PluginConfigParams::KEY_DUMP_KERNELS</code> and value: <code>PluginConfigParams::NO</code> before network loading.</p>
<p>How to verify that this option is disabled:</p><ol type="1">
<li>Delete all <code>clDNN_program*.cl</code> files from the current directory</li>
<li>Run your application to load a network</li>
<li>Examine the working directory for the presence of any kernel file (for example, <code>clDNN_program0.cl</code>)</li>
</ol>
<h2>See Also</h2>
<ul>
<li><a class="el" href="_docs_IE_DG_supported_plugins_Supported_Devices.html">Supported Devices</a> </li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>