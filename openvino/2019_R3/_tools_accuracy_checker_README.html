<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Deep Learning accuracy validation framework - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Deep Learning accuracy validation framework </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Installation</h2>
<h3>Prerequisites</h3>
<p>Install prerequisites first:</p>
<h4>1. Python</h4>
<p><b>accuracy checker</b> uses <b>Python 3</b>. Install it first:</p>
<ul>
<li><a href="https://www.python.org/downloads/">Python3</a>, <a href="https://pypi.python.org/pypi/setuptools">setuptools</a>:</li>
</ul>
<div class="fragment"><div class="line">sudo apt-get install python3 python3-dev python3-setuptools python3-pip</div></div><!-- fragment --><p>Python setuptools and python package manager (pip) install packages into system directory by default. Installation of accuracy checker tested only via <a href="https://docs.python.org/3/tutorial/venv.html">virtual environment</a>.</p>
<p>In order to use virtual environment you should install it first:</p>
<div class="fragment"><div class="line">python3 -m pip install virtualenv</div><div class="line">python3 -m virtualenv -p `which python3` &lt;directory_for_environment&gt;</div></div><!-- fragment --><p>Before starting to work inside virtual environment, it should be activated:</p>
<div class="fragment"><div class="line">source &lt;directory_for_environment&gt;/bin/activate</div></div><!-- fragment --><p>Virtual environment can be deactivated using command</p>
<div class="fragment"><div class="line">deactivate</div></div><!-- fragment --><h4>2. Frameworks</h4>
<p>The next step is installing backend frameworks for Accuracy Checker.</p>
<p>In order to evaluate some models required frameworks have to be installed. Accuracy-Checker supports these frameworks:</p>
<ul>
<li><a href="https://software.intel.com/en-us/openvino-toolkit/documentation/get-started">OpenVINO</a>.</li>
<li>./tools/accuracy_checker/accuracy_checker/launcher/caffe_installation_readme.md "Caffe".</li>
<li><a href="https://mxnet.incubator.apache.org/versions/master/">MxNet</a>.</li>
<li><a href="https://docs.opencv.org/4.1.0/d2/de6/tutorial_py_setup_in_ubuntu.html">OpenCV DNN</a>.</li>
<li><a href="https://www.tensorflow.org/">TensorFlow</a>.</li>
<li><a href="https://github.com/microsoft/onnxruntime/blob/master/README.md">ONNX Runtime</a>.</li>
<li><a href="https://pytorch.org/">Pytorch</a></li>
</ul>
<p>You can use any of them or several at a time.</p>
<h3>Install accuracy checker</h3>
<p>If all prerequisite are installed, then you are ready to install <b>accuracy checker</b>:</p>
<div class="fragment"><div class="line">python3 setup.py install</div></div><!-- fragment --><h4>Usage</h4>
<p>You may test your installation and get familiar with accuracy checker by running <a class="el" href="_tools_accuracy_checker_sample_README.html">sample</a>.</p>
<p>Once you installed accuracy checker you can evaluate your configurations with:</p>
<div class="fragment"><div class="line">accuracy_check -c path/to/configuration_file -m /path/to/models -s /path/to/source/data -a /path/to/annotation</div></div><!-- fragment --><p>All relative paths in config files will be prefixed with values specified in command line:</p>
<ul>
<li><code>-c, --config</code> path to configuration file.</li>
<li><code>-m, --models</code> specifies directory in which models and weights declared in config file will be searched.</li>
<li><code>-s, --source</code> specifies directory in which input images will be searched.</li>
<li><code>-a, --annotations</code> specifies directory in which annotation and meta files will be searched.</li>
</ul>
<p>You may refer to <code>-h, --help</code> to full list of command line options. Some optional arguments are:</p>
<ul>
<li><code>-r, --root</code> prefix for all relative paths.</li>
<li><code>-d, --definitions</code> path to the global configuration file</li>
<li><code>-e, --extensions</code> directory with <a class="el" href="namespaceInferenceEngine.html" title="Inference Engine API. ">InferenceEngine</a> extensions.</li>
<li><code>-b, --bitstreams</code> directory with bitstream (for Inference Engine with fpga plugin).</li>
<li>`-C, '&ndash;converted_models<code>directory to store Model Optimizer converted models (used for DLSDK launcher only). -</code>-tf, &ndash;target_framework<code>framework for infer. -</code>-td, &ndash;target_devices` devices for infer. You can specify several devices using space as a delimiter.</li>
</ul>
<h4>Configuration</h4>
<p>There is config file which declares validation process. Every validated model has to have its entry in <code>models</code> list with distinct <code>name</code> and other properties described below.</p>
<p>There is also definitions file, which declares global options shared across all models. Config file has priority over definitions file.</p>
<p>example:</p>
<div class="fragment"><div class="line">models:</div><div class="line">- name: model_name</div><div class="line">  launchers:</div><div class="line">    - framework: caffe</div><div class="line">      model:   public/alexnet/caffe/bvlc_alexnet.prototxt</div><div class="line">      weights: public/alexnet/caffe/bvlc_alexnet.caffemodel</div><div class="line">      adapter: classification</div><div class="line">      batch: 128</div><div class="line">  datasets:</div><div class="line">    - name: dataset_name</div></div><!-- fragment --><h3>Launchers</h3>
<p>Launcher is a description of how your model should be executed. Each launcher configuration starts with setting <code>framework</code> name. Currently <em>caffe</em>, <em>dlsdk</em>, <em>mxnet</em>, <em>tf</em>, <em>tf_lite</em>, <em>opencv</em>, <em>onnx_runtime</em> supported. Launcher description can have differences. Please view:</p>
<ul>
<li><a class="el" href="_tools_accuracy_checker_accuracy_checker_launcher_caffe_launcher_readme.html">how to configure Caffe launcher</a>.</li>
<li><a class="el" href="_tools_accuracy_checker_accuracy_checker_launcher_dlsdk_launcher_readme.html">how to configure DLSDK launcher</a>.</li>
<li><a class="el" href="_tools_accuracy_checker_accuracy_checker_launcher_opencv_launcher_readme.html">how to configure OpenCV launcher</a>.</li>
<li><a class="el" href="_tools_accuracy_checker_accuracy_checker_launcher_mxnet_launcher_readme.html">how to configure MxNet Launcher</a>.</li>
<li><a class="el" href="_tools_accuracy_checker_accuracy_checker_launcher_tf_launcher_readme.html">how to configure TensorFlow Launcher</a>.</li>
<li><a class="el" href="_tools_accuracy_checker_accuracy_checker_launcher_tf_lite_launcher_readme.html">how to configure TensorFlow Lite Launcher</a>.</li>
<li><a class="el" href="_tools_accuracy_checker_accuracy_checker_launcher_onnx_runtime_launcher_readme.html">how to configure ONNX Runtime Launcher</a>.</li>
<li>./tools/accuracy_checker/accuracy_checker/launcher/pytorch_launcher_readme.md "how to configure Pytorch Launcher"</li>
</ul>
<h3>Datasets</h3>
<p>Dataset entry describes data on which model should be evaluated, all required preprocessing and postprocessing/filtering steps, and metrics that will be used for evaluation.</p>
<p>If your dataset data is a well-known competition problem (COCO, Pascal VOC, ...) and/or can be potentially reused for other models it is reasonable to declare it in some global configuration file (<em>definition</em> file). This way in your local configuration file you can provide only <code>name</code> and all required steps will be picked from global one. To pass path to this global configuration use <code>--definition</code> argument of CLI.</p>
<p>Each dataset must have:</p>
<ul>
<li><code>name</code> - unique identifier of your model/topology.</li>
<li><code>data_source</code>: path to directory where input data is stored.</li>
<li><code>metrics</code>: list of metrics that should be computed.</li>
</ul>
<p>And optionally:</p><ul>
<li><code>preprocessing</code>: list of preprocessing steps applied to input data. If you want calculated metrics to match reported, you must reproduce preprocessing from canonical paper of your topology or ask topology author about required steps.</li>
<li><code>postprocessing</code>: list of postprocessing steps.</li>
<li><code>reader</code>: approach for data reading. Default reader is <code>opencv_imread</code>.</li>
</ul>
<p>Also it must contain data related to annotation. You can convert annotation inplace using:</p><ul>
<li><code>annotation_conversion</code>: parameters for annotation conversion</li>
</ul>
<p>or use existing annotation file and dataset meta:</p><ul>
<li><code>annotation</code> - path to annotation file, you must <b>convert annotation to representation of dataset problem first</b>, you may choose one of the converters from <em>annotation-converters</em> if there is already converter for your dataset or write your own.</li>
<li><code>dataset_meta</code>: path to metadata file (generated by converter). More detailed information about annotation conversion you can find in <a class="el" href="_tools_accuracy_checker_accuracy_checker_annotation_converters_README.html">Annotation Conversion Guide</a>.</li>
</ul>
<p>example of dataset definition:</p>
<div class="fragment"><div class="line">- name: dataset_name</div><div class="line">  annotation: annotation.pickle</div><div class="line">  data_source: images_folder</div><div class="line"></div><div class="line">  preprocessing:</div><div class="line">    - type: resize</div><div class="line">      dst_width: 256</div><div class="line">      dst_height: 256</div><div class="line"></div><div class="line">    - type: normalization</div><div class="line">      mean: imagenet</div><div class="line"></div><div class="line">    - type: crop</div><div class="line">      dst_width: 227</div><div class="line">      dst_height: 227</div><div class="line"></div><div class="line">  metrics:</div><div class="line">    - type: accuracy</div></div><!-- fragment --><h3>Preprocessing, Metrics, Postprocessing</h3>
<p>Each entry of preprocessing, metrics, postprocessing must have <code>type</code> field, other options are specific to type. If you do not provide any other option, then it will be picked from <em>definitions</em> file.</p>
<p>You can find useful following instructions:</p>
<ul>
<li><a class="el" href="_tools_accuracy_checker_accuracy_checker_annotation_converters_README.html">how to convert annotations</a></li>
<li><a class="el" href="_tools_accuracy_checker_accuracy_checker_preprocessor_README.html">how to use preprocessings</a>.</li>
<li><a class="el" href="_tools_accuracy_checker_accuracy_checker_postprocessor_README.html">how to use postprocessings</a>.</li>
<li><a class="el" href="_tools_accuracy_checker_accuracy_checker_metrics_README.html">how to use metrics</a>.</li>
<li><a class="el" href="_tools_accuracy_checker_accuracy_checker_data_readers_README.html">how to use readers</a>.</li>
</ul>
<p>You may optionally provide <code>reference</code> field for metric, if you want calculated metric tested against specific value (i.e. reported in canonical paper).</p>
<p>Some metrics support providing vector results ( e. g. mAP is able to return average precision for each detection class). You can change view mode for metric results using <code>presenter</code> (e.g. <code>print_vector</code>, <code>print_scalar</code>).</p>
<p>example:</p>
<div class="fragment"><div class="line">metrics:</div><div class="line">- type: accuracy</div><div class="line">  top_k: 5</div><div class="line">  reference: 86.43</div><div class="line">  threshold: 0.005</div></div><!-- fragment --><h3>Testing new models</h3>
<p>Typical workflow for testing new model include:</p>
<ol type="1">
<li>Convert annotation of your dataset. Use one of the converters from annotation-converters, or write your own if there is no converter for your dataset. You can find detailed instruction how to use converters in <a class="el" href="_tools_accuracy_checker_accuracy_checker_annotation_converters_README.html">Annotation Conversion Guide</a>.</li>
</ol>
<ol type="1">
<li>Choose one of <em>adapters</em> or write your own. Adapter converts raw output produced by framework to high level problem specific representation (e.g. <em>ClassificationPrediction</em>, <em>DetectionPrediction</em>, etc).</li>
</ol>
<ol type="1">
<li>Reproduce preprocessing, metrics and postprocessing from canonical paper.</li>
</ol>
<ol type="1">
<li>Create entry in config file and execute. </li>
</ol>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>