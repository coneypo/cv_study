<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Adapters - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Adapters </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Adapter is a function for conversion network infer output to metric specific format. You can use 2 ways to set adapter for topology:</p><ul>
<li>Define adapter as a string.</li>
</ul>
<div class="fragment"><div class="line">adapter: classification</div></div><!-- fragment --><ul>
<li>Define adapter as a dictionary, using <code>type:</code> for setting adapter name. This approach gives opportunity to set additional parameters for adapter if it is required.</li>
</ul>
<div class="fragment"><div class="line">adapter:</div><div class="line">  type: reid</div><div class="line">  grn_workaround: False</div></div><!-- fragment --><p>AccuracyChecker supports following set of adapters:</p><ul>
<li><code>classification</code> - converting output of classification model to <code>ClassificationPrediction</code> representation.</li>
<li><code>segmentation</code> - converting output of semantic segmentation model to <code>SeegmentationPrediction</code> representation.<ul>
<li><code>make_argmax</code> - allows to apply argmax operation to output values.</li>
</ul>
</li>
<li><code>segmentation_one_class</code> - converting output of semantic segmentation to <code>SeegmentationPrediction</code> representation. It is suitable for situation when model's output is probability of belong each pixel to foreground class.<ul>
<li><code>threshold</code> - minimum probability threshold for valid class belonging.</li>
</ul>
</li>
<li><code>tiny_yolo_v1</code> - converting output of Tiny YOLO v1 model to <code>DetectionPrediction</code> representation.</li>
<li><code>reid</code> - converting output of reidentification model to <code>ReIdentificationPrediction</code> representation.<ul>
<li><code>grn_workaround</code> - enabling processing output with adding Global Region Normalization layer.</li>
</ul>
</li>
<li><code>yolo_v2</code> - converting output of YOLO v2 family models to <code>DetectionPrediction</code> representation.<ul>
<li><code>classes</code> - number of detection classes (default 20).</li>
<li><code>anchors</code> - anchor values provided as comma-separated list or one of precomputed:<ul>
<li><code>yolo_v2</code> - <code>[1.3221, 1.73145, 3.19275, 4.00944, 5.05587, 8.09892, 9.47112, 4.84053, 11.2364, 10.0071]</code>,</li>
<li><code>tiny_yolo_v2</code> - <code>[1.08, 1.19, 3.42, 4.41, 6.63, 11.38, 9.42, 5.11, 16.62, 10.52]</code></li>
</ul>
</li>
<li><code>coords</code> - number of bbox coordinates (default 4).</li>
<li><code>num</code> - num parameter from DarkNet configuration file (default 5).</li>
</ul>
</li>
<li><code>yolo_v3</code> - converting output of YOLO v3 family models to <code>DetectionPrediction</code> representation.<ul>
<li><code>classes</code> - number of detection classes (default 80).</li>
<li><code>anchors</code> - anchor values provided as comma-separited list or precomputed:<ul>
<li><code>yolo_v3</code> - <code>[10.0, 13.0, 16.0, 30.0, 33.0, 23.0, 30.0, 61.0, 62.0, 45.0, 59.0, 119.0, 116.0, 90.0, 156.0, 198.0, 373.0, 326.0]</code></li>
<li><code>tiny_yolo_v3</code> - <code>[10.0, 14.0, 23.0, 27.0, 37.0, 58.0, 81.0, 82.0, 135.0, 169.0, 344.0, 319.0]</code></li>
</ul>
</li>
<li><code>coords</code> - number of bbox coordinates (default 4).</li>
<li><code>num</code> - num parameter from DarkNet configuration file (default 3).</li>
<li><code>threshold</code> - minimal objectness score value for valid detections (default 0.001).</li>
<li><code>input_width</code> and <code>input_height</code> - network input width and height correspondingly (default 416).</li>
<li><code>outputs</code> - the list of output layers names (optional), if specified there should be exactly 3 output layers provided.</li>
</ul>
</li>
<li><code>lpr</code> - converting output of license plate recognition model to <code>CharacterRecognitionPrediction</code> representation.</li>
<li><code>ssd</code> - converting output of SSD model to <code>DetectionPrediction</code> representation.</li>
<li><code>ssd_mxnet</code> - converting output of SSD-based models from MxNet framework to <code>DetectionPrediction</code> representation.</li>
<li><code>pytorch_ssd_decoder</code> - converts output of SSD model from Pytorch without embedded decoder.<ul>
<li><code>scores_out</code> - name of output layer with bounding boxes scores.</li>
<li><code>boxes_out</code> - name of output layer with bounding boxes coordinates.</li>
<li><code>confidence_threshold</code> - lower bound for valid boxes scores (optional, default 0.05).</li>
<li><code>nms_threshold</code> - overlap threshold for NMS (optional, default 0.5).</li>
<li><code>keep_top_k</code> - maximal number of boxes which should be kept (optional, default 200).</li>
</ul>
</li>
<li><code>ssd_onnx</code> - converting output of SSD-based model from Pytorch with NonMaxSuppression layer.<ul>
<li><code>labels_out</code> - name of output layer with labels or regular expression for it searching.</li>
<li><code>scores_out</code>- name of output layer with scores or regular expression for it searching.</li>
<li><code>bboxes_out</code> - name of output layer with bboxes or regular expression for it searching.</li>
</ul>
</li>
<li><code>tf_object_detection</code> - converting output of detection models from TensorFlow object detection API to <code>DetectionPrediction</code>.<ul>
<li><code>classes_out</code> - name of output layer with predicted classes.</li>
<li><code>boxes_out</code> - name of output layer with predicted boxes coordinates in format [y0, x0, y1, x1].</li>
<li><code>scores_out</code> - name of output layer with detection scores.</li>
<li><code>num_detections_out</code> - name of output layer which contains the number of valid detections.</li>
</ul>
</li>
<li><code>face_person_detection</code> - converting face person detection model output with 2 detection outputs to <code>ContainerPredition</code>, where value of parameters <code>face_out</code>and <code>person_out</code> are used for identification <code>DetectionPrediction</code> in container.<ul>
<li><code>face_out</code> - face detection output layer name.</li>
<li><code>person_out</code> - person detection output layer name.</li>
</ul>
</li>
<li><code>person_attributes</code> - converting person attributes recognition model output to <code>MultiLabelRecognitionPrediction</code>.<ul>
<li><code>attributes_recognition_out</code> - output layer name with attributes scores. (optional, used if your model has more than one outputs).</li>
</ul>
</li>
<li><code>vehicle_attributes</code> - converting vehicle attributes recognition model output to <code>ContainerPrediction</code> where value of parameters <code>color_out</code>and <code>type_out</code> are used for identification <code>ClassificationPrediction</code> in container.<ul>
<li><code>color_out</code> - vehicle color attribute output layer name.</li>
<li><code>type_out</code>- vehicle type attribute output layer name.</li>
</ul>
</li>
<li><code>head_pose</code> - converting head pose estimation model output to <code>ContainerPrediction</code> where names of parameters <code>angle_pitch</code>, <code>angle_yaw</code> and <code>angle_roll</code> are used for identification <code>RegressionPrediction</code> in container.<ul>
<li><code>angle_pitch</code> - output layer name for pitch angle.</li>
<li><code>angle_yaw</code>- output layer name for yaw angle.</li>
<li><code>angle_roll</code> - output layer name for roll angle.</li>
</ul>
</li>
<li><code>age_gender</code> - converting age gender recognition model output to <code>ContainerPrediction</code> with <code>ClassificationPrediction</code> named <code>gender</code> for gender recognition, <code>ClassificationPrediction</code> named <code>age_classification</code> and <code>RegressionPrediction</code> named <code>age_error</code> for age recognition.<ul>
<li><code>age_out</code> - output layer name for age recognition.</li>
<li><code>gender_out</code> - output layer name for gender recognition.</li>
</ul>
</li>
<li><code>action_detection</code> - converting output of model for person detection and action recognition tasks to <code>ContainerPrediction</code> with <code>DetectionPrdiction</code> for class agnostic metric calculation and <code>ActionDetectionPrediction</code> for action recognition. The representations in container have names <code>class_agnostic_prediction</code> and <code>action_prediction</code> respectively.<ul>
<li><code>priorbox_out</code> - name of layer containing prior boxes in SSD format.</li>
<li><code>loc_out</code> - name of layer containing box coordinates in SSD format.</li>
<li><code>main_conf_out</code> - name of layer containing detection confidences.</li>
<li><code>add_conf_out_prefix</code> - prefix for generation name of layers containing action confidences if topology has several following layers or layer name.</li>
<li><code>add_conf_out_count</code> - number of layers with action confidences (optional, you can not provide this argument if action confidences contained in one layer).</li>
<li><code>num_action_classes</code> - number classes for action recognition.</li>
<li><code>detection_threshold</code> - minimal detection confidences level for valid detections.</li>
<li><code>actions_scores_threshold</code> - minimal actions confidences level for valid detections.</li>
<li><code>action_scale</code> - scale for correct action score calculation.</li>
</ul>
</li>
<li><code>super_resolution</code> - converting output of single image super resolution network to <code>SuperResolutionPrediction</code>.<ul>
<li><code>reverse_channels</code> - allow switching output image channels e.g. RGB to BGR (Optional. Default value is False).</li>
</ul>
</li>
<li><code>landmarks_regression</code> - converting output of model for landmarks regression to <code>FacialLandmarksPrediction</code>.</li>
<li><code>pixel_link_text_detection</code> - converting output of PixelLink like model for text detection to <code>TextDetectionPrediction</code>.<ul>
<li><code>pixel_class_out</code> - name of layer containing information related to text/no-text classification for each pixel.</li>
<li><code>pixel_link_out</code> - name of layer containing information related to linkage between pixels and their neighbors.</li>
<li><code>pixel_class_confidence_threshold</code> - confidence threshold for valid segmentation mask (Optional, default 0.8).</li>
<li><code>pixel_link_confidence_threshold</code> - confidence threshold for valid pixel links (Optional, default 0.8).</li>
<li><code>min_area</code> - minimal area for valid text prediction (Optional, default 0).</li>
<li><code>min_height</code> - minimal height for valid text prediction (Optional, default 0).</li>
</ul>
</li>
<li><code>ctpn_text_detection</code> - converting output of CTPN like model for text detection to <code>TextDetectionPrediction</code>.<ul>
<li><code>cls_prob_out</code> - name of output layer with class probabilities.</li>
<li><code>bbox_pred_out</code> - name of output layer with predicted boxes.</li>
<li><code>min_size</code> - minimal valid detected text proposals size (Optional, default 8).</li>
<li><code>min_ratio</code> - minimal width / height ratio for valid text line (Optional, default 0.5).</li>
<li><code>line_min_score</code> - minimal confidence for text line (Optional, default 0.9).</li>
<li><code>text_proposals_width</code> - minimal width for text proposal (Optional, default 16).</li>
<li><code>min_num_proposals</code> - minimal number for text proposals (Optional, default 2).</li>
<li><code>pre_nms_top_n</code> - saved top n proposals before NMS applying (Optional, default 12000).</li>
<li><code>post_nms_top_n</code> - saved top n proposals after NMS applying (Optional, default 1000).</li>
<li><code>nms_threshold</code> - overlap threshold for NMS (Optional, default 0.7).</li>
</ul>
</li>
<li><code>east_text_detection</code> - converting output of EAST like model for text detection to <code>TextDetectionPrediction</code>.<ul>
<li><code>score_map_out</code> - the name of output layer which contains score map.</li>
<li><code>geometry_map_out</code> - the name of output layer which contains geometry map.</li>
<li><code>score_map_threshold</code> - threshold for score map (Optional, default 0.8).</li>
<li><code>nms_threshold</code> - threshold for text boxes NMS (Optional, default 0.2).</li>
<li><code>box_threshold</code> - minimal confidence threshold for text boxes (Optional, default 0.1).</li>
</ul>
</li>
<li><code>human_pose_estimation</code> - converting output of model for human pose estimation to <code>PoseEstimationPrediction</code>.<ul>
<li><code>part_affinity_fields_out</code> - name of output layer with keypoints pairwise relations (part affinity fields).</li>
<li><code>keypoints_heatmap_out</code> - name of output layer with keypoints heatmaps.</li>
</ul>
</li>
<li><code>beam_search_decoder</code> - realization CTC Beam Search decoder for symbol sequence recognition, converting model output to <code>CharacterRecognitionPrediction</code>.<ul>
<li><code>beam_size</code> - size of the beam to use during decoding (default 10).</li>
<li><code>blank_label</code> - index of the CTC blank label.</li>
<li><code>softmaxed_probabilities</code> - indicator that model uses softmax for output layer (default False).</li>
</ul>
</li>
<li><code>gaze_estimation</code> - converting output of gaze estimation model to <code>GazeVectorPrediction</code>.</li>
<li><code>hit_ratio_adapter</code> - converting output NCF model to <code>HitRatioPrediction</code>.</li>
<li><code>brain_tumor_segmentation</code> - converting output of brain tumor segmentation model to <code>BrainTumorSegmentationPrediction</code>.</li>
<li><code>nmt</code> - converting output of neural machine translation model to <code>MachineTranslationPrediction</code>.<ul>
<li><code>vocabulary_file</code> - file which contains vocabulary for encoding model predicted indexes to words (e. g. vocab.bpe.32000.de). Path can be prefixed with <code>--models</code> arguments.</li>
<li><code>eos_index</code> - index end of string symbol in vocabulary (Optional, used in cases when launcher does not support dynamic output shape for cut off empty prediction). </li>
</ul>
</li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>