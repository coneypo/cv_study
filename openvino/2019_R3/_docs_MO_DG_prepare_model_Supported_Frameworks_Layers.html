<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Supported Framework Layers - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Supported Framework Layers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Caffe* Supported Layers and the Mapping to the Intermediate Representation Layers</h2>
<p>Standard Caffe* layers:</p>
<table class="doxtable">
<tr>
<th align="left">Number </th><th align="left">Layer Name in Caffe* </th><th align="left">Layer Name in the Intermediate Representation  </th></tr>
<tr>
<td align="left">1</td><td align="left">Input </td><td align="left">Input </td></tr>
<tr>
<td align="left">2</td><td align="left">GlobalInput </td><td align="left">Input </td></tr>
<tr>
<td align="left">3</td><td align="left">InnerProduct </td><td align="left">FullyConnected </td></tr>
<tr>
<td align="left">4</td><td align="left">Dropout </td><td align="left">Ignored, does not appear in IR </td></tr>
<tr>
<td align="left">5</td><td align="left">Convolution </td><td align="left">Convolution </td></tr>
<tr>
<td align="left">6</td><td align="left">Deconvolution </td><td align="left">Deconvolution </td></tr>
<tr>
<td align="left">7</td><td align="left">Pooling </td><td align="left">Pooling </td></tr>
<tr>
<td align="left">8</td><td align="left">BatchNorm </td><td align="left">BatchNormalization </td></tr>
<tr>
<td align="left">9</td><td align="left">LRN </td><td align="left">Norm </td></tr>
<tr>
<td align="left">10</td><td align="left">Power </td><td align="left">Power </td></tr>
<tr>
<td align="left">11</td><td align="left">ReLU </td><td align="left">ReLU </td></tr>
<tr>
<td align="left">12</td><td align="left">Scale </td><td align="left">ScaleShift </td></tr>
<tr>
<td align="left">13</td><td align="left">Concat </td><td align="left">Concat </td></tr>
<tr>
<td align="left">14</td><td align="left">Eltwise </td><td align="left">Eltwise </td></tr>
<tr>
<td align="left">15</td><td align="left">Flatten </td><td align="left">Flatten </td></tr>
<tr>
<td align="left">16</td><td align="left">Reshape </td><td align="left">Reshape </td></tr>
<tr>
<td align="left">17</td><td align="left">Slice </td><td align="left">Slice </td></tr>
<tr>
<td align="left">18</td><td align="left">Softmax </td><td align="left">SoftMax </td></tr>
<tr>
<td align="left">19</td><td align="left">Permute </td><td align="left">Permute </td></tr>
<tr>
<td align="left">20</td><td align="left">ROIPooling </td><td align="left">ROIPooling </td></tr>
<tr>
<td align="left">21</td><td align="left">Tile </td><td align="left">Tile </td></tr>
<tr>
<td align="left">22</td><td align="left">ShuffleChannel </td><td align="left">Reshape + Split + Permute + Concat </td></tr>
<tr>
<td align="left">23</td><td align="left">Axpy </td><td align="left">ScaleShift + Eltwise </td></tr>
<tr>
<td align="left">24</td><td align="left">BN </td><td align="left">ScaleShift </td></tr>
<tr>
<td align="left">25</td><td align="left">DetectionOutput </td><td align="left">DetectionOutput </td></tr>
<tr>
<td align="left">26</td><td align="left">StridedSlice </td><td align="left">StridedSlice </td></tr>
<tr>
<td align="left">27</td><td align="left">Bias </td><td align="left">Eltwise(operation = sum) </td></tr>
</table>
<h2>MXNet* Supported Symbols and the Mapping to the Intermediate Representation Layers</h2>
<p>Standard MXNet* symbols:</p>
<table class="doxtable">
<tr>
<th>Number</th><th align="center">Symbol Name in MXNet*</th><th align="right">Layer Name in the Intermediate Representation  </th></tr>
<tr>
<td>1</td><td align="center">BatchNorm </td><td align="right">BatchNormalization </td></tr>
<tr>
<td>2</td><td align="center">Crop </td><td align="right">Crop </td></tr>
<tr>
<td>3</td><td align="center">ScaleShift </td><td align="right">ScaleShift </td></tr>
<tr>
<td>4</td><td align="center">Pooling </td><td align="right">Pooling </td></tr>
<tr>
<td>5</td><td align="center">SoftmaxOutput </td><td align="right">SoftMax </td></tr>
<tr>
<td>6</td><td align="center">SoftmaxActivation </td><td align="right">SoftMax </td></tr>
<tr>
<td>7</td><td align="center">null </td><td align="right">Ignored, does not appear in IR </td></tr>
<tr>
<td>8</td><td align="center">Convolution </td><td align="right">Convolution </td></tr>
<tr>
<td>9</td><td align="center">Deconvolution </td><td align="right">Deconvolution </td></tr>
<tr>
<td>10</td><td align="center">Activation(act_type = relu) </td><td align="right">ReLU </td></tr>
<tr>
<td>11</td><td align="center">ReLU </td><td align="right">ReLU </td></tr>
<tr>
<td>12</td><td align="center">LeakyReLU </td><td align="right">ReLU (negative_slope = 0.25) </td></tr>
<tr>
<td>13</td><td align="center">Concat </td><td align="right">Concat </td></tr>
<tr>
<td>14</td><td align="center">elemwise_add </td><td align="right">Eltwise(operation = sum) </td></tr>
<tr>
<td>15</td><td align="center">_Plus </td><td align="right">Eltwise(operation = sum) </td></tr>
<tr>
<td>16</td><td align="center">Flatten </td><td align="right">Flatten </td></tr>
<tr>
<td>17</td><td align="center">Reshape </td><td align="right">Reshape </td></tr>
<tr>
<td>18</td><td align="center">FullyConnected </td><td align="right">FullyConnected </td></tr>
<tr>
<td>19</td><td align="center">UpSampling </td><td align="right">Resample </td></tr>
<tr>
<td>20</td><td align="center">transpose </td><td align="right">Permute </td></tr>
<tr>
<td>21</td><td align="center">LRN </td><td align="right">Norm </td></tr>
<tr>
<td>22</td><td align="center">L2Normalization </td><td align="right">Normalize </td></tr>
<tr>
<td>23</td><td align="center">Dropout </td><td align="right">Ignored, does not appear in IR </td></tr>
<tr>
<td>24</td><td align="center">_copy </td><td align="right">Ignored, does not appear in IR </td></tr>
<tr>
<td>25</td><td align="center">_contrib_MultiBoxPrior </td><td align="right">PriorBox </td></tr>
<tr>
<td>26</td><td align="center">_contrib_MultiBoxDetection </td><td align="right">DetectionOutput </td></tr>
<tr>
<td>27</td><td align="center">broadcast_mul </td><td align="right">ScaleShift </td></tr>
<tr>
<td>28</td><td align="center">sigmoid </td><td align="right">sigmoid </td></tr>
<tr>
<td>29</td><td align="center">Activation (act_type = tanh) </td><td align="right">Activation (operation = tanh) </td></tr>
<tr>
<td>30</td><td align="center">LeakyReLU (act_type = prelu) </td><td align="right">PReLU </td></tr>
<tr>
<td>31</td><td align="center">LeakyReLU (act_type = elu) </td><td align="right">Activation (operation = elu) </td></tr>
<tr>
<td>32</td><td align="center">elemwise_mul </td><td align="right">Eltwise (operation = mul) </td></tr>
<tr>
<td>33</td><td align="center">add_n <div class="image">
<img src="mxnet_add_n_ref.png" alt="mxnet_add_n_ref.png"/>
</div>
 </td><td align="right">Eltwise (operation = sum) <div class="image">
<img src="ir_add_n_ref.png" alt="ir_add_n_ref.png"/>
</div>
 </td></tr>
<tr>
<td>34</td><td align="center">ElementWiseSum <div class="image">
<img src="mxnet_add_n_ref.png" alt="mxnet_add_n_ref.png"/>
</div>
 </td><td align="right">Eltwise (operation = sum) or ScaleShift <div class="image">
<img src="ir_add_n_ref.png" alt="ir_add_n_ref.png"/>
</div>
 </td></tr>
<tr>
<td>35</td><td align="center">_mul_scalar </td><td align="right">Power </td></tr>
<tr>
<td>36</td><td align="center">broadcast_add </td><td align="right">Eltwise (operation = sum) </td></tr>
<tr>
<td>37</td><td align="center">slice_axis </td><td align="right">Crop </td></tr>
<tr>
<td>38</td><td align="center">Custom </td><td align="right"><a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Customize_Model_Optimizer.html">Custom Layers in the Model Optimizer</a> </td></tr>
<tr>
<td>39</td><td align="center">_minus_scalar </td><td align="right">Power </td></tr>
<tr>
<td>40</td><td align="center">Pad </td><td align="right">Pad </td></tr>
<tr>
<td>41</td><td align="center">_contrib_Proposal </td><td align="right">Proposal </td></tr>
<tr>
<td>42</td><td align="center">ROIPooling </td><td align="right">ROIPooling </td></tr>
<tr>
<td>43</td><td align="center">stack </td><td align="right">Concat </td></tr>
<tr>
<td>44</td><td align="center">swapaxis </td><td align="right">Permute </td></tr>
<tr>
<td>45</td><td align="center">zeros </td><td align="right">Const </td></tr>
<tr>
<td>45</td><td align="center">rnn </td><td align="right">TensorIterator </td></tr>
<tr>
<td>46</td><td align="center">rnn_param_concat </td><td align="right">Concat </td></tr>
<tr>
<td>47</td><td align="center">slice_channel </td><td align="right">Split </td></tr>
<tr>
<td>48</td><td align="center">_maximum </td><td align="right">Eltwise(operation = max) </td></tr>
<tr>
<td>49</td><td align="center">_minimum </td><td align="right">Power(scale=-1) + Eltwise(operation = max) + Power(scale=-1) </td></tr>
<tr>
<td>50</td><td align="center">InstanceNorm</td><td align="right">scale * (x - mean) / sqrt(variance + epsilon) + B </td></tr>
<tr>
<td>51</td><td align="center">Embedding</td><td align="right">Gather </td></tr>
<tr>
<td>52</td><td align="center">DeformableConvolution </td><td align="right">DeformableConvolution </td></tr>
<tr>
<td>53</td><td align="center">DeformablePSROIPooling </td><td align="right">PSROIPooling (method=deformable) </td></tr>
<tr>
<td>54</td><td align="center">Where </td><td align="right">Select </td></tr>
<tr>
<td>55</td><td align="center">exp </td><td align="right">Exp </td></tr>
<tr>
<td>56</td><td align="center">slice_like </td><td align="right">Crop </td></tr>
<tr>
<td>57</td><td align="center">div_scalar </td><td align="right">Power(power = -1) + Eltwise(operation = mul) </td></tr>
<tr>
<td>58</td><td align="center">minus_scalar </td><td align="right">Eltwise(operation = sum) + Power(scale=-1) </td></tr>
<tr>
<td>59</td><td align="center">greater_scalar </td><td align="right">Eltwise(operation=Greater) </td></tr>
<tr>
<td>60</td><td align="center">elemtwise_sub </td><td align="right">Eltwise(operation = sum) + Power(scale=-1) </td></tr>
<tr>
<td>61</td><td align="center">expand_dims </td><td align="right">Unsqueeze </td></tr>
</table>
<h2>TensorFlow* Supported Operations and the Mapping to Intermediate Representation Layers</h2>
<p>Some TensorFlow* operations do not match to any Inference Engine layer, but are still supported by the Model Optimizer and can be used on constant propagation path. These layers are labeled 'Constant propagation' in the table.</p>
<p>Standard TensorFlow* operations:</p>
<table class="doxtable">
<tr>
<th>Number</th><th align="center">Operation Name in TensorFlow </th><th align="right">Layer Name in the Intermediate Representation  </th></tr>
<tr>
<td>1</td><td align="center">Transpose </td><td align="right">Permute </td></tr>
<tr>
<td>2</td><td align="center">LRN </td><td align="right">Norm </td></tr>
<tr>
<td>3</td><td align="center">Split </td><td align="right">Split </td></tr>
<tr>
<td>4</td><td align="center">SplitV </td><td align="right">Split </td></tr>
<tr>
<td>5</td><td align="center">FusedBatchNorm </td><td align="right">ScaleShift (can be fused into Convolution or FullyConnected) </td></tr>
<tr>
<td>6</td><td align="center">Relu6 </td><td align="right">Clamp </td></tr>
<tr>
<td>7</td><td align="center">DepthwiseConv2dNative</td><td align="right">Convolution </td></tr>
<tr>
<td>8</td><td align="center">ExpandDims </td><td align="right">Unsqueeze </td></tr>
<tr>
<td>9</td><td align="center">Slice </td><td align="right">Split </td></tr>
<tr>
<td>10</td><td align="center">ConcatV2 </td><td align="right">Concat </td></tr>
<tr>
<td>11</td><td align="center">MatMul </td><td align="right">FullyConnected </td></tr>
<tr>
<td>12</td><td align="center">Pack </td><td align="right">Reshapes and Concat </td></tr>
<tr>
<td>13</td><td align="center">StridedSlice </td><td align="right">StridedSlice or Split </td></tr>
<tr>
<td>14</td><td align="center">Prod </td><td align="right">Constant propagation </td></tr>
<tr>
<td>15</td><td align="center">Const </td><td align="right">Const </td></tr>
<tr>
<td>16</td><td align="center">Tile </td><td align="right">Tile </td></tr>
<tr>
<td>17</td><td align="center">Placeholder </td><td align="right">Input </td></tr>
<tr>
<td>18</td><td align="center">Pad </td><td align="right">Fused into Convolution or Pooling layers (not supported as single operation) </td></tr>
<tr>
<td>19</td><td align="center">Conv2D </td><td align="right">Convolution </td></tr>
<tr>
<td>20</td><td align="center">Conv2DBackpropInput </td><td align="right">Deconvolution </td></tr>
<tr>
<td>21</td><td align="center">Identity </td><td align="right">Ignored, does not appear in the IR </td></tr>
<tr>
<td>22</td><td align="center">Add </td><td align="right">Eltwise(operation = sum) or ScaleShift </td></tr>
<tr>
<td>23</td><td align="center">Mul </td><td align="right">Eltwise(operation = mul) </td></tr>
<tr>
<td>24</td><td align="center">Maximum </td><td align="right">Eltwise(operation = max) </td></tr>
<tr>
<td>25</td><td align="center">Rsqrt </td><td align="right">Power(power=-0.5) </td></tr>
<tr>
<td>26</td><td align="center">Neg </td><td align="right">Power(scale=-1) </td></tr>
<tr>
<td>27</td><td align="center">Sub </td><td align="right">Eltwise(operation = sum) + Power(scale=-1) </td></tr>
<tr>
<td>28</td><td align="center">Relu </td><td align="right">ReLU </td></tr>
<tr>
<td>29</td><td align="center">AvgPool </td><td align="right">Pooling (pool_method=avg) </td></tr>
<tr>
<td>30</td><td align="center">MaxPool </td><td align="right">Pooling (pool_method=max) </td></tr>
<tr>
<td>31</td><td align="center">Mean </td><td align="right">Pooling (pool_method = avg) (sequential reduce dimensions are supported only) </td></tr>
<tr>
<td>32</td><td align="center">RandomUniform </td><td align="right">Not supported </td></tr>
<tr>
<td>33</td><td align="center">BiasAdd </td><td align="right">Fused or converted to ScaleShift </td></tr>
<tr>
<td>34</td><td align="center">Reshape </td><td align="right">Reshape </td></tr>
<tr>
<td>35</td><td align="center">Squeeze </td><td align="right">Squeeze </td></tr>
<tr>
<td>36</td><td align="center">Shape </td><td align="right">Constant propagation (or layer generation if the "--keep_shape_ops" command line parameter has been specified) </td></tr>
<tr>
<td>37</td><td align="center">Softmax </td><td align="right">SoftMax </td></tr>
<tr>
<td>38</td><td align="center">SpaceToBatchND </td><td align="right">Supported in a pattern when converted to Convolution layer dilation attribute, Constant propagation </td></tr>
<tr>
<td>39</td><td align="center">BatchToSpaceND </td><td align="right">Supported in a pattern when converted to Convolution layer dilation attribute, Constant propagation </td></tr>
<tr>
<td>40</td><td align="center">StopGradient </td><td align="right">Ignored, does not appear in IR </td></tr>
<tr>
<td>41</td><td align="center">Square </td><td align="right">Constant propagation </td></tr>
<tr>
<td>42</td><td align="center">Sum </td><td align="right">Pool(pool_method = avg) + Eltwise(operation = mul) </td></tr>
<tr>
<td>43</td><td align="center">Range </td><td align="right">Constant propagation </td></tr>
<tr>
<td>44</td><td align="center">CropAndResize </td><td align="right">ROIPooling (if the the method is 'bilinear') </td></tr>
<tr>
<td>45</td><td align="center">ArgMax </td><td align="right">ArgMax </td></tr>
<tr>
<td>46</td><td align="center">DepthToSpace</td><td align="right">Reshape + Permute + Reshape (works for CPU only because of 6D tensors) </td></tr>
<tr>
<td>47</td><td align="center">ExtractImagePatches </td><td align="right">ReorgYolo </td></tr>
<tr>
<td>48</td><td align="center">ResizeBilinear </td><td align="right">Interp </td></tr>
<tr>
<td>49</td><td align="center">ResizeNearestNeighbor </td><td align="right">Resample </td></tr>
<tr>
<td>50</td><td align="center">Unpack </td><td align="right">Split + Reshape (removes dimension being unpacked) if the number of parts is equal to size along given axis </td></tr>
<tr>
<td>51</td><td align="center">AddN </td><td align="right">Several Eltwises </td></tr>
<tr>
<td>52</td><td align="center">Concat </td><td align="right">Concat </td></tr>
<tr>
<td>53</td><td align="center">Minimum </td><td align="right">Power(scale=-1) + Eltwise(operation = max) + Power(scale=-1) </td></tr>
<tr>
<td>54</td><td align="center">TopkV2 </td><td align="right">TopK </td></tr>
<tr>
<td>55</td><td align="center">RealDiv </td><td align="right">Power(power = -1) and Eltwise(operation = mul) </td></tr>
<tr>
<td>56</td><td align="center">SquaredDifference </td><td align="right">Power(scale = -1) + Eltwise(operation = sum) + Power(power = 2) </td></tr>
<tr>
<td>57</td><td align="center">Gather </td><td align="right">Gather </td></tr>
<tr>
<td>58</td><td align="center">GatherV2 </td><td align="right">Gather </td></tr>
<tr>
<td>59</td><td align="center">ResourceGather</td><td align="right">Gather </td></tr>
<tr>
<td>60</td><td align="center">Sqrt </td><td align="right">Power(power=0.5) </td></tr>
<tr>
<td>61</td><td align="center">Square</td><td align="right">Power(power=2) </td></tr>
<tr>
<td>62</td><td align="center">Pad </td><td align="right">Pad </td></tr>
<tr>
<td>63</td><td align="center">PadV2 </td><td align="right">Pad </td></tr>
<tr>
<td>64</td><td align="center">MirrorPad </td><td align="right">Pad </td></tr>
<tr>
<td>65</td><td align="center">ReverseSequence </td><td align="right">ReverseSequence </td></tr>
<tr>
<td>66</td><td align="center">ZerosLike </td><td align="right">Constant propagation </td></tr>
<tr>
<td>67</td><td align="center">Fill </td><td align="right">Broadcast </td></tr>
<tr>
<td>68</td><td align="center">Cast </td><td align="right">Cast to the following data types are removed from the graph float32, double, int32, int64 </td></tr>
<tr>
<td>69</td><td align="center">Enter </td><td align="right">Supported only when it is fused to the TensorIterator layer </td></tr>
<tr>
<td>70</td><td align="center">Exit </td><td align="right">Supported only when it is fused to the TensorIterator layer </td></tr>
<tr>
<td>71</td><td align="center">LoopCond </td><td align="right">Supported only when it is fused to the TensorIterator layer </td></tr>
<tr>
<td>72</td><td align="center">Merge </td><td align="right">Supported only when it is fused to the TensorIterator layer </td></tr>
<tr>
<td>73</td><td align="center">NextIteration </td><td align="right">Supported only when it is fused to the TensorIterator layer </td></tr>
<tr>
<td>74</td><td align="center">TensorArrayGatherV3 </td><td align="right">Supported only when it is fused to the TensorIterator layer </td></tr>
<tr>
<td>75</td><td align="center">TensorArrayReadV3 </td><td align="right">Supported only when it is fused to the TensorIterator layer </td></tr>
<tr>
<td>76</td><td align="center">TensorArrayScatterV3 </td><td align="right">Supported only when it is fused to the TensorIterator layer </td></tr>
<tr>
<td>77</td><td align="center">TensorArraySizeV3 </td><td align="right">Supported only when it is fused to the TensorIterator layer </td></tr>
<tr>
<td>78</td><td align="center">TensorArrayV3 </td><td align="right">Supported only when it is fused to the TensorIterator layer </td></tr>
<tr>
<td>79</td><td align="center">TensorArrayWriteV3 </td><td align="right">Supported only when it is fused to the TensorIterator layer </td></tr>
<tr>
<td>80</td><td align="center">Equal </td><td align="right">Eltwise(operation = equal) </td></tr>
<tr>
<td>81</td><td align="center">Exp </td><td align="right">Eltwise(operation = exp) </td></tr>
<tr>
<td>82</td><td align="center">Greater </td><td align="right">Eltwise(operation = greater) </td></tr>
<tr>
<td>83</td><td align="center">GreaterEqual </td><td align="right">Eltwise(operation = greater_equal) </td></tr>
<tr>
<td>84</td><td align="center">Less </td><td align="right">Eltwise(operation = less) </td></tr>
<tr>
<td>85</td><td align="center">LogicalAnd </td><td align="right">Eltwise(operation = logical_and) </td></tr>
<tr>
<td>86</td><td align="center">Min </td><td align="right">Constant propagation </td></tr>
<tr>
<td>87</td><td align="center">Max </td><td align="right">Reshape + Pooling (pool_method=max) + Reshape </td></tr>
<tr>
<td>88</td><td align="center">GatherNd </td><td align="right">Supported if it can be replaced with Gather </td></tr>
<tr>
<td>89</td><td align="center">PlaceholderWithDefault </td><td align="right">Const </td></tr>
<tr>
<td>90</td><td align="center">Rank </td><td align="right">Constant propagation </td></tr>
<tr>
<td>91</td><td align="center">Round </td><td align="right">Constant propagation </td></tr>
<tr>
<td>92</td><td align="center">Sigmoid </td><td align="right">Activation(operation = sigmoid) </td></tr>
<tr>
<td>93</td><td align="center">Size </td><td align="right">Constant propagation </td></tr>
<tr>
<td>94</td><td align="center">Switch </td><td align="right">Control flow propagation </td></tr>
<tr>
<td>94</td><td align="center">Swish </td><td align="right">Mul(x, Sigmoid(x)) </td></tr>
</table>
<h2>Kaldi* Supported Layers and the Mapping to the Intermediate Representation Layers</h2>
<p>Standard Kaldi* Layers:</p>
<table class="doxtable">
<tr>
<th align="left">Number </th><th align="left">Layer Name in Kaldi*</th><th align="left">Layer name in the Intermediate Representation  </th></tr>
<tr>
<td align="left">1</td><td align="left">AddShift </td><td align="left">Will be fused or converted to ScaleShift </td></tr>
<tr>
<td align="left">2</td><td align="left">AffineComponent </td><td align="left">FullyConnected </td></tr>
<tr>
<td align="left">3</td><td align="left">AffineTransform </td><td align="left">FullyConnected </td></tr>
<tr>
<td align="left">4</td><td align="left">ConvolutionalComponent </td><td align="left">Convolution </td></tr>
<tr>
<td align="left">5</td><td align="left">Convolutional1DComponent </td><td align="left">Convolution </td></tr>
<tr>
<td align="left">6</td><td align="left">FixedAffineComponent </td><td align="left">FullyConnected </td></tr>
<tr>
<td align="left">7</td><td align="left">LstmProjected </td><td align="left"><div class="image">
<img src="LSTMNode.png" alt="LSTMNode.png"/>
</div>
 </td></tr>
<tr>
<td align="left">8</td><td align="left">LstmProjectedStreams </td><td align="left">The same as for LstmProjected </td></tr>
<tr>
<td align="left">9</td><td align="left">MaxPoolingComponent </td><td align="left">Pooling (pool_method = max) </td></tr>
<tr>
<td align="left">10</td><td align="left">NormalizeComponent </td><td align="left">ScaleShift </td></tr>
<tr>
<td align="left">11</td><td align="left">RectifiedLinearComponent </td><td align="left">ReLU </td></tr>
<tr>
<td align="left">12</td><td align="left">ParallelComponent <div class="image">
<img src="kaldi_pc.png" alt="kaldi_pc.png"/>
</div>
 </td><td align="left"><div class="image">
<img src="pc.png" alt="pc.png"/>
</div>
 </td></tr>
<tr>
<td align="left">13</td><td align="left">Rescale </td><td align="left">Will be fused or converted to ScaleShift </td></tr>
<tr>
<td align="left">14</td><td align="left">Sigmoid </td><td align="left">Activation (operation = sigmoid) </td></tr>
<tr>
<td align="left">15</td><td align="left">Softmax </td><td align="left">Softmax </td></tr>
<tr>
<td align="left">16</td><td align="left">SoftmaxComponent </td><td align="left">Softmax </td></tr>
<tr>
<td align="left">17</td><td align="left">SpliceComponent </td><td align="left"><div class="image">
<img src="splice.png" alt="splice.png"/>
</div>
 </td></tr>
<tr>
<td align="left">18</td><td align="left">TanhComponent </td><td align="left">Activation (operation = tanh) </td></tr>
</table>
<h2>ONNX* Supported Operators and the mapping to the Intermediate Representation layers</h2>
<p>Standard ONNX* operators:</p>
<table class="doxtable">
<tr>
<th align="left">Number </th><th align="left">Operator name in ONNX* </th><th align="left">Layer type in the Intermediate Representation  </th></tr>
<tr>
<td align="left">1</td><td align="left">Add </td><td align="left">Eltwise(operation = sum) (added 'axis' support) or ScaleShift </td></tr>
<tr>
<td align="left">2</td><td align="left">AveragePool </td><td align="left">Pooling (pool_method=avg) </td></tr>
<tr>
<td align="left">3</td><td align="left">BatchNormalization </td><td align="left">ScaleShift (can be fused into Convlution or FC) </td></tr>
<tr>
<td align="left">4</td><td align="left">Concat </td><td align="left">Concat </td></tr>
<tr>
<td align="left">5</td><td align="left">Constant </td><td align="left">Const </td></tr>
<tr>
<td align="left">6</td><td align="left">Conv </td><td align="left">Convolution </td></tr>
<tr>
<td align="left">7</td><td align="left">ConvTranspose </td><td align="left">Deconvolution (added <code>auto_pad</code> and <code>output_shape</code> attributes support)) </td></tr>
<tr>
<td align="left">8</td><td align="left">Div </td><td align="left">Eltwise(operation = mul)-&gt;Power </td></tr>
<tr>
<td align="left">9</td><td align="left">Dropout </td><td align="left">Ignored, does not apeear in IR </td></tr>
<tr>
<td align="left">10</td><td align="left">Elu </td><td align="left">Activation (ELU) </td></tr>
<tr>
<td align="left">11</td><td align="left">Flatten </td><td align="left">Reshape </td></tr>
<tr>
<td align="left">12</td><td align="left">Gemm </td><td align="left">FullyConnected or GEMM depending on inputs </td></tr>
<tr>
<td align="left">13</td><td align="left">GlobalAveragePool </td><td align="left">Pooling (pool_method=avg) </td></tr>
<tr>
<td align="left">14</td><td align="left">Identity </td><td align="left">Ignored, does not appear in IR </td></tr>
<tr>
<td align="left">15</td><td align="left">LRN </td><td align="left">Norm </td></tr>
<tr>
<td align="left">16</td><td align="left">LeakyRelu </td><td align="left">ReLU </td></tr>
<tr>
<td align="left">17</td><td align="left">MatMul </td><td align="left">FullyConnected </td></tr>
<tr>
<td align="left">17</td><td align="left">MaxPool </td><td align="left">Pooling (pool_method=max) </td></tr>
<tr>
<td align="left">19</td><td align="left">Mul </td><td align="left">Eltwise(operation = mul) (added <code>axis</code> support) </td></tr>
<tr>
<td align="left">20</td><td align="left">Relu </td><td align="left">ReLU </td></tr>
<tr>
<td align="left">21</td><td align="left">Reshape </td><td align="left">Reshape </td></tr>
<tr>
<td align="left">22</td><td align="left">Shape </td><td align="left">Constant propagation </td></tr>
<tr>
<td align="left">23</td><td align="left">Softmax </td><td align="left">SoftMax </td></tr>
<tr>
<td align="left">24</td><td align="left">Squeeze </td><td align="left">Squeeze </td></tr>
<tr>
<td align="left">25</td><td align="left">Sub </td><td align="left">Power-&gt;Eltwise(operation = sum) </td></tr>
<tr>
<td align="left">26</td><td align="left">Sum </td><td align="left">Eltwise(operation = sum) </td></tr>
<tr>
<td align="left">27</td><td align="left">Transpose </td><td align="left">Permute </td></tr>
<tr>
<td align="left">28</td><td align="left">Unsqueeze </td><td align="left">Reshape </td></tr>
<tr>
<td align="left">29</td><td align="left">Upsample </td><td align="left">Resample </td></tr>
<tr>
<td align="left">30</td><td align="left">ImageScaler </td><td align="left">ScaleShift </td></tr>
<tr>
<td align="left">31</td><td align="left">Affine </td><td align="left">ScaleShift </td></tr>
<tr>
<td align="left">32</td><td align="left">Reciprocal </td><td align="left">Power(power=-1) </td></tr>
<tr>
<td align="left">33</td><td align="left">Crop </td><td align="left">Split </td></tr>
<tr>
<td align="left">34</td><td align="left">Tanh </td><td align="left">Activation (operation = tanh) </td></tr>
<tr>
<td align="left">35</td><td align="left">Sigmoid </td><td align="left">Activation (operation = sigmoid) </td></tr>
<tr>
<td align="left">36</td><td align="left">Pow </td><td align="left">Power </td></tr>
<tr>
<td align="left">37</td><td align="left">ConvTranspose </td><td align="left"></td></tr>
<tr>
<td align="left">38</td><td align="left">Gather </td><td align="left">Gather </td></tr>
<tr>
<td align="left">39</td><td align="left">ConstantFill </td><td align="left">Constant propagation </td></tr>
<tr>
<td align="left">40</td><td align="left">ReduceMean </td><td align="left">Reshape + Pooling(pool_method=avg) + Reshape (sequential reduce dimensions are supported only) </td></tr>
<tr>
<td align="left">41</td><td align="left">ReduceSum </td><td align="left">Reshape + Pooling(pool_method=avg) + Power(scale=reduce_dim_size) + Reshape (sequential reduce dimensions are supported only) </td></tr>
<tr>
<td align="left">42</td><td align="left">Gather </td><td align="left">Gather </td></tr>
<tr>
<td align="left">43</td><td align="left">Gemm </td><td align="left">GEMM </td></tr>
<tr>
<td align="left">44</td><td align="left">GlobalMaxPool </td><td align="left">Pooling (pool_method=max) </td></tr>
<tr>
<td align="left">45</td><td align="left">Neg </td><td align="left">Power(scale=-1) </td></tr>
<tr>
<td align="left">46</td><td align="left">Pad </td><td align="left">Pad </td></tr>
<tr>
<td align="left">47</td><td align="left">ArgMax </td><td align="left">ArgMax </td></tr>
<tr>
<td align="left">48</td><td align="left">Clip </td><td align="left">Clamp </td></tr>
<tr>
<td align="left">49</td><td align="left">DetectionOutput (Intel experimental) </td><td align="left">DetectionOutputONNX </td></tr>
<tr>
<td align="left">50</td><td align="left">PriorBox (Intel experimental) </td><td align="left">PriorBoxONNX </td></tr>
<tr>
<td align="left">51</td><td align="left">RNN </td><td align="left">TensorIterator(with RNNCell in a body) </td></tr>
<tr>
<td align="left">52</td><td align="left">GRU </td><td align="left">TensorIterator(with GRUCell in a body) </td></tr>
<tr>
<td align="left">53</td><td align="left">LSTM </td><td align="left">TensorIterator(with LSTMCell in a body) </td></tr>
<tr>
<td align="left">54</td><td align="left">FakeQuantize (Intel experimental) </td><td align="left">FakeQuantize </td></tr>
<tr>
<td align="left">55</td><td align="left">Erf </td><td align="left">Erf </td></tr>
<tr>
<td align="left">56</td><td align="left">BatchMatMul </td><td align="left">GEMM </td></tr>
<tr>
<td align="left">57</td><td align="left">SpaceToDepth </td><td align="left">Reshape + Permute + Reshape </td></tr>
<tr>
<td align="left">58</td><td align="left">Fill </td><td align="left">Broadcast </td></tr>
<tr>
<td align="left">59</td><td align="left">Select </td><td align="left">Select </td></tr>
<tr>
<td align="left">60</td><td align="left">OneHot </td><td align="left">OneHot </td></tr>
<tr>
<td align="left">61</td><td align="left">TopK </td><td align="left">TopK </td></tr>
<tr>
<td align="left">62</td><td align="left">GatherTree </td><td align="left">GatherTree </td></tr>
<tr>
<td align="left">63</td><td align="left">LogicalAnd </td><td align="left">Eltwise(operation = LogicalAnd) </td></tr>
<tr>
<td align="left">64</td><td align="left">LogicalOr </td><td align="left">Eltwise(operation = LogicalOr) </td></tr>
<tr>
<td align="left">65</td><td align="left">Equal </td><td align="left">Eltwise(operation = Equal) </td></tr>
<tr>
<td align="left">66</td><td align="left">NotEqual </td><td align="left">Eltwise(operation = NotEqual) </td></tr>
<tr>
<td align="left">67</td><td align="left">Less </td><td align="left">Eltwise(operation = Less) </td></tr>
<tr>
<td align="left">68</td><td align="left">LessEqual </td><td align="left">Eltwise(operation = LessEqual) </td></tr>
<tr>
<td align="left">69</td><td align="left">Greater </td><td align="left">Eltwise(operation = Greater) </td></tr>
<tr>
<td align="left">70</td><td align="left">GreaterEqual </td><td align="left">Eltwise(operation = GreaterEqual) </td></tr>
<tr>
<td align="left">71</td><td align="left">ConstantOfShape </td><td align="left">Broadcast </td></tr>
<tr>
<td align="left">72</td><td align="left">Expand </td><td align="left">Broadcast </td></tr>
<tr>
<td align="left">73</td><td align="left">Not </td><td align="left">Activation (operation = not) </td></tr>
<tr>
<td align="left">74</td><td align="left">ReduceMin </td><td align="left">ReduceMin </td></tr>
<tr>
<td align="left">75</td><td align="left">NonMaxSuppression </td><td align="left">NonMaxSuppression </td></tr>
<tr>
<td align="left">76</td><td align="left">Floor </td><td align="left">Activation (operation = floor) </td></tr>
<tr>
<td align="left">77</td><td align="left">Slice </td><td align="left">Split or StridedSlice </td></tr>
</table>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>