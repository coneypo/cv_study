<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Convert GNMT* Model to the Intermediate Representation (IR) - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Convert GNMT* Model to the Intermediate Representation (IR) </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This tutorial explains how to convert Google* Neural Machine Translation (GNMT) model to the Intermediate Representation (IR).</p>
<p>On GitHub*, you can find several public versions of TensorFlow* GNMT model implementation. This tutorial explains how to convert the GNMT model from the <a href="https://github.com/tensorflow/nmt">TensorFlow* Neural Machine Translation (NMT) repository</a> to the IR.</p>
<h2>Create a Patch File <a class="anchor" id="patch-file"></a></h2>
<p>Before converting the model, you need to create a patch file for the repository. The patch modifies the framework code by adding a special command-line argument to the framework options that enables inference graph dumping:</p>
<ol type="1">
<li>Go to a writable directory and create a <code>GNMT_inference.patch</code> file.</li>
<li>Copy the following diff code to the file: <div class="fragment"><div class="line">diff --git a/nmt/inference.py b/nmt/inference.py</div><div class="line">index 2cbef07..e185490 100644</div><div class="line">--- a/nmt/inference.py</div><div class="line">+++ b/nmt/inference.py</div><div class="line">@@ -17,9 +17,11 @@</div><div class="line"> from __future__ import print_function</div><div class="line"></div><div class="line"> import codecs</div><div class="line">+import os</div><div class="line"> import time</div><div class="line"></div><div class="line"> import tensorflow as tf</div><div class="line">+from tensorflow.python.framework import graph_io</div><div class="line"></div><div class="line"> from . import attention_model</div><div class="line"> from . import gnmt_model</div><div class="line">@@ -105,6 +107,29 @@ def start_sess_and_load_model(infer_model, ckpt_path):</div><div class="line">   return sess, loaded_infer_model</div><div class="line"></div><div class="line"></div><div class="line">+def inference_dump_graph(ckpt_path, path_to_dump, hparams, scope=None):</div><div class="line">+    model_creator = get_model_creator(hparams)</div><div class="line">+    infer_model = model_helper.create_infer_model(model_creator, hparams, scope)</div><div class="line">+    sess = tf.Session(</div><div class="line">+        graph=infer_model.graph, config=utils.get_config_proto())</div><div class="line">+    with infer_model.graph.as_default():</div><div class="line">+        loaded_infer_model = model_helper.load_model(</div><div class="line">+            infer_model.model, ckpt_path, sess, &quot;infer&quot;)</div><div class="line">+    utils.print_out(&quot;Dumping inference graph to {}&quot;.format(path_to_dump))</div><div class="line">+    loaded_infer_model.saver.save(</div><div class="line">+        sess,</div><div class="line">+        os.path.join(path_to_dump + &#39;inference_GNMT_graph&#39;)</div><div class="line">+        )</div><div class="line">+    utils.print_out(&quot;Dumping done!&quot;)</div><div class="line">+</div><div class="line">+    output_node_name = &#39;index_to_string_Lookup&#39;</div><div class="line">+    utils.print_out(&quot;Freezing GNMT graph with output node {}...&quot;.format(output_node_name))</div><div class="line">+    frozen = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,</div><div class="line">+                                                          [output_node_name])</div><div class="line">+    graph_io.write_graph(frozen, &#39;.&#39;, os.path.join(path_to_dump, &#39;frozen_GNMT_inference_graph.pb&#39;), as_text=False)</div><div class="line">+    utils.print_out(&quot;Freezing done. Freezed model frozen_GNMT_inference_graph.pb saved to {}&quot;.format(path_to_dump))</div><div class="line">+</div><div class="line">+</div><div class="line"> def inference(ckpt_path,</div><div class="line">               inference_input_file,</div><div class="line">               inference_output_file,</div><div class="line">diff --git a/nmt/nmt.py b/nmt/nmt.py</div><div class="line">index f5823d8..a733748 100644</div><div class="line">--- a/nmt/nmt.py</div><div class="line">+++ b/nmt/nmt.py</div><div class="line">@@ -310,6 +310,13 @@ def add_arguments(parser):</div><div class="line">   parser.add_argument(&quot;--num_intra_threads&quot;, type=int, default=0,</div><div class="line">                       help=&quot;number of intra_op_parallelism_threads&quot;)</div><div class="line"></div><div class="line">+  # Special argument for inference model dumping without inference</div><div class="line">+  parser.add_argument(&quot;--dump_inference_model&quot;, type=&quot;bool&quot;, nargs=&quot;?&quot;,</div><div class="line">+                      const=True, default=False,</div><div class="line">+                      help=&quot;Argument for dump inference graph for specified trained ckpt&quot;)</div><div class="line">+</div><div class="line">+  parser.add_argument(&quot;--path_to_dump&quot;, type=str, default=&quot;&quot;,</div><div class="line">+                      help=&quot;Path to dump inference graph.&quot;)</div><div class="line"></div><div class="line"> def create_hparams(flags):</div><div class="line">   &quot;&quot;&quot;Create training hparams.&quot;&quot;&quot;</div><div class="line">@@ -396,6 +403,9 @@ def create_hparams(flags):</div><div class="line">       language_model=flags.language_model,</div><div class="line">       num_intra_threads=flags.num_intra_threads,</div><div class="line">       num_inter_threads=flags.num_inter_threads,</div><div class="line">+</div><div class="line">+      dump_inference_model=flags.dump_inference_model,</div><div class="line">+      path_to_dump=flags.path_to_dump,</div><div class="line">   )</div><div class="line"></div><div class="line"></div><div class="line">@@ -613,7 +623,7 @@ def create_or_load_hparams(</div><div class="line">   return hparams</div><div class="line"></div><div class="line"></div><div class="line">-def run_main(flags, default_hparams, train_fn, inference_fn, target_session=&quot;&quot;):</div><div class="line">+def run_main(flags, default_hparams, train_fn, inference_fn, inference_dump, target_session=&quot;&quot;):</div><div class="line">   &quot;&quot;&quot;Run main.&quot;&quot;&quot;</div><div class="line">   # Job</div><div class="line">   jobid = flags.jobid</div><div class="line">@@ -653,8 +663,26 @@ def run_main(flags, default_hparams, train_fn, inference_fn, target_session=&quot;&quot;):</div><div class="line">         out_dir, default_hparams, flags.hparams_path,</div><div class="line">         save_hparams=(jobid == 0))</div><div class="line"></div><div class="line">-  ## Train / Decode</div><div class="line">-  if flags.inference_input_file:</div><div class="line">+  #  Dumping inference model</div><div class="line">+  if flags.dump_inference_model:</div><div class="line">+      # Inference indices</div><div class="line">+      hparams.inference_indices = None</div><div class="line">+      if flags.inference_list:</div><div class="line">+          (hparams.inference_indices) = (</div><div class="line">+              [int(token) for token in flags.inference_list.split(&quot;,&quot;)])</div><div class="line">+</div><div class="line">+      # Ckpt</div><div class="line">+      ckpt = flags.ckpt</div><div class="line">+      if not ckpt:</div><div class="line">+          ckpt = tf.train.latest_checkpoint(out_dir)</div><div class="line">+</div><div class="line">+      # Path to dump graph</div><div class="line">+      assert flags.path_to_dump != &quot;&quot;, &quot;Please, specify path_to_dump model.&quot;</div><div class="line">+      path_to_dump = flags.path_to_dump</div><div class="line">+      if not tf.gfile.Exists(path_to_dump): tf.gfile.MakeDirs(path_to_dump)</div><div class="line">+</div><div class="line">+      inference_dump(ckpt, path_to_dump, hparams)</div><div class="line">+  elif flags.inference_input_file:</div><div class="line">     # Inference output directory</div><div class="line">     trans_file = flags.inference_output_file</div><div class="line">     assert trans_file</div><div class="line">@@ -693,7 +721,8 @@ def main(unused_argv):</div><div class="line">   default_hparams = create_hparams(FLAGS)</div><div class="line">   train_fn = train.train</div><div class="line">   inference_fn = inference.inference</div><div class="line">-  run_main(FLAGS, default_hparams, train_fn, inference_fn)</div><div class="line">+  inference_dump = inference.inference_dump_graph</div><div class="line">+  run_main(FLAGS, default_hparams, train_fn, inference_fn, inference_dump)</div><div class="line"></div><div class="line"></div><div class="line"> if __name__ == &quot;__main__&quot;:</div></div><!-- fragment --></li>
<li>Save and close the file.</li>
</ol>
<h2>Convert GNMT Model to the IR</h2>
<blockquote class="doxtable">
<p><b>NOTE</b>: Please, use TensorFlow version 1.13 or lower. </p>
</blockquote>
<p><b>Step 1</b>. Clone the GitHub repository and check out the commit:</p>
<ol type="1">
<li>Clone the NMT reposirory: <div class="fragment"><div class="line">git clone https://github.com/tensorflow/nmt.git</div></div><!-- fragment --></li>
<li>Check out the necessary commit: <div class="fragment"><div class="line">git checkout b278487980832417ad8ac701c672b5c3dc7fa553</div></div><!-- fragment --></li>
</ol>
<p><b>Step 2</b>. Get a trained model. You have two options:</p>
<ul>
<li>Train the model with the GNMT <code>wmt16_gnmt_4_layer.json</code> or <code>wmt16_gnmt_8_layer.json</code> configuration file using the NMT framework.</li>
<li>Use the pretrained checkpoints provided in the NMT repository. Refer to the <a href="https://github.com/tensorflow/nmt#benchmarks">Benchmarks</a> section for more information (<em>checkpoints in this section are outdated and can be incompatible with the current repository version. To avoid confusion, train a model by yourself</em>).</li>
</ul>
<p>This tutorial assumes the use of the trained GNMT model from <code>wmt16_gnmt_4_layer.json</code> config, German to English translation.</p>
<p><b>Step 3</b>. Create an inference graph:</p>
<p>The OpenVINO&trade; assumes that a model is used for inference only. Hence, before converting the model into the IR, you need to transform the training graph into the inference graph. For the GNMT model, the training graph and the inference graph have different decoders: the training graph uses a greedy search decoding algorithm, while the inference graph uses a beam search decoding algorithm.</p>
<ol type="1">
<li>Apply the <code>GNMT_inference.patch</code> patch to the repository. Refer to the <a href="#patch-file">Create a Patch File</a> instructions if you do not have it: <div class="fragment"><div class="line">git apply /path/to/patch/GNMT_inference.patch</div></div><!-- fragment --></li>
<li>Run the NMT framework to dump the inference model:</li>
</ol>
<div class="fragment"><div class="line">python -m nmt.nmt</div><div class="line">    --src=de</div><div class="line">    --tgt=en</div><div class="line">    --ckpt=/path/to/ckpt/translate.ckpt</div><div class="line">    --hparams_path=/path/to/repository/nmt/nmt/standard_hparams/wmt16_gnmt_4_layer.json</div><div class="line">    --vocab_prefix=/path/to/vocab/vocab.bpe.32000</div><div class="line">    --out_dir=&quot;&quot;</div><div class="line">    --dump_inference_model</div><div class="line">    --infer_mode beam_search</div><div class="line">    --path_to_dump /path/to/dump/model/</div></div><!-- fragment --><p>If you use different checkpoints, use the corresponding values for the <code>src</code>,<code>tgt</code>,<code>ckpt</code>,<code>hparams_path</code>, and <code>vocab_prefix</code> parameters. Inference checkpoint <code>inference_GNMT_graph</code> and frozen inference graph <code>frozen_GNMT_inference_graph.pb</code> will appear in the <code>/path/to/dump/model/</code> folder.</p>
<p><b>Step 4</b>. Convert the model to the IR:</p>
<div class="fragment"><div class="line">python3 path/to/model_optimizer/mo_tf.py</div><div class="line">--input_model /path/to/dump/model/frozen_GNMT_inference_graph.pb</div><div class="line">--input &quot;IteratorGetNext:1[1],IteratorGetNext:0[1 50],dynamic_seq2seq/Cast_1:0[1]-&gt;[2],dynamic_seq2seq/Cast:0[1]-&gt;[1]&quot;</div><div class="line">--output dynamic_seq2seq/decoder/decoder/GatherTree</div><div class="line">--output_dir /path/to/output/IR/</div></div><!-- fragment --><p>Input and output cutting with the <code>--input</code> and <code>--output</code> options is required since OpenVINO&trade; does not support <code>IteratorGetNext</code> and <code>LookupTableFindV2</code> operations.</p>
<p>Input cutting:</p>
<ul>
<li><code>IteratorGetNext</code> operation iterates over a dataset. It is cut by output ports: port 0 contains data tensor with shape <code>[batch_size, max_sequence_length]</code>, port 1 contains <code>sequence_length</code> for every batch with shape <code>[batch_size]</code>.</li>
<li><code>LookupTableFindV2</code> operations (<code>dynamic_seq2seq/hash_table_Lookup_1</code> and <code>dynamic_seq2seq/hash_table_Lookup</code> nodes in the graph) are cut with constant values by next operations (<code>dynamic_seq2seq/Cast_1</code> and <code>dynamic_seq2seq/Cast</code> respectively).</li>
</ul>
<p>Output cutting:</p>
<ul>
<li><code>LookupTableFindV2</code> operation is cut from the output and the <code>dynamic_seq2seq/decoder/decoder/GatherTree</code> node is treated as a new exit point.</li>
</ul>
<p>For more information about model cutting, refer to <a class="el" href="_docs_MO_DG_prepare_model_convert_model_Cutting_Model.html">Cutting Off Parts of a Model</a>.</p>
<h2>How to Use GNMT Model <a class="anchor" id="run_GNMT"></a></h2>
<blockquote class="doxtable">
<p><b>NOTE</b>: This step assumes you have converted a model to the Intermediate Representation. </p>
</blockquote>
<p>Inputs of the model:</p><ul>
<li><code>IteratorGetNext/placeholder_out_port_0</code> input with shape <code>[batch_size, max_sequence_length]</code> contains <code>batch_size</code> decoded input sentences. Every sentence is decoded the same way as indices of sentence elements in vocabulary and padded with index of <code>eos</code> (end of sentence symbol). If the length of the sentence is less than <code>max_sequence_length</code>, remaining elements are filled with index of <code>eos</code> token.</li>
<li><code>IteratorGetNext/placeholder_out_port_1</code> input with shape <code>[batch_size]</code> contains sequence lengths for every sentence from the first input. \ For example, if <code>max_sequence_length = 50</code>, <code>batch_size = 1</code> and the sentence has only 30 elements, then the input tensor for <code>IteratorGetNext/placeholder_out_port_1</code> should be <code>[30]</code>.</li>
</ul>
<p>Outputs of the model:</p>
<ul>
<li><code>dynamic_seq2seq/decoder/decoder/GatherTree</code> tensor with shape <code>[max_sequence_length * 2, batch, beam_size]</code>, that contains <code>beam_size</code> best translations for every sentence from input (also decoded as indices of words in vocabulary). \ <blockquote class="doxtable">
<p><b>NOTE</b>: Shape of this tensor in TensorFlow* can be different: instead of <code>max_sequence_length * 2</code>, it can be any value less than that, because OpenVINO&trade; does not support dynamic shapes of outputs, while TensorFlow can stop decoding iterations when <code>eos</code> symbol is generated.* </p>
</blockquote>
</li>
</ul>
<h4>How to RUN GNMT IR <a class="anchor" id="run_GNMT"></a></h4>
<ol type="1">
<li>With benchmark app: <div class="fragment"><div class="line">python3 benchmark_app.py -m &lt;path to the generated GNMT IR&gt; -d CPU</div></div><!-- fragment --></li>
<li>With Inference Engine Python API:</li>
</ol>
<blockquote class="doxtable">
<p><b>NOTE</b>: Before running the example, insert a path to your GNMT <code>.xml</code> and <code>.bin</code> files into <code>MODEL_PATH</code> and <code>WEIGHTS_PATH</code>, and fill <code>input_data_tensor</code> and <code>seq_lengths</code> tensors according to your input data. </p>
</blockquote>
<div class="fragment"><div class="line">from openvino.inference_engine import IENetwork, IECore</div><div class="line"></div><div class="line">MODEL_PATH = &#39;/path/to/IR/frozen_GNMT_inference_graph.xml&#39;</div><div class="line">WEIGHTS_PATH = &#39;/path/to/IR/frozen_GNMT_inference_graph.bin&#39;</div><div class="line"></div><div class="line"># Creating network</div><div class="line">net = IENetwork(</div><div class="line">    model=MODEL_PATH,</div><div class="line">    weights=WEIGHTS_PATH)</div><div class="line"></div><div class="line"># Creating input data</div><div class="line">input_data = {&#39;IteratorGetNext/placeholder_out_port_0&#39;: input_data_tensor,</div><div class="line">              &#39;IteratorGetNext/placeholder_out_port_1&#39;: seq_lengths}</div><div class="line"></div><div class="line"># Creating plugin and loading extensions</div><div class="line">ie = IECore()</div><div class="line">ie.add_extension(extension_path=&quot;libcpu_extension.so&quot;, device_name=&quot;CPU&quot;)</div><div class="line"></div><div class="line"># Loading network</div><div class="line">exec_net = ie.load_network(network=net, device_name=&quot;CPU&quot;)</div><div class="line"></div><div class="line"># Run inference</div><div class="line">result_ie = exec_net.infer(input_data)</div></div><!-- fragment --><p>For more information about Python API, refer to <a class="el" href="_inference_engine_ie_bridges_python_docs_api_overview.html">Inference Engine Python API Overview</a>. </p>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>