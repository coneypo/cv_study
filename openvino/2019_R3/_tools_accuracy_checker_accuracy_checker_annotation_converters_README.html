<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Annotation Converters - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Annotation Converters </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Annotation converter is a function which converts annotation file to suitable for metric evaluation format. Each annotation converter expects specific annotation file format or data structure, which depends on original dataset. If converter for your data format is not supported by Accuracy Checker, you can provide your own annotation converter. Each annotation converter has parameters available for configuration.</p>
<p>Process of conversion can be implemented in two ways:</p><ul>
<li>via configuration file</li>
<li>via command line</li>
</ul>
<h3>Describing annotation conversion in configuration file.</h3>
<p>Annotation conversion can be provided in <code>dataset</code> section your configuration file to convert annotation inplace before every evaluation. Each conversion configuration should contain <code>converter</code> field filled selected converter name and provide converter specific parameters (more details in supported converters section). All paths can be prefixed via command line with <code>-s, --source</code> argument.</p>
<p>You can additionally use optional parameters like:</p><ul>
<li><code>subsample_size</code> - Dataset subsample size. You can specify the number of ground truth objects or dataset ratio in percentage. Please, be careful to use this option, some datasets does not support subsampling. You can also specify <code>subsample_seed</code> if you want to generate subsample with specific random seed.</li>
<li><code>annotation</code> - path to store converted annotation pickle file. You can use this parameter if you need to reuse converted annotation to avoid subsequent conversions.</li>
<li><code>dataset_meta</code> - path to store mata information about converted annotation if it is provided.</li>
<li><code>analyze_dataset</code> - flag which allow to get statistics about converted dataset. Supported annotations: <code>ClassificationAnnotation</code>, <code>DetectionAnnotation</code>, <code>MultiLabelRecognitionAnnotation</code>, <code>RegressionAnnotation</code>. Default value is False.</li>
</ul>
<p>Example of usage:</p>
<div class="fragment"><div class="line">annotation_conversion:</div><div class="line">  # Converter name which will be called for conversion.</div><div class="line">  converter: sample</div><div class="line">  # Converter specific parameters, can be different depend on converter realization.</div><div class="line">  data_dir: sample/sample_dataset</div><div class="line"># (Optional) subsample generation. Can be also used with prepared annotation file.</div><div class="line">subsample_size: 1000</div><div class="line"># (Optional) paths to store annotation files for following usage. In the next evaluation these files will be directly used instead running conversion.</div><div class="line">annotation: sample_dataset.pickle</div><div class="line">dataset_meta: sample_dataset.json</div></div><!-- fragment --><h3>Conversing process via command line.</h3>
<p>The command line for annotation conversion looks like:</p>
<div class="fragment"><div class="line">convert_annotation &lt;converter_name&gt; &lt;converter_specific parameters&gt;</div></div><!-- fragment --><p> All converter specific options should have format <code>--&lt;parameter_name&gt; &lt;parameter_value&gt;</code> You may refer to <code>-h, --help</code> to full list of command line options. Some optional arguments are:</p>
<ul>
<li><code>-o, --output_dir</code> - directory to save converted annotation and meta info.</li>
<li><code>-a, --annotation_name</code> - annotation file name.</li>
<li><code>-m, --meta_name</code> - meta info file name.</li>
</ul>
<h3>Supported converters</h3>
<p>Accuracy Checker supports following list of annotation converters and specific for them parameters:</p><ul>
<li><code>cifar10</code> - converts CIFAR 10 classification dataset to <code>ClassificationAnnotation</code><ul>
<li><code>data_batch_file</code> - path to pickle file which contain dataset batch (e.g. test_batch)</li>
<li><code>has_background</code> - allows to add background label to original labels and convert dataset for 11 classes instead 10 (default value is False).</li>
<li><code>convert_images</code> - allows to convert images from pickle file to user specified directory (default value is False).</li>
<li><code>converted_images_dir</code> - path to converted images location.</li>
</ul>
</li>
<li><code>mnist_csv</code> - convert MNIST dataset for handwritten digit recognition stored in csv format to <code>ClassificationAnnotation</code>.<ul>
<li><code>annotation_file</code> - path to dataset file in csv format.</li>
<li><code>convert_images</code> - allows to convert images from annotation file to user specified directory (default value is False).</li>
<li><code>converted_images_dir</code> - path to converted images location if enabled <code>convert_images</code>.</li>
</ul>
</li>
<li><code>imagenet</code> - convert ImageNet dataset for image classification task to <code>ClassificationAnnotation</code>.<ul>
<li><code>annotation_file</code> - path to annotation in txt format.</li>
<li><code>labels_file</code> - path to file with word description of labels (synset_words).</li>
<li><code>has_background</code> - allows to add background label to original labels and convert dataset for 1001 classes instead 1000 (default value is False).</li>
</ul>
</li>
<li><code>voc_detection</code> - converts Pascal VOC annotation for detection task to <code>DetectionAnnotation</code>.<ul>
<li><code>imageset_file</code> - path to file with validation image list.</li>
<li><code>annotations_dir</code> - path to directory with annotation files.</li>
<li><code>images_dir</code> - path to directory with images related to devkit root (default JPEGImages).</li>
<li><code>has_background</code> - allows convert dataset with/without adding background_label. Accepted values are True or False. (default is True)</li>
</ul>
</li>
<li><code>voc_segmentation</code> - converts Pascal VOC annotation for semantic segmentation task to <code>SegmentationAnnotation</code>.<ul>
<li><code>imageset_file</code> - path to file with validation image list.</li>
<li><code>images_dir</code> - path to directory with images related to devkit root (default JPEGImages).</li>
<li><code>mask_dir</code> - path to directory with ground truth segmentation masks related to devkit root (default SegmentationClass).</li>
</ul>
</li>
<li><code>mscoco_detection</code> - converts MS COCO dataset for object detection task to <code>DetectionAnnotation</code>.<ul>
<li><code>annotation_file</code> - path ot annotation file in json format.</li>
<li><code>has_background</code> - allows convert dataset with/without adding background_label. Accepted values are True or False. (default is False).</li>
<li><code>use_full_label_map</code> - allows to use original label map (with 91 object categories) from paper instead public available(80 categories).</li>
<li><code>sort_annotations</code> - allows to save annotations in image id ascend order.</li>
</ul>
</li>
<li><code>mscoco_segmentation</code> - converts MS COCO dataset for object instance segmentation task to <code>CocoInstanceSegmentationAnnotation</code>.<ul>
<li><code>annotation_file</code> - path ot annotation file in json format.</li>
<li><code>has_background</code> - allows convert dataset with/without adding background_label. Accepted values are True or False. (default is False).</li>
<li><code>use_full_label_map</code> - allows to use original label map (with 91 object categories) from paper instead public available(80 categories).</li>
<li><code>sort_annotations</code> - allows to save annotations in image id ascend order.</li>
</ul>
</li>
<li><code>mscoco_mask_rcnn</code> - converts MS COCO dataset to <code>ContainerAnnotation</code> with <code>DetectionAnnotation</code> and <code>CocoInstanceSegmentationAnnotation</code> named <code>detection_annotation</code> and <code>segmentation_annotation</code> respectively.<ul>
<li><code>annotation_file</code> - path ot annotation file in json format.</li>
<li><code>has_background</code> - allows convert dataset with/without adding background_label. Accepted values are True or False. (default is False).</li>
<li><code>use_full_label_map</code> - allows to use original label map (with 91 object categories) from paper instead public available(80 categories).</li>
<li><code>sort_annotations</code> - allows to save annotations in image id ascend order.</li>
</ul>
</li>
<li><code>mscoco_keypoints</code> - converts MS COCO dataset for keypoints localization task to <code>PoseEstimationAnnotation</code>.<ul>
<li><code>annotation_file</code> - path ot annotation file in json format.</li>
</ul>
</li>
<li><code>wider</code> - converts from Wider Face dataset to <code>DetectionAnnotation</code>.<ul>
<li><code>annotation_file</code> - path to txt file, which contains ground truth data in WiderFace dataset format.</li>
<li><code>label_start</code> - specifies face label index in label map. Default value is 1. You can provide another value, if you want to use this dataset for separate label validation, in case when your network predicts other class for faces.</li>
</ul>
</li>
<li><code>detection_opencv_storage</code> - converts detection annotation stored in Detection OpenCV storage format to <code>DetectionAnnotation</code>.<ul>
<li><code>annotation_file</code> - path to annotation in xml format.</li>
<li><code>image_names_file</code> - path to txt file, which contains image name list for dataset.</li>
<li><code>label_start</code> - specifies label index start in label map. Default value is 1. You can provide another value, if you want to use this dataset for separate label validation.</li>
<li><code>background_label</code> - specifies which index will be used for background label. You can not provide this parameter if your dataset has not background label.</li>
</ul>
</li>
<li><code>cityscapes</code> - converts CityScapes Dataset to <code>SegmentationAnnotation</code>.<ul>
<li><code>dataset_root_dir</code> - path to dataset root.</li>
<li><code>images_subfolder</code> - path from dataset root to directory with validation images (Optional, default <code>imgsFine/leftImg8bit/val</code>).</li>
<li><code>masks_subfolder</code> - path from dataset root to directory with ground truth masks (Optional, <code>gtFine/val</code>).</li>
<li><code>masks_suffix</code> - suffix for mask file names (Optional, default <code>_gtFine_labelTrainIds</code>).</li>
<li><code>images_suffix</code> - suffix for image file names (Optional, default <code>_leftImg8bit</code>).</li>
<li><code>use_full_label_map</code> - allows to use full label map with 33 classes instead train label map with 18 classes (Optional, default <code>False</code>).</li>
</ul>
</li>
<li><code>vgg_face</code> - converts VGG Face 2 dataset for facial landmarks regression task to <code>FacialLandmarksAnnotation</code>.<ul>
<li><code>landmarks_csv_file</code> - path to csv file with coordinates of landmarks points.</li>
<li><code>bbox_csv_file</code> - path to cvs file which contains bounding box coordinates for faces (optional parameter).</li>
</ul>
</li>
<li><code>lfw</code> - converts Labeled Faces in the Wild dataset for face reidentification to <code>ReidentificationClassificationAnnotation</code>.<ul>
<li><code>pairs_file</code> - path to file with annotation positive and negative pairs.</li>
<li><code>train_file</code> - path to file with annotation positive and negative pairs used for network train (optional parameter).</li>
<li><code>landmarks_file</code> - path to file with facial landmarks coordinates for annotation images (optional parameter).</li>
</ul>
</li>
<li><code>mars</code> - converts MARS person reidentification dataset to <code>ReidentificationAnnotation</code>.<ul>
<li><code>data_dir</code> - path to data directory, where gallery (<code>bbox_test</code>) and <code>query</code> subdirectories are located.</li>
</ul>
</li>
<li><code>market1501_reid</code> - converts Market1501 person reidentification dataset to <code>ReidentificationAnnotation</code>.<ul>
<li><code>data_dir</code> - path to data directory, where gallery (<code>bounding_box_test</code>) and <code>query</code> subdirectories are located.</li>
</ul>
</li>
<li><code>super_resolution</code> - converts dataset for single image super resolution task to <code>SuperResolutionAnnotation</code>.<ul>
<li><code>data_dir</code> - path to folder, where images in low and high resolution are located.</li>
<li><code>lr_suffix</code> - low resolution file name's suffix (default lr).</li>
<li><code>hr_suffix</code> - high resolution file name's suffix (default hr).</li>
<li><code>annotation_loader</code> - which library will be used for ground truth image reading. Supported: <code>opencv</code>, <code>pillow</code> (Optional. Default value is pillow). Note, color space of image depends on loader (OpenCV uses BGR, Pillow uses RGB for image reading).</li>
<li><code>two_streams</code> - enable 2 input streams where usually first for original image and second for upsampled image. (Optional, default False).</li>
<li><code>upsample_suffix</code> - upsample images file name's suffix (default upsample).</li>
</ul>
</li>
<li><code>multi_frame_super_resolution</code> - converts dataset for super resolution task with multiple input frames usage.<ul>
<li><code>data_dir</code> - path to folder, where images in low and high resolution are located.</li>
<li><code>lr_suffix</code> - low resolution file name's suffix (default lr).</li>
<li><code>hr_suffix</code> - high resolution file name's suffix (default hr).</li>
<li><code>annotation_loader</code> - which library will be used for ground truth image reading. Supported: <code>opencv</code>, <code>pillow</code> (Optional. Default value is pillow). Note, color space of image depends on loader (OpenCV uses BGR, Pillow uses RGB for image reading).</li>
<li><code>number_input_frames</code> - the number of input frames per inference.</li>
</ul>
</li>
<li><code>icdar_detection</code> - converts ICDAR13 and ICDAR15 datasets for text detection challenge to <code>TextDetectionAnnotation</code>.<ul>
<li><code>data_dir</code> - path to folder with annotations on txt format.</li>
</ul>
</li>
<li><code>icdar13_recognition</code> - converts ICDAR13 dataset for text recognition task to <code>CharecterRecognitionAnnotation</code>.<ul>
<li><code>annotation_file</code> - path to annotation file in txt format.</li>
</ul>
</li>
<li><code>brats</code> - converts BraTS dataset format to <code>BrainTumorSegmentationAnnotation</code> format.<ul>
<li><code>data_dir</code> - dataset root directory, which contain subdirectories with validation data (<code>imagesTr</code>) and ground truth labels (<code>labelsTr</code>). Optionally you can provide relative path for these subdirectories (if they have different location) using <code>image_folder</code> and <code>mask_folder</code> parameters respectively.</li>
</ul>
</li>
<li><code>movie_lens_converter</code> - converts Movie Lens Datasets format to <code>HitRatioAnnotation</code> format.<ul>
<li><code>rating_file</code> - path to file which contains movieId with top score for each userID (for example ml-1m-test-ratings.csv)</li>
<li><code>negative_file</code> - path to file which contains negative examples.</li>
<li><code>users_max_number</code> - the number of users which will be used for validation (Optional, it gives opportunity to cut list of users. If argument is not provided, full list of users will be used.).</li>
</ul>
</li>
<li><code>brats_numpy</code> - converts Brain Tumor Segmentation dataset to <code>BrainTumorSegmentationAnnotation</code>. This converter works with Numpy representation of BraTS dataset.<ul>
<li><code>data_dir</code> - path to dataset root directory.</li>
<li><code>ids_file</code> - path to file, which contains names of images in dataset</li>
<li><code>labels_file</code> - path to file, which contains labels (optional, if omitted no labels will be shown)</li>
<li><code>data_suffix</code> - suffix for files with data (default <code>_data_cropped</code>)</li>
<li><code>label_suffix</code> - suffix for files with groundtruth data (default <code>_label_cropped</code>)</li>
<li><code>boxes_file</code> - path to file with brain boxes (optional). Set this option with including postprocessor <code>segmentation-prediction-resample</code>(see <a class="el" href="_tools_accuracy_checker_accuracy_checker_postprocessor_README.html">Postprocessors</a>).</li>
</ul>
</li>
<li><code>wmt</code> - converts WMT dataset for Machine Translation task to <code>MachineTranslationAnnotation</code>.<ul>
<li><code>input_file</code> - path to file which contains input sentences tokens for translation.</li>
<li><code>reference_file</code> - path to file with reference for translation.</li>
</ul>
</li>
<li><code>common_semantic_segmentation</code> - converts general format of datasets for semantic segmentation task to <code>SegmentationAnnotation</code>. The converter expects following dataset structure:<ol type="1">
<li>images and GT masks are located in separated directories (e.g. <code>&lt;dataset_root&gt;/images</code> for images and <code>&lt;dataset_root&gt;/masks</code> for masks respectively)</li>
<li>images and GT masks has common part in names and can have difference in prefix and postfix (e.g. image name is image0001.jpeg, mask for it is gt0001.png are acceptable. In this case base_part - 0001, image_prefix - image, image_postfix - .jpeg, mask_prefix - gt, mask_postfix - .png)</li>
</ol>
<ul>
<li><code>images_dir</code> - path to directory with images.</li>
<li><code>masks_dir</code> - path to directory with GT masks.</li>
<li><code>image_prefix</code> - prefix part for image file names. (Optional, default is empty).</li>
<li><code>image_postfix</code> - postfix part for image file names (optional, default is <code>.png</code>).</li>
<li><code>mask_prefix</code> - prefix part for mask file names. (Optional, default is empty).</li>
<li><code>image_postfix</code> - postfix part for mask file names (optional, default is <code>.png</code>).</li>
<li><code>mask_loader</code> - the way how GT mask should be loaded. Supported methods: <code>pillow</code>, <code>opencv</code>, <code>nifti</code>, <code>numpy</code>, <code>scipy</code>.</li>
<li><code>dataset_meta</code> - path to json file with prepared dataset meta info. It should contains <code>label_map</code> key with dictionary in format class_id: class_name and optionally <code>segmentation_colors</code> (if your dataset uses color encoding). Segmentation colors is a list of channel-wise values for each class. (e.g. if your dataset has 3 classes in BGR colors, segmentation colors for it will looks like: <code>[[255, 0, 0], [0, 255, 0], [0, 0, 255]]</code>). (Optional, you can provide self-created file as <code>dataset_meta</code> in your config).</li>
</ul>
</li>
<li><code>camvid</code> - converts CamVid dataset format to <code>SegmentationAnnotation</code>.<ul>
<li><code>annotation_file</code> - file in txt format which contains list of validation pairs (<code>&lt;path_to_image&gt;</code> <code>&lt;path_to_annotation&gt;</code> separated by space)</li>
</ul>
</li>
<li><code>image_retrieval</code> - converts dataset for image retrieval task to <code>ReidentificationAnnotation</code>. Dataset should have following structure:<ol type="1">
<li>the dataset root directory contains 2 subdirectory named <code>gallery</code> and <code>queries</code> for gallery images and query images respectively.</li>
<li>Every of these subdirectories should contains text file with list of pairs: <code>&lt;path_to_image&gt;</code> <code>&lt;image_ID&gt;</code> (image_path and image_ID should be separated by space), where <code>&lt;path_to_image&gt;</code> is path to the image related dataset root, <code>&lt;image_ID&gt;</code> is the number which represent image id in the gallery.</li>
</ol>
<ul>
<li><code>data_dir</code> - path to dataset root directory.</li>
<li><code>gallery_annotation_file</code> - file with gallery images and IDs concordance in txt format (Optional, default value is <code>&lt;data_dir&gt;/gallery/list.txt</code>)</li>
<li><code>queries_annotation_file</code> - file with queries images and IDs concordance in txt format (Optional, default value is <code>&lt;data_dir&gt;/queries/list.txt</code>)</li>
</ul>
</li>
<li><code>cvat_object_detection</code> - converts <a href="https://github.com/opencv/cvat/blob/develop/cvat/apps/documentation/xml_format.md#xml-annotation-format">CVAT XML annotation version 1.1</a> format for images to <code>DetectionAnnotation</code>.<ul>
<li><code>annotation_file</code> - path to xml file in appropriate format.</li>
<li><code>has_background</code> - allows prepend original labels with special class represented background and convert dataset for n+1 classes instead n (default value is True).</li>
</ul>
</li>
<li><code>cvat_attributes_recognition</code> - converts <a href="https://github.com/opencv/cvat/blob/develop/cvat/apps/documentation/xml_format.md#xml-annotation-format">CVAT XML annotation version 1.1</a> format for images to <code>ClassificationAnnotation</code> or <code>ContainerAnnotation</code> with <code>ClassificationAnnotation</code> as value type and attribute names as keys (in multiple attributes case). Used bbox attributes as annotation classes.<ul>
<li><code>annotation_file</code> - path to xml file in appropriate format.</li>
<li><code>label</code> - the dataset label which will be used for attributes collection (e.g. if your dataset contains 2 labels: <code>face</code> and <code>person</code> and you want recognise attributes for face, you should use <code>face</code> as value for this parameter).</li>
</ul>
</li>
<li><code>cvat_age_gender</code> - converts <a href="https://github.com/opencv/cvat/blob/develop/cvat/apps/documentation/xml_format.md#xml-annotation-format">CVAT XML annotation version 1.1</a> format for images which represent dataset for age gender recognition to <code>ContainerAnnotation</code> with <code>ClassificationAnnotation</code> for gender recognition, <code>ClassificationAnnotation</code> for age classification and <code>RegeressionAnnotation</code> for age regression. The identifiers for representations following: <code>gender_annotation</code>, <code>age_class_annotation</code>, <code>age_regression_annotation</code>.<ul>
<li><code>annotation_file</code> - path to xml file in appropriate format.</li>
</ul>
</li>
<li><code>cvat_facial_landmarks</code> - converts <a href="https://github.com/opencv/cvat/blob/develop/cvat/apps/documentation/xml_format.md#xml-annotation-format">CVAT XML annotation version 1.1</a> format for images to <code>FacialLandmarksAnnotation</code>.<ul>
<li><code>annotation_file</code> - path to xml file in appropriate format.</li>
</ul>
</li>
<li><code>cvat_pose_estimation</code> - converts <a href="https://github.com/opencv/cvat/blob/develop/cvat/apps/documentation/xml_format.md#xml-annotation-format">CVAT XML annotation version 1.1</a> format for images to <code>PoseEstimationAnnotation</code>.<ul>
<li><code>annotation_file</code> - path to xml file in appropriate format.</li>
</ul>
</li>
<li><code>cvat_text_recognition</code> - converts <a href="https://github.com/opencv/cvat/blob/develop/cvat/apps/documentation/xml_format.md#xml-annotation-format">CVAT XML annotation version 1.1</a> format for images to <code>CharacterRecognitionAnnotation</code>.<ul>
<li><code>annotation_file</code> - path to xml file in appropriate format.</li>
</ul>
</li>
<li><code>cvat_binary_multilabel_attributes_recognition</code> - converts <a href="https://github.com/opencv/cvat/blob/develop/cvat/apps/documentation/xml_format.md#xml-annotation-format">CVAT XML annotation version 1.1</a> format for images to <code>MultiLabelRecognitionAnnotation</code>. Used bbox attributes as annotation classes. Each attribute field should contains <code>T</code> or <code>F</code> values for attribute existence/non-existence on the image respectively.<ul>
<li><code>annotation_file</code> - path to xml file in appropriate format.</li>
<li><code>label</code> - the dataset label which will be used for attributes collection (e.g. if your dataset contains 2 labels: <code>face</code> and <code>person</code> and you want recognise attributes for face, you should use <code>face</code> as value for this parameter).</li>
</ul>
</li>
<li><code>cvat_person_detection_action_recognition</code> converts dataset with <a href="https://github.com/opencv/cvat/blob/develop/cvat/apps/documentation/xml_format.md#xml-annotation-format">CVAT XML annotation version 1.1</a> for person detection and action recognition task to <code>ContainerAnnotation</code> with <code>DetectionAnnotation</code> for person detection quality estimation named <code>person_annotation</code> and <code>ActionDetectionAnnotation</code> for action recognition named <code>action_annotation</code>.<ul>
<li><code>annotation_file</code> - path to xml file with ground truth.</li>
<li><code>use_case</code> - use case, which determines the dataset label map. Supported range actions:<ul>
<li><code>common_3_actions</code>(seating, standing, raising hand)</li>
<li><code>common_6_actions</code>(seating, writing, raising hand, standing, turned around, lie on the desk)</li>
<li><code>teacher</code> (standing, writing, demonstrating)</li>
<li><code>raising_hand</code> (seating, raising hand)</li>
</ul>
</li>
</ul>
</li>
<li><code>lpr_txt</code> - converts annotation for license plate recognition task in txt format to <code>CharacterRecognitionAnnotation</code>.<ul>
<li><code>annotation_file</code> - path to txt annotation.</li>
<li><code>decoding_dictionary</code> - path to file containing dictionary for output decoding. </li>
</ul>
</li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>