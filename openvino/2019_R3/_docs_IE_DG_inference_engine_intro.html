<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Introduction to Inference Engine - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Introduction to Inference Engine </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>After you have used the Model Optimizer to create an Intermediate Representation (IR), use the Inference Engine to infer input data.</p>
<p>The Inference Engine is a C++ library with a set of C++ classes to infer input data (images) and get a result. The C++ library provides an API to read the Intermediate Representation, set the input and output formats, and execute the model on devices.</p>
<p>To learn about how to use the Inference Engine API for your application, see the <a class="el" href="_docs_IE_DG_Integrate_with_customer_application_new_API.html">Integrating Inference Engine in Your Application</a> documentation.</p>
<p>Complete API Reference is in the full offline package documentation:</p><ol type="1">
<li>Go to <code>&lt;INSTALL_DIR&gt;/deployment_tools/documentation/</code>, where <code>&lt;INSTALL_DIR&gt;</code> is the OpenVINO toolkit installation directory.</li>
<li>Open <code>index.html</code> in an Internet browser.</li>
<li>Select <b>API References</b> from the menu at the top of the screen.</li>
<li>From the <b>API References</b> page, select <b>Inference Engine API References</b>.</li>
</ol>
<p>Inference Engine uses a plugin architecture. Inference Engine plugin is a software component that contains complete implementation for inference on a certain Intel&reg; hardware device: CPU, GPU, VPU, FPGA, etc. Each plugin implements the unified API and provides additional hardware-specific APIs.</p>
<h2>Modules in the Inference Engine component </h2>
<h3>Core Inference Engine Libraries</h3>
<p>Your application must link to the core Inference Engine library:</p><ul>
<li>Linux* OS: <code>libinference_engine.so</code></li>
<li>Windows* OS: <code>inference_engine.dll</code></li>
</ul>
<p>The required C++ header files are located in the <code>include</code> directory.</p>
<p>This library contains the classes to:</p><ul>
<li>Read the network (<a class="el" href="classInferenceEngine_1_1CNNNetReader.html" title="This is a wrapper class used to build and parse a network from the given IR. All the methods here can...">InferenceEngine::CNNNetReader</a>)</li>
<li>Manipulate network information (<a class="el" href="classInferenceEngine_1_1CNNNetwork.html" title="This class contains all the information about the Neural Network and the related binary information...">InferenceEngine::CNNNetwork</a>)</li>
<li>Create Inference Engine Core object to work with devices (<a class="el" href="classInferenceEngine_1_1Core.html" title="This class represents Inference Engine Core entity. It can throw exceptions safely for the applicatio...">InferenceEngine::Core</a>)</li>
<li>Execute and pass inputs and outputs (<a class="el" href="classInferenceEngine_1_1ExecutableNetwork.html" title="wrapper over IExecutableNetwork ">InferenceEngine::ExecutableNetwork</a> and <a class="el" href="classInferenceEngine_1_1InferRequest.html" title="This class is a wrapper of IInferRequest to provide setters/getters of input/output which operates wi...">InferenceEngine::InferRequest</a>)</li>
</ul>
<h3>Device-specific Plugin Libraries</h3>
<p>For each supported target device, Inference Engine provides a plugin — a DLL/shared library that contains complete implementation for inference on this particular device. The following plugins are available:</p>
<table class="doxtable">
<tr>
<th>Plugin </th><th>Device Type  </th></tr>
<tr>
<td>CPU</td><td>Intel® Xeon® with Intel® AVX2 and AVX512, Intel® Core™ Processors with Intel® AVX2, Intel® Atom® Processors with Intel® SSE </td></tr>
<tr>
<td>GPU</td><td>Intel® Processor Graphics, including Intel® HD Graphics and Intel® Iris® Graphics </td></tr>
<tr>
<td>FPGA</td><td>Intel® Programmable Acceleration Card with Intel® Arria® 10 GX FPGA, Intel® Vision Accelerator Design with an Intel® Arria 10 FPGA (Speed Grade 1), Intel® Vision Accelerator Design with an Intel® Arria 10 FPGA (Speed Grade 2) </td></tr>
<tr>
<td>MYRIAD</td><td>Intel® Movidius™ Neural Compute Stick powered by the Intel® Movidius™ Myriad™ 2, Intel® Neural Compute Stick 2 powered by the Intel® Movidius™ Myriad™ X </td></tr>
<tr>
<td>GNA</td><td>Intel® Speech Enabling Developer Kit, Amazon Alexa* Premium Far-Field Developer Kit, Intel® Pentium® Silver processor J5005, Intel® Celeron® processor J4005, Intel® Core™ i3-8121U processor </td></tr>
<tr>
<td>HETERO</td><td>Automatic splitting of a network inference between several devices (for example if a device doesn't support certain layers </td></tr>
<tr>
<td>MULTI</td><td>Simultaneous inference of the same network on several devices in parallel </td></tr>
</table>
<p>The table below shows the plugin libraries and dependencies for Linux and Windows platforms.</p>
<table class="doxtable">
<tr>
<th>Plugin </th><th>Library name for Linux </th><th>Dependency libraries for Linux </th><th>Library name for Windows </th><th>Dependency libraries for Windows  </th></tr>
<tr>
<td>CPU </td><td><code>libMKLDNNPlugin.so</code> </td><td><code>libmklml_tiny.so</code>, <code>libiomp5md.so</code> </td><td><code>MKLDNNPlugin.dll</code> </td><td><code>mklml_tiny.dll</code>, <code>libiomp5md.dll</code> </td></tr>
<tr>
<td>GPU </td><td><code>libclDNNPlugin.so</code> </td><td><code>libclDNN64.so</code> </td><td><code>clDNNPlugin.dll</code> </td><td><code>clDNN64.dll</code> </td></tr>
<tr>
<td>FPGA </td><td><code>libdliaPlugin.so</code> </td><td><code>libdla_compiler_core.so</code>, <code>libdla_runtime_core.so</code> </td><td><code>dliaPlugin.dll</code> </td><td><code>dla_compiler_core.dll</code>, <code>dla_runtime_core.dll</code> </td></tr>
<tr>
<td>MYRIAD </td><td><code>libmyriadPlugin.so</code> </td><td>No dependencies </td><td><code>myriadPlugin.dll</code> </td><td>No dependencies </td></tr>
<tr>
<td>HDDL </td><td><code>libHDDLPlugin.so</code> </td><td><code>libbsl.so</code>, <code>libhddlapi.so</code>, <code>libmvnc-hddl.so</code> </td><td><code>HDDLPlugin.dll</code> </td><td><code>bsl.dll</code>, <code>hddlapi.dll</code>, <code>json-c.dll</code>, <code>libcrypto-1_1-x64.dll</code>, <code>libssl-1_1-x64.dll</code>, <code>mvnc-hddl.dll</code> </td></tr>
<tr>
<td>GNA </td><td><code>libGNAPlugin.so</code> </td><td><code>libgna_api.so</code> </td><td><code>GNAPlugin.dll</code> </td><td><code>gna.dll</code> </td></tr>
<tr>
<td>HETERO </td><td><code>libHeteroPlugin.so</code> </td><td>Same as for selected plugins </td><td><code>HeteroPlugin.dll</code> </td><td>Same as for selected plugins </td></tr>
<tr>
<td>MULTI </td><td><code>libMultiDevicePlugin.so</code> </td><td>Same as for selected plugins </td><td><code>MultiDevicePlugin.dll</code> </td><td>Same as for selected plugins </td></tr>
</table>
<p>Make sure those libraries are in your computer's path or in the place you pointed to in the plugin loader. Make sure each plugin's related dependencies are in the:</p>
<ul>
<li>Linux: <code>LD_LIBRARY_PATH</code></li>
<li>Windows: <code>PATH</code></li>
</ul>
<p>On Linux, use the script <code>bin/setupvars.sh</code> to set the environment variables.</p>
<p>On Windows, run the <code>bin\setupvars.bat</code> batch file to set the environment variables.</p>
<p>To learn more about supported devices and corresponding plugins, see the <a class="el" href="_docs_IE_DG_supported_plugins_Supported_Devices.html">Supported Devices</a> chapter.</p>
<h2>Common Workflow for Using the Inference Engine API </h2>
<p>The common workflow contains the following steps:</p>
<ol type="1">
<li><b>Read the Intermediate Representation</b> - Using the <code><a class="el" href="classInferenceEngine_1_1CNNNetReader.html" title="This is a wrapper class used to build and parse a network from the given IR. All the methods here can...">InferenceEngine::CNNNetReader</a></code> class, read an Intermediate Representation file into an object of the <code><a class="el" href="classInferenceEngine_1_1CNNNetwork.html" title="This class contains all the information about the Neural Network and the related binary information...">InferenceEngine::CNNNetwork</a></code> class. This class represents the network in the host memory.</li>
<li><b>Prepare inputs and outputs format</b> - After loading the network, specify input and output precision and the layout on the network. For these specification, use the <code><a class="el" href="classInferenceEngine_1_1CNNNetwork.html#add0cc549f1bd88f0f14abe52c897eb54" title="Wraps original method ICNNNetwork::getInputsInfo. ">InferenceEngine::CNNNetwork::getInputsInfo()</a></code> and <code><a class="el" href="classInferenceEngine_1_1CNNNetwork.html#a3df61f333b129dbaebf96ae3cc18cd06" title="Wraps original method ICNNNetwork::getOutputsInfo. ">InferenceEngine::CNNNetwork::getOutputsInfo()</a></code>.</li>
<li><b>Create Inference Engine Core object</b> - Create an <code><a class="el" href="classInferenceEngine_1_1Core.html" title="This class represents Inference Engine Core entity. It can throw exceptions safely for the applicatio...">InferenceEngine::Core</a></code> object to work with different devices, all device plugins are managed internally by the <code>Core</code> object. Pass per device loading configurations specific to this device (<code><a class="el" href="classInferenceEngine_1_1Core.html#a268e2d24595061e9d804460cc6ca9ad3" title="Sets configuration for device, acceptable keys can be found in ie_plugin_config.hpp. ">InferenceEngine::Core::SetConfig</a></code>), and register extensions to this device (<code><a class="el" href="classInferenceEngine_1_1Core.html#aa1ddb53f4160bf0735239f4aa0c12320" title="Registers extension for the specified plugin. ">InferenceEngine::Core::AddExtension</a></code>).</li>
<li><b>Compile and Load Network to device</b> - Use the <code><a class="el" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff" title="Creates an executable network from a network object. Users can create as many networks as they need a...">InferenceEngine::Core::LoadNetwork()</a></code> method with specific device (e.g. <code>CPU</code>, <code>GPU</code>, etc.) to compile and load the network on the device. Pass in the per-target load configuration for this compilation and load operation.</li>
<li><b>Set input data</b> - With the network loaded, you have an <code><a class="el" href="classInferenceEngine_1_1ExecutableNetwork.html" title="wrapper over IExecutableNetwork ">InferenceEngine::ExecutableNetwork</a></code> object. Use this object to create an <code><a class="el" href="classInferenceEngine_1_1InferRequest.html" title="This class is a wrapper of IInferRequest to provide setters/getters of input/output which operates wi...">InferenceEngine::InferRequest</a></code> in which you signal the input buffers to use for input and output. Specify a device-allocated memory and copy it into the device memory directly, or tell the device to use your application memory to save a copy.</li>
<li><b>Execute</b> - With the input and output memory now defined, choose your execution mode:<ul>
<li>Synchronously - <code><a class="el" href="classInferenceEngine_1_1InferRequest.html#a3391ce30894abde730523e9ca9371ce8" title="Wraps original method IInferRequest::Infer. ">InferenceEngine::InferRequest::Infer()</a></code> method. Blocks until inference is completed.</li>
<li>Asynchronously - <code><a class="el" href="classInferenceEngine_1_1InferRequest.html#a405293e8423d82a5b45f642a3bef0d24" title="Start inference of specified input(s) in asynchronous mode. ">InferenceEngine::InferRequest::StartAsync()</a></code> method. Check status with the <code><a class="el" href="classInferenceEngine_1_1InferRequest.html#ab5887acbd127429f628b77f4970cc701" title="Wraps original method IInferRequest::Wait. ">InferenceEngine::InferRequest::Wait()</a></code> method (0 timeout), wait, or specify a completion callback.</li>
</ul>
</li>
<li><b>Get the output</b> - After inference is completed, get the output memory or read the memory you provided earlier. Do this with the <code><a class="el" href="classInferenceEngine_1_1IInferRequest.html#af885a8b36fa184ec403f2b7790a26f25" title="Gets input/output data for inference. ">InferenceEngine::IInferRequest::GetBlob()</a></code> method.</li>
</ol>
<h2>Further Reading </h2>
<p>For more details on the Inference Engine API, refer to the <a class="el" href="_docs_IE_DG_Integrate_with_customer_application_new_API.html">Integrating Inference Engine in Your Application</a> documentation. </p>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>