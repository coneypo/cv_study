<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Inference Engine API Changes History - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Inference Engine API Changes History </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The sections below contain detailed list of changes made to the Inference Engine API in recent releases.</p>
<h2>2019 R3</h2>
<h3>New API</h3>
<p><b>New supported layers:</b></p>
<ul>
<li><a class="el" href="classInferenceEngine_1_1SparseFillEmptyRowsLayer.html" title="This class represents SparseFillEmptyRows layer SparseFillEmptyRows fills empty rows in a sparse tens...">InferenceEngine::SparseFillEmptyRowsLayer</a> new class</li>
<li><a class="el" href="classInferenceEngine_1_1UniqueLayer.html" title="This class represents Unique layer. The Unique operation searches for unique elements in 1-D input...">InferenceEngine::UniqueLayer</a> new class</li>
<li><a class="el" href="classInferenceEngine_1_1NonMaxSuppressionLayer.html" title="This class represents a standard NonMaxSuppression layer. ">InferenceEngine::NonMaxSuppressionLayer</a> new class</li>
<li><a class="el" href="classInferenceEngine_1_1ScatterLayer.html" title="This class represents a standard Scatter layer. ">InferenceEngine::ScatterLayer</a> new class</li>
</ul>
<p><b>FPGA plugin streaming support:</b></p>
<ul>
<li><a class="el" href="dlia__config_8hpp.html#a984ffb2c10cb65116bbfea0f672b7e7f" title="Shortcut for defining FPGA metric values. ">DLIA_METRIC_VALUE(INPUT_STREAMING)</a> value to <a class="el" href="ie__plugin__config_8hpp.html#a69d0efa20c5b2bec020a706279f0c7be" title="shortcut for defining common Inference Engine metrics ">METRIC_KEY(OPTIMIZATION_CAPABILITIES)</a></li>
<li><a class="el" href="dlia__config_8hpp.html#a5011654d99f7c0f6dbfbd3a298f7f698" title="Shortcut for defining FPGA configuration keys. ">DLIA_CONFIG_KEY(ENABLE_STREAMING)</a> config key</li>
</ul>
<h3>Removed API</h3>
<ul>
<li>InferenceEngine::EltwiseLayer::Select from <a class="el" href="classInferenceEngine_1_1EltwiseLayer.html#a4ebd064f9d0098f23db0f018f960a3e2" title="Defines possible operations that can be used. ">InferenceEngine::EltwiseLayer::eOperation</a> enumeration</li>
</ul>
<h2>2019 R2</h2>
<h3>New API</h3>
<p><b>Inference Engine Core API:</b></p>
<ul>
<li>Introduced <a class="el" href="classInferenceEngine_1_1Core.html" title="This class represents Inference Engine Core entity. It can throw exceptions safely for the applicatio...">InferenceEngine::Core</a> high level class to manage devices</li>
</ul>
<p><b>Query API extensions to <a class="el" href="classInferenceEngine_1_1ExecutableNetwork.html" title="wrapper over IExecutableNetwork ">InferenceEngine::ExecutableNetwork</a> and <a class="el" href="classInferenceEngine_1_1IExecutableNetwork.html" title="This is an interface of an executable network. ">InferenceEngine::IExecutableNetwork</a>:</b></p>
<ul>
<li><a class="el" href="classInferenceEngine_1_1ExecutableNetwork.html#aa1ed6418b25be96a413c452ca8c1480a">InferenceEngine::ExecutableNetwork::SetConfig</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1ExecutableNetwork.html#af43953e9d84965914d1ce50f90480145" title="Gets configuration for current executable network. ">InferenceEngine::ExecutableNetwork::GetConfig</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1ExecutableNetwork.html#a5b38590cad3a68144c679af5f5a6090d">InferenceEngine::ExecutableNetwork::GetMetric</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1IExecutableNetwork.html#ab0960afa5470727239d334fa1de5d04e" title="Sets configuration for current executable network. ">InferenceEngine::IExecutableNetwork::SetConfig</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1IExecutableNetwork.html#ad0c289951019aef4a7950d58517c2c13" title="Gets configuration for current executable network. ">InferenceEngine::IExecutableNetwork::GetConfig</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1IExecutableNetwork.html#a6ea9d412e4a520c82a4c92edf113f485" title="Gets general runtime metric for an executable network. ">InferenceEngine::IExecutableNetwork::GetMetric</a> method</li>
</ul>
<p><b>Metrics and values for Query API:</b></p>
<ul>
<li><a class="el" href="ie__plugin__config_8hpp.html#a69d0efa20c5b2bec020a706279f0c7be" title="shortcut for defining common Inference Engine metrics ">METRIC_KEY(AVAILABLE_DEVICES)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#a69d0efa20c5b2bec020a706279f0c7be" title="shortcut for defining common Inference Engine metrics ">METRIC_KEY(SUPPORTED_METRICS)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#a69d0efa20c5b2bec020a706279f0c7be" title="shortcut for defining common Inference Engine metrics ">METRIC_KEY(SUPPORTED_CONFIG_KEYS)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#a69d0efa20c5b2bec020a706279f0c7be" title="shortcut for defining common Inference Engine metrics ">METRIC_KEY(FULL_DEVICE_NAME)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#a69d0efa20c5b2bec020a706279f0c7be" title="shortcut for defining common Inference Engine metrics ">METRIC_KEY(OPTIMIZATION_CAPABILITIES)</a><ul>
<li><a class="el" href="ie__plugin__config_8hpp.html#ad6dd157c1a4d27888bfdcdf1b64cfdb2" title="shortcut for defining metric values ">METRIC_VALUE(FP32)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#ad6dd157c1a4d27888bfdcdf1b64cfdb2" title="shortcut for defining metric values ">METRIC_VALUE(FP16)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#ad6dd157c1a4d27888bfdcdf1b64cfdb2" title="shortcut for defining metric values ">METRIC_VALUE(INT8)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#ad6dd157c1a4d27888bfdcdf1b64cfdb2" title="shortcut for defining metric values ">METRIC_VALUE(BIN)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#ad6dd157c1a4d27888bfdcdf1b64cfdb2" title="shortcut for defining metric values ">METRIC_VALUE(WINOGRAD)</a></li>
<li><a class="el" href="dlia__config_8hpp.html#a984ffb2c10cb65116bbfea0f672b7e7f" title="Shortcut for defining FPGA metric values. ">DLIA_METRIC_VALUE(FP11)</a></li>
</ul>
</li>
<li><a class="el" href="ie__plugin__config_8hpp.html#a69d0efa20c5b2bec020a706279f0c7be" title="shortcut for defining common Inference Engine metrics ">METRIC_KEY(RANGE_FOR_STREAMS)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#a69d0efa20c5b2bec020a706279f0c7be" title="shortcut for defining common Inference Engine metrics ">METRIC_KEY(NUMBER_OF_WAITING_INFER_REQUESTS)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#a69d0efa20c5b2bec020a706279f0c7be" title="shortcut for defining common Inference Engine metrics ">METRIC_KEY(NUMBER_OF_EXEC_INFER_REQUESTS)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#a69d0efa20c5b2bec020a706279f0c7be" title="shortcut for defining common Inference Engine metrics ">METRIC_KEY(DEVICE_THERMAL)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#a69d0efa20c5b2bec020a706279f0c7be" title="shortcut for defining common Inference Engine metrics ">METRIC_KEY(RANGE_FOR_ASYNC_INFER_REQUESTS)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#adb48efa632ae9bacfa86b8a3a0d9541e" title="shortcut for defining common Inference Engine ExecutableNetwork metrics ">EXEC_NETWORK_METRIC_KEY(NETWORK_NAME)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#adb48efa632ae9bacfa86b8a3a0d9541e" title="shortcut for defining common Inference Engine ExecutableNetwork metrics ">EXEC_NETWORK_METRIC_KEY(OPTIMAL_NUMBER_OF_INFER_REQUESTS)</a></li>
</ul>
<p><b>Common API:</b></p>
<ul>
<li>CLDNN_CONFIG_KEY(INT8_ENABLED) config key<ul>
<li><a class="el" href="ie__plugin__config_8hpp.html#aad09cfba062e8ec9fb7ab9383f656ec7" title="shortcut for defining configuration keys ">CONFIG_KEY(GPU_THROUGHPUT_AUTO)</a></li>
<li><a class="el" href="ie__plugin__config_8hpp.html#aad09cfba062e8ec9fb7ab9383f656ec7" title="shortcut for defining configuration keys ">CONFIG_KEY(GPU_THROUGHPUT_STREAMS)</a></li>
</ul>
</li>
<li><a class="el" href="dlia__config_8hpp.html#a5011654d99f7c0f6dbfbd3a298f7f698" title="Shortcut for defining FPGA configuration keys. ">DLIA_CONFIG_KEY(IO_TRANSFORMATIONS_NATIVE)</a> config key</li>
<li><a class="el" href="dlia__config_8hpp.html#a5011654d99f7c0f6dbfbd3a298f7f698" title="Shortcut for defining FPGA configuration keys. ">DLIA_CONFIG_KEY(DUMP_SUPPORTED_LAYERS_INFORMATION)</a> config key</li>
<li><a class="el" href="gna__config_8hpp.html#a208a60b02fdd02d379486a1625bf5ebc" title="Shortcut for defining configuration values. ">GNA_CONFIG_VALUE(SW_FP32)</a> config value for <a class="el" href="gna__config_8hpp.html#a8124b15c4055e25f8c5792f2afe1e822" title="Shortcut for defining configuration keys. ">GNA_CONFIG_KEY(DEVICE_MODE)</a> key</li>
<li><a class="el" href="multi__device__config_8hpp.html#aa887cd604b772a3a51ba73f9652ae6c4" title="A macro which provides a MULTI-mangled name for configuration key with name name ">MULTI_CONFIG_KEY(DEVICE_PRIORITIES)</a> config key for <code>MULTI</code> device</li>
<li>InferenceEngine::CNNNetReader::ReadNetwork(const std::wstring &amp;filepath) new method</li>
<li>InferenceEngine::CNNNetReader::ReadWeights(const std::wstring &amp;filepath) new method</li>
<li><a class="el" href="classInferenceEngine_1_1ExecutableNetwork.html#af3ca64e37ee06de7f9253ce8828ddfec" title="Constructs ExecutableNetwork from the initialized shared_pointer. ">InferenceEngine::ExecutableNetwork::ExecutableNetwork(IExecutableNetwork::Ptr actual, InferenceEnginePluginPtr plg)</a> constructor with additional <code>plg</code> parameter</li>
<li><a class="el" href="classInferenceEngine_1_1InferRequest.html#a236f5a941f5e155f61d48134a00082a0">InferenceEngine::InferRequest::InferRequest(IInferRequest::Ptr request, InferenceEnginePluginPtr plg)</a> constructor with additional <code>plg</code> parameter</li>
<li><a class="el" href="classInferenceEngine_1_1Data.html#a2e3373a63cbb27493d18b9561728671c" title="Sets a name the Data object. ">InferenceEngine::Data::setName</a> method</li>
<li><a class="el" href="structInferenceEngine_1_1QueryNetworkResult.html#aff431e5d7451f364dee1c1c54ca78333" title="A map of supported layers: ">InferenceEngine::QueryNetworkResult::supportedLayersMap</a></li>
<li><a class="el" href="classInferenceEngine_1_1Precision.html#ade75bd7073b4aa966c0dda4025bcd0f5a76288d38cc52b68fb2127a4eb84b3b80">InferenceEngine::Precision::I64</a> extension to <a class="el" href="classInferenceEngine_1_1Precision.html#ade75bd7073b4aa966c0dda4025bcd0f5">InferenceEngine::Precision::ePrecision</a> enumeration</li>
</ul>
<p><b>New supported primitives:</b></p>
<ul>
<li><a class="el" href="classInferenceEngine_1_1Builder_1_1DeformableConvolutionLayer.html" title="The class represents a builder for Deconvolution layer. ">InferenceEngine::Builder::DeformableConvolutionLayer</a> new class</li>
<li><a class="el" href="classInferenceEngine_1_1DeformableConvolutionLayer.html" title="This class represents a standard deformable convolution layer. ">InferenceEngine::DeformableConvolutionLayer</a> new class</li>
<li>InferenceEngine::EltwiseLayer::Logical_NOT, InferenceEngine::EltwiseLayer::Mean, InferenceEngine::EltwiseLayer::Select extensions to <a class="el" href="classInferenceEngine_1_1EltwiseLayer.html#a4ebd064f9d0098f23db0f018f960a3e2" title="Defines possible operations that can be used. ">InferenceEngine::EltwiseLayer::eOperation</a> enumeration</li>
<li><a class="el" href="classInferenceEngine_1_1OneHotLayer.html" title="This class represents a OneHot layer Converts input into OneHot representation. ">InferenceEngine::OneHotLayer</a> new class</li>
<li><a class="el" href="classInferenceEngine_1_1SelectLayer.html" title="This class represents a SelectLayer layer SelectLayer layer takes elements from the second (“then”)...">InferenceEngine::SelectLayer</a> new class</li>
<li><a class="el" href="classInferenceEngine_1_1BroadcastLayer.html" title="This class represents a standard Broadcast layer Broadcast modifies input tensor dimensions according...">InferenceEngine::BroadcastLayer</a> new class</li>
<li><a class="el" href="classInferenceEngine_1_1MathLayer.html" title="This class represents a standard Math layers Math modifies input tensor dimensions according paramete...">InferenceEngine::MathLayer</a> new class</li>
<li><a class="el" href="classInferenceEngine_1_1ReduceLayer.html" title="This class represents a standard Reduce layers Reduce modifies input tensor according parameters...">InferenceEngine::ReduceLayer</a> new class</li>
<li><a class="el" href="classInferenceEngine_1_1TopKLayer.html" title="This class represents a standard TopK layer TopK picks top K values from input tensor according param...">InferenceEngine::TopKLayer</a> new class</li>
</ul>
<p><b>Extensions to Blob creation API:</b></p>
<ul>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#aa07152ba7c9910abab46a7e8e58323bc" title="Checks if the Blob object can be cast to the type T*. ">InferenceEngine::Blob::is</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#aa07152ba7c9910abab46a7e8e58323bc" title="Checks if the Blob object can be cast to the type T*. ">InferenceEngine::Blob::is</a> const method</li>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#abc7e63536f5f3811ba2b01455ad06954" title="Casts this Blob object to the type T*. Use InferenceEngine::as() to operate with shared Blob objects ...">InferenceEngine::Blob::as</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#abc7e63536f5f3811ba2b01455ad06954" title="Casts this Blob object to the type T*. Use InferenceEngine::as() to operate with shared Blob objects ...">InferenceEngine::Blob::as</a> const method</li>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#abe84a6133053bfc7c6114a17c1bc114e" title="Gets an allocator for allocator-based blobs. ">InferenceEngine::Blob::getAllocator</a> abstract method</li>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#a1a8e46ea2dc37ca1fb0414c1e6ffe066" title="Gets a handle to allocated memory. ">InferenceEngine::Blob::getHandle</a> abstract method</li>
<li><a class="el" href="classInferenceEngine_1_1MemoryBlob.html" title="This class implements a container object that represents a tensor in memory (host and remote/accelera...">InferenceEngine::MemoryBlob</a> class</li>
<li><a class="el" href="namespaceInferenceEngine.html#a5ee5ca7708cc67a9a0becc2593d0558a" title="Extra information about input color format for preprocessing. ">InferenceEngine::ColorFormat</a> enumeration</li>
<li><a class="el" href="classInferenceEngine_1_1PreProcessInfo.html#a3a10ba0d562a2268fe584d4d2db94cac" title="Changes the color format of the input data provided by the user This function should be called before...">InferenceEngine::PreProcessInfo::setColorFormat</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1PreProcessInfo.html#a259085aba02d149edac544323fdf33f2" title="Gets a color format associated with the input. ">InferenceEngine::PreProcessInfo::getColorFormat</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1CompoundBlob.html" title="This class represents a blob that contains other blobs. ">InferenceEngine::CompoundBlob</a> class to work with blobs consisting of several planes</li>
<li><a class="el" href="classInferenceEngine_1_1NV12Blob.html" title="Represents a blob that contains two planes (Y and UV) in NV12 color format. ">InferenceEngine::NV12Blob</a> class representing NV12 blob with two planes</li>
</ul>
<h3>Deprecated API</h3>
<p>The methods listed below are deprecated and will be removed in 2019 R4 release:</p>
<p><b>Common API:</b></p>
<ul>
<li><a class="el" href="classInferenceEngine_1_1InputInfo.html#a21eddb418455fc69b440aeaf73ed8770" title="Gets a precision of the input data provided by user. ">InferenceEngine::InputInfo::getInputPrecision</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1InputInfo.html#a3fe7b3d2c6b80b65d0cfa25d63974957" title="Changes the precision of the input data provided by the user. This function should be called before l...">InferenceEngine::InputInfo::setInputPrecision</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1InputInfo.html#a2b9ec898d5b5b1044dd8deb9153c58de" title="Gets dimensions/shape of the input data with reversed order. ">InferenceEngine::InputInfo::getDims</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1CNNLayer.html#a0f6e1da6858f707ee56024232db0397f">InferenceEngine::CNNLayer::GetParamsAsBool</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1CNNNetwork.html#a2a50347b2518f6319c17de23ff11de46" title="Initialises helper class from externally managed pointer. ">InferenceEngine::CNNNetwork::CNNNetwork(ICNNNetwork* actual)</a> constructor</li>
<li><a class="el" href="classInferenceEngine_1_1CNNNetwork.html#a80fbfb862936435bab7f8acac12cb4a2" title="Sets tha target device. ">InferenceEngine::CNNNetwork::setTargetDevice</a> method</li>
<li><a class="el" href="hetero__plugin__config_8hpp.html#aa455ce33b7e7a245be639015625a9768" title="Shortcut for defining HETERO configuration keys. ">HETERO_CONFIG_KEY(DUMP_DLA_MESSAGES)</a> config key</li>
<li><a class="el" href="classInferenceEngine_1_1ILayerImplFactory.html#a8e06d653a84f05bf252cb4d8fa3c8222" title="Sets output shapes by input shapes. ">InferenceEngine::ILayerImplFactory::getShapes</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1IShapeInferImpl.html#a3a60a8308d33864a1f221eebc4410b62" title="check that reshape can be applied, that parameters and shapes are valid ">InferenceEngine::IShapeInferImpl::inferShapes(const std::vector&lt;SizeVector&gt;&amp;, const std::map&lt;std::string, std::string&gt;&amp; , const std::map&lt;std::string, Blob::Ptr&gt;&amp;, std::vector&lt;SizeVector&gt;&amp;, ResponseDesc*)</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1Data.html#a869fde45d50969fbce8cf009468b119c" title="Sets the batch value in the data dimensions. Batch is defined as the last element in the dimensions v...">InferenceEngine::Data::setBatchSize</a> method</li>
<li><a class="el" href="structInferenceEngine_1_1QueryNetworkResult.html#a81e1efbc2b538ebb049bc62aedb18fa6" title="Set of supported layers by specific device. ">InferenceEngine::QueryNetworkResult::supportedLayers</a> field</li>
<li><a class="el" href="classInferenceEngine_1_1ICNNNetwork.html#abe649d332d99d5c6abda004cfe659ad1" title="Changes the inference batch size. ">InferenceEngine::ICNNNetwork::setBatchSize(const size_t size)</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#a93a3c1e844bf4d757fea920ae309526e" title="Changes Tensor size to the specified dimensions. If it was allocated, the previous data is deallocate...">InferenceEngine::Blob::Resize</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#a27f75ca59de5f9fd5dd6c4f7e455e4a9" title="Changes tensor size to the specified dimensions without changing memory. The total size remains uncha...">InferenceEngine::Blob::Reshape</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1TBlob.html#a9fbc7cc30941f4702c9eaddec10bbb67" title="Copies data from the given vector to the blob. ">InferenceEngine::TBlob::set</a> method</li>
</ul>
<p><b><a class="el" href="classInferenceEngine_1_1IInferencePlugin.html" title="This class is a main plugin interface. ">InferenceEngine::IInferencePlugin</a> and <a class="el" href="namespaceInferenceEngine.html" title="Inference Engine API. ">InferenceEngine</a>:InferencePlugin obsolete methods:</b></p>
<ul>
<li><a class="el" href="classInferenceEngine_1_1InferencePlugin.html#a0ca00d832aa35ecdefdfb456b62e51d4" title="Wraps original method IInferencePlugin::LoadNetwork(ICNNNetwork &amp;, ResponseDesc *) ...">InferenceEngine::InferencePlugin::LoadNetwork(ICNNNetwork &amp;network)</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1InferencePlugin.html#a70d4e46ade4bc580c7c69e7eeb92274d" title="Wraps original method IInferencePlugin::Infer(const BlobMap&amp;, BlobMap&amp;, ResponseDesc *) ...">InferenceEngine::InferencePlugin::Infer</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1InferencePlugin.html#a0b709d4cb7c70474191cf98a7f82a0b4" title="Wraps original method IInferencePlugin::GetPerformanceCounts. ">InferenceEngine::InferencePlugin::GetPerformanceCounts</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1InferencePlugin.html#ae830df73582b71b65443e9fb6143323d" title="Wraps original method IInferencePlugin::QueryNetwork(const ICNNNetwork&amp;, QueryNetworkResult&amp; ) const...">InferenceEngine::InferencePlugin::QueryNetwork(const ICNNNetwork &amp;network, QueryNetworkResult &amp;res) const </a>method</li>
<li><a class="el" href="classInferenceEngine_1_1IInferencePlugin.html#a1f750b4c49d009ad7b59bbcf1b63a489" title="Loads a pre-built network with weights to the engine. In case of success the plugin will be ready to ...">InferenceEngine::IInferencePlugin::LoadNetwork(ICNNNetwork &amp;network, ResponseDesc *resp)</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1IInferencePlugin.html#a9d2f325d953d4f7fea65ae9f3188022b" title="Infers an image(s). Input and output dimensions depend on the topology. As an example for classificat...">InferenceEngine::IInferencePlugin::Infer(const Blob &amp;input, Blob &amp;result, ResponseDesc *resp)</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1IInferencePlugin.html#a250c56d6fdea13fbeab3e11e185f014f" title="Infers tensors. Input and output dimensions depend on the topology. As an example for classification ...">InferenceEngine::IInferencePlugin::Infer(const BlobMap &amp;input, BlobMap &amp;result, ResponseDesc *resp)</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1IInferencePlugin.html#ae49088f17b7a3c48342ded0590f447d2" title="Queries performance measures per layer to get feedback of what is the most time consuming layer Note:...">InferenceEngine::IInferencePlugin::GetPerformanceCounts</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1IInferencePlugin.html#a72f596a132004abfa98c8fabc1b0fc9f" title="Query plugin if it supports specified network. ">InferenceEngine::IInferencePlugin::QueryNetwork(const ICNNNetwork&amp; network, QueryNetworkResult&amp; res) const </a>method</li>
</ul>
<p><b>Fields in <a class="el" href="classInferenceEngine_1_1Data.html" title="This class represents the main Data representation node. ">InferenceEngine::Data</a> class are replaced with appropriate methods:</b></p>
<ul>
<li><a class="el" href="classInferenceEngine_1_1Data.html#a79c4e40319d6b67d86cfdfac968eccac" title="A precision type of this Data instance. ">InferenceEngine::Data::precision</a> field</li>
<li><a class="el" href="classInferenceEngine_1_1Data.html#aea3b125b5618450e64fa509caa90a08d" title="A data layout of this Data instance. ">InferenceEngine::Data::layout</a> field</li>
<li><a class="el" href="classInferenceEngine_1_1Data.html#a22beb6fec19c44d4747759dbb886e507" title="A tensor dimension array (the order is opposite to the order in the IR: w,h,c,n) of this Data instanc...">InferenceEngine::Data::dims</a> field</li>
<li><a class="el" href="classInferenceEngine_1_1Data.html#a22b9e7f34d940304ebb76eddbb878891" title="A pointer to the layer that creates this data element, null for input data elements. ">InferenceEngine::Data::creatorLayer</a> field</li>
<li><a class="el" href="classInferenceEngine_1_1Data.html#a1db203f40de7e9cb7e50281e7dd4bce2" title="A unique name that identifies this data node. ">InferenceEngine::Data::name</a> field</li>
<li><a class="el" href="classInferenceEngine_1_1Data.html#a10098f1512e9fdec72e35ec6ada6f9c7" title="A map of layers that use this node as input. It is useful for recursive NN graph traversal. ">InferenceEngine::Data::inputTo</a> field</li>
<li><a class="el" href="classInferenceEngine_1_1Data.html#a05ccc8b4379ab65b85482bc3b2270513" title="A user utility place holder. ">InferenceEngine::Data::userObject</a> field</li>
</ul>
<p><b>Heterogeneous plugin:</b></p>
<ul>
<li><a class="el" href="classInferenceEngine_1_1IHeteroDeviceLoader.html" title="This interface describes a mechanism of custom loaders to be used in heterogeneous plugin during sett...">InferenceEngine::IHeteroDeviceLoader</a> class</li>
<li><a class="el" href="classInferenceEngine_1_1IHeteroInferencePlugin.html" title="This interface extends regular plugin interface for heterogeneous case. Not all plugins implements it...">InferenceEngine::IHeteroInferencePlugin</a> class</li>
<li><a class="el" href="namespaceInferenceEngine.html#ad1e0584f3d04eacd2c16b4ba19ca3173" title="A C++ helper to work with objects created by the plugin. Implements different interfaces. ">InferenceEngine::HeteroPluginPtr</a> class</li>
<li>operator InferenceEngine::InferencePlugin::HeteroPluginPtr operator</li>
</ul>
<p><b>Blob creation API with dimensions in reverse order:</b></p>
<ul>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#a748b0f06cb8766ae1ef1d9c1647b6048" title="Constructor. Creates an empty Blob object with the specified precision. ">InferenceEngine::Blob::Blob(Precision p)</a> constructor</li>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#a5b10c2d757486e3f9b3e7528fa2e8b98" title="The constructor creates an empty Blob object with the specified precision and layout. ">InferenceEngine::Blob::Blob(Precision p, Layout l)</a> constructor</li>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#a65d4f220cce320c1474af8d2b703944d" title="The constructor creates an empty Blob object with the specified precision and dimensions. ">InferenceEngine::Blob::Blob(Precision p, const SizeVector &amp;dims)</a> constructor</li>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#a516e9df025258899e67f7b627bf896d7" title="The constructor creates an empty Blob object with the specified precision, layout and dimensions...">InferenceEngine::Blob::Blob(Precision p, Layout l, const SizeVector &amp;dims)</a> constructor</li>
<li><a class="el" href="classInferenceEngine_1_1TBlob.html#a02059b45d25c5c8b7129d00ca3e4ebe0" title="Creates a TBlob object with the specified precision and type, but does not allocate the memory...">InferenceEngine::TBlob::TBlob(Precision p, Layout l)</a> constructor</li>
<li><a class="el" href="classInferenceEngine_1_1TBlob.html#ab06700639b9ccf99c677eb42652f87ea" title="Creates a TBlob object with the specified dimensions but does not allocate the memory. Use the allocate() method to allocate memory. ">InferenceEngine::TBlob::TBlob(Precision p, Layout l, const SizeVector&amp; dims)</a> constructor</li>
<li><a class="el" href="classInferenceEngine_1_1TBlob.html#a2f31cd0b74eeff909f2dcafcc7b32ee4" title="The constructor creates a TBlob object with the specified dimensions on the pre-allocated memory...">InferenceEngine::TBlob::TBlob(Precision p, Layout l, const SizeVector&amp; dims, T* ptr, size_t data_size)</a> constructor</li>
<li><a class="el" href="classInferenceEngine_1_1TBlob.html#aab97cf11457b816513359609889226cc" title="Constructor. Creates a TBlob object with the specified precision, layout, dimensions and custom memor...">InferenceEngine::TBlob::TBlob(Precision p, Layout l, const SizeVector &amp;dims, std::shared_ptr&lt;IAllocator&gt; alloc)</a> constructor</li>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#adc2f6fc7702f436c0ea0e481302b0dc6" title="Returns the tensor precision of the current Blob object. ">InferenceEngine::Blob::type()</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#a1025036e79df819deb667f7e89efc244" title="Returns the tensor precision of the current Blob object. ">InferenceEngine::Blob::precision()</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#ae48f4e76179557852bade76862f26134" title="Returns the tensor layout of the current Blob object. ">InferenceEngine::Blob::layout()</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1Blob.html#a42424b05bbe1a95160fe531397c0edce" title="Returns the tensor dimensions vector with reversed order. ">InferenceEngine::Blob::dims()</a> method</li>
<li><a class="el" href="namespaceInferenceEngine.html#ad9bb08c6ea48c086cec356d10151d44e" title="Creates a blob with given precision and dimensions. ">InferenceEngine::make_shared_blob(Precision p, Layout l, const SizeVector &amp;dims)</a> function</li>
<li><a class="el" href="namespaceInferenceEngine.html#a6006dce98d23aaba065fbb4ee61e88a5" title="Creates a blob with the NCHW layout, given precision, and given dimensions. ">InferenceEngine::make_shared_blob(Precision p, const SizeVector &amp;dims)</a> function</li>
<li><a class="el" href="namespaceInferenceEngine.html#a0a6f953bacd234e4d73b764d8193d799" title="Creates a blob with the given precision. ">InferenceEngine::make_shared_blob(Precision p, Layout l, const TArg &amp;arg)</a> function</li>
<li><a class="el" href="namespaceInferenceEngine.html#a9e304dc78c6eb323278abd6e5acf35e0" title="Creates a blob with the NCHW layout and given tensor precision. ">InferenceEngine::make_shared_blob(Precision p, const TArg &amp;arg)</a> function</li>
<li><a class="el" href="namespaceInferenceEngine.html#ad04b877a25d3ac3493465cbe68a2abde" title="Gets a shared pointer for the new TBlob instance. The created instance is based on move semantics fro...">InferenceEngine::make_shared_blob(TBlob&lt;TypeTo&gt; &amp;&amp;arg)</a> function</li>
<li><a class="el" href="namespaceInferenceEngine.html#a4bcf841606cfca0b3952d7f4ba4eb4fc" title="Creates a blob with the given precision. ">InferenceEngine::make_shared_blob(Precision p, Layout l)</a> function</li>
<li><a class="el" href="namespaceInferenceEngine.html#ab36fc1a3db9d786437947b8c01c3ee51" title="Creates a blob with the given precision, layout and dimensions from the vector of values...">InferenceEngine::make_shared_blob(Precision p, Layout l, SizeVector dims, const std::vector&lt;TypeTo&gt; &amp;arg)</a> function</li>
<li><a class="el" href="namespaceInferenceEngine.html#afb63fa8ff7bd64fa395a3a019d3e41eb" title="Creates a blob with the given precision from the vector of values. ">InferenceEngine::make_shared_blob(Precision p, Layout l, const std::vector&lt;TypeTo&gt; &amp;arg)</a> function</li>
<li><a class="el" href="namespaceInferenceEngine.html#a60d1508cad5c45bb06374858271e443e" title="Creates a blob with the NCHW layout and the given precision from the vector of values. ">InferenceEngine::make_shared_blob(Precision p, const std::vector&lt;TypeTo&gt; &amp;arg)</a> function</li>
<li><a class="el" href="namespaceInferenceEngine.html#a09c7d81ebd136670bd0dd3327dd3d6c4" title="Creates a blob with the given precision from the pointer to the pre-allocated memory. ">InferenceEngine::make_shared_blob(Precision p, Layout l, const SizeVector &amp;dims, TypeTo * ptr, size_t size)</a> function</li>
<li><a class="el" href="namespaceInferenceEngine.html#a073cf5a871f359f365e2a5c4d10fd0cf" title="Creates a blob with the NCHW layout and the given precision from the pointer to the pre-allocated mem...">InferenceEngine::make_shared_blob(Precision p, const SizeVector &amp;dims, TypeTo * ptr, size_t size)</a> function</li>
<li><a class="el" href="namespaceInferenceEngine.html#a5dee71b1ace705d01a7f870ca3e3e0bd">InferenceEngine::I_N</a> variable</li>
<li><a class="el" href="namespaceInferenceEngine.html#a8094f72bf759c368feda7919ce8ef1d6">InferenceEngine::I_C</a> variable</li>
<li><a class="el" href="namespaceInferenceEngine.html#a339223ecfcce8b43223fd8c4afff3b57">InferenceEngine::I_H</a> variable</li>
<li><a class="el" href="namespaceInferenceEngine.html#ab49e5f757631ca69371b2056613aad1c">InferenceEngine::I_W</a> variable</li>
<li><a class="el" href="classInferenceEngine_1_1LayoutOffsetCounter.html" title="This class helps calculating offset in different layouts. ">InferenceEngine::LayoutOffsetCounter</a> class</li>
<li><a class="el" href="namespaceInferenceEngine.html#af506a3239abdef996709ad0f65d372e1">InferenceEngine::ConvertLayout</a> function</li>
</ul>
<p><b>API working with device enumeration:</b></p>
<ul>
<li><a class="el" href="namespaceInferenceEngine.html#ad053545315ac52b96eaac05ad8def796" title="Describes known device types. ">InferenceEngine::TargetDevice</a> enumeration</li>
<li><a class="el" href="classInferenceEngine_1_1TargetDeviceInfo.html" title="Describes the relationship between the enumerator type and the actual device&#39;s name. ">InferenceEngine::TargetDeviceInfo</a> class</li>
<li><a class="el" href="namespaceInferenceEngine.html#a0f8aa7b7692492425f94a0c5d2b05592" title="Returns the device name. ">InferenceEngine::getDeviceName</a> function</li>
<li><a class="el" href="structInferenceEngine_1_1FindPluginRequest.html" title="Defines a message that contains the InferenceEngine::TargetDevice object to find a plugin for...">InferenceEngine::FindPluginRequest</a> class</li>
<li><a class="el" href="structInferenceEngine_1_1FindPluginResponse.html" title="Defines a message that contains a list of appropriate plugin names. ">InferenceEngine::FindPluginResponse</a> class</li>
<li><a class="el" href="namespaceInferenceEngine.html#a4ddd9d5d6a53cefe1d57ddb291b929d5" title="Finds an appropriate plugin for requested target device. ">InferenceEngine::findPlugin(const FindPluginRequest &amp;req, FindPluginResponse &amp;result, ResponseDesc *resp)</a> function</li>
<li><a class="el" href="classInferenceEngine_1_1ICNNNetwork.html#ac1896dd3645734522b8f2b68a40d3f8e" title="Sets a desirable device to perform all work on. Some plug-ins might not support some target devices a...">InferenceEngine::ICNNNetwork::setTargetDevice</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1ICNNNetwork.html#a0ac8ac85442ca15673a9e7865a578364" title="Gets the target device. If setTargetDevice() was not called before, returns eDefault. ">InferenceEngine::ICNNNetwork::getTargetDevice</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1PluginDispatcher.html#a60c86bfa7ed4fbcf65372f37d49e69d5" title="Loads a plugin from directories that is suitable for the device string. ">InferenceEngine::PluginDispatcher::getPluginByDevice</a> method</li>
<li><a class="el" href="classInferenceEngine_1_1PluginDispatcher.html#ae6ab327fcd6e8141fef1591a2486e2ca" title="Loads a plugin from directories that is suitable for the device. ">InferenceEngine::PluginDispatcher::getSuitablePlugin</a> method </li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>