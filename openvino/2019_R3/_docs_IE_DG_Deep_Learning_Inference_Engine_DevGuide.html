<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Inference Engine Developer Guide - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Inference Engine Developer Guide </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Introduction to the OpenVINO™ Toolkit</h2>
<p>The OpenVINO™ toolkit is a comprehensive toolkit that you can use to develop and deploy vision-oriented solutions on Intel® platforms. Vision-oriented means the solutions use images or videos to perform specific tasks. A few of the solutions use cases include autonomous navigation, digital surveillance cameras, robotics, and mixed-reality headsets.</p>
<p>The OpenVINO™ toolkit:</p>
<ul>
<li>Enables CNN-based deep learning inference on the edge</li>
<li>Supports heterogeneous execution across an Intel&reg; CPU, Intel&reg; Integrated Graphics, Intel&reg; Movidius&trade; Neural Compute Stick and Intel&reg; Neural Compute Stick 2</li>
<li>Speeds time-to-market via an easy-to-use library of computer vision functions and pre-optimized kernels</li>
<li>Includes optimized calls for computer vision standards including OpenCV*, OpenCL&trade;, and OpenVX*</li>
</ul>
<p>The OpenVINO™ toolkit includes the following components:</p>
<ul>
<li>Intel® Deep Learning Deployment Toolkit (Intel® DLDT)<ul>
<li><a class="el" href="_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html">Deep Learning Model Optimizer</a> — A cross-platform command-line tool for importing models and preparing them for optimal execution with the Deep Learning Inference Engine. The Model Optimizer supports converting Caffe*, TensorFlow*, MXNet*, Kaldi*, ONNX* models.</li>
<li><a class="el" href="_docs_IE_DG_inference_engine_intro.html">Deep Learning Inference Engine</a> — A unified API to allow high performance inference on many hardware types including Intel® CPU, Intel® Processor Graphics, Intel® FPGA, Intel® Movidius™ Neural Compute Stick, and Intel® Neural Compute Stick 2.</li>
</ul>
</li>
<li><a href="https://docs.opencv.org/">OpenCV</a> — OpenCV* community version compiled for Intel® hardware. Includes PVL libraries for computer vision.</li>
<li>Drivers and runtimes for OpenCL™ version 2.1</li>
<li><a href="https://software.intel.com/en-us/media-sdk">Intel® Media SDK</a></li>
<li><a href="https://software.intel.com/en-us/cvsdk-ovx-guide">OpenVX*</a> — Intel's implementation of OpenVX* optimized for running on Intel® hardware (CPU, GPU, IPU).</li>
<li><a class="el" href="_docs_IE_DG_Samples_Overview.html">Demos and samples</a>.</li>
</ul>
<p>This Guide provides overview of the Inference Engine describing the typical workflow for performing inference of a pre-trained and optimized deep learning model and a set of sample applications.</p>
<blockquote class="doxtable">
<p><b>NOTES:</b></p><ul>
<li>Before you perform inference with the Inference Engine, your models should be converted to the Inference Engine format using the Model Optimizer. To learn about how to use Model Optimizer, refer to the <a class="el" href="_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html">Model Optimizer Developer Guide</a>. To learn about the pre-trained and optimized models delivered with the OpenVINO™ toolkit, refer to <a class="el" href="_models_intel_index.html">Pre-Trained Models</a>.</li>
<li><a href="https://software.intel.com/en-us/system-studio">Intel® System Studio</a> is an all-in-one, cross-platform tool suite, purpose-built to simplify system bring-up and improve system and IoT device application performance on Intel® platforms. If you are using the Intel® Distribution of OpenVINO™ with Intel® System Studio, go to <a href="https://software.intel.com/en-us/articles/get-started-with-openvino-and-intel-system-studio-2019">Get Started with Intel® System Studio</a>. </li>
</ul>
</blockquote>
<h2>Table of Contents</h2>
<ul>
<li><a class="el" href="_docs_IE_DG_Introduction.html">Introduction to Intel® Deep Learning Deployment Toolkit</a></li>
<li><a class="el" href="_docs_IE_DG_API_Changes.html">Inference Engine API Changes History</a></li>
<li><a class="el" href="_docs_IE_DG_inference_engine_intro.html">Introduction to Inference Engine</a></li>
<li><a class="el" href="_docs_IE_DG_NNBuilder.html">Introduction to Inference Engine Neural Network Builder</a></li>
<li><a class="el" href="_docs_IE_DG_Memory_primitives.html">Understanding Inference Engine Memory Primitives</a></li>
<li><a class="el" href="_docs_IE_DG_InferenceEngine_QueryAPI.html">Introduction to Inference Engine Device Query API</a></li>
<li><a class="el" href="_docs_IE_DG_Integrate_your_kernels_into_IE.html">Adding Your Own Kernels to the Inference Engine</a></li>
<li><a class="el" href="_docs_IE_DG_Integrate_with_customer_application_new_API.html">Integrating Inference Engine in Your Application</a></li>
<li><a class="el" href="_docs_IE_DG_Migration_CoreAPI.html">Migration from Inference Engine Plugin API to Core API</a></li>
<li><a class="el" href="_docs_IE_DG_Intro_to_Performance.html">Introduction to Performance Topics</a></li>
<li><a class="el" href="_inference_engine_ie_bridges_python_docs_api_overview.html">Inference Engine Python API Overview</a></li>
<li><a class="el" href="_docs_IE_DG_DynamicBatching.html">Using Dynamic Batching feature</a></li>
<li><a class="el" href="_docs_IE_DG_ShapeInference.html">Using Static Shape Infer feature</a></li>
<li><a class="el" href="_docs_IE_DG_Int8Inference.html">Using Low-Precision 8-bit Integer Inference</a><ul>
<li><a class="el" href="_inference_engine_tools_calibration_tool_README.html">Using Calibration Tool to Prepare Model Executed in Low-Precision 8-bit Integer Computation</a></li>
</ul>
</li>
<li>Utilities to Validate Your Converted Model<ul>
<li><a class="el" href="_docs_IE_DG_Cross_Check_Tool.html">Using Cross Check Tool for Per-Layer Comparison Between Plugins</a></li>
</ul>
</li>
<li><a class="el" href="_docs_IE_DG_supported_plugins_Supported_Devices.html">Supported Devices</a><ul>
<li><a class="el" href="_docs_IE_DG_supported_plugins_CL_DNN.html">GPU</a></li>
<li><a class="el" href="_docs_IE_DG_supported_plugins_CPU.html">CPU</a></li>
<li><a class="el" href="_docs_IE_DG_supported_plugins_FPGA.html">FPGA</a></li>
<li><a class="el" href="_docs_IE_DG_supported_plugins_VPU.html">VPU</a><ul>
<li><a class="el" href="_docs_IE_DG_supported_plugins_MYRIAD.html">MYRIAD</a></li>
<li><a class="el" href="_docs_IE_DG_supported_plugins_HDDL.html">HDDL</a></li>
</ul>
</li>
<li><a class="el" href="_docs_IE_DG_supported_plugins_HETERO.html">Heterogeneous execution</a></li>
<li><a class="el" href="_docs_IE_DG_supported_plugins_GNA.html">GNA</a></li>
<li><b>NEW!</b> <a class="el" href="_docs_IE_DG_supported_plugins_MULTI.html">MULTI</a></li>
</ul>
</li>
<li><a class="el" href="_models_intel_index.html">Pre-Trained Models</a></li>
<li><a class="el" href="_docs_IE_DG_Known_Issues_Limitations.html">Known Issues</a></li>
<li><a class="el" href="_docs_IE_DG_Legal_Information.html">Legal Information</a></li>
</ul>
<p><b>Typical Next Step:</b> <a class="el" href="_docs_IE_DG_Introduction.html">Introduction to Intel® Deep Learning Deployment Toolkit</a> </p>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>