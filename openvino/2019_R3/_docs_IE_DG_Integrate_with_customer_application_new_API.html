<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Integrate the Inference Engine with Your Application - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Integrate the Inference Engine with Your Application </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This section provides a high-level description of the process of integrating the Inference Engine into your application. Refer to the <a class="el" href="_inference_engine_samples_hello_classification_README.html">Hello Classification Sample</a> sources for example of using the Inference Engine in applications.</p>
<blockquote class="doxtable">
<p><b>NOTE</b>: For 2019 R2 Release, the new Inference Engine Core API is introduced. This guide is updated to reflect the new API approach. The Inference Engine Plugin API is still supported, but is going to be deprecated in future releases. Please, refer to <a class="el" href="_docs_IE_DG_Migration_CoreAPI.html">Migration from Inference Engine Plugin API to Core API</a> guide to update your application. </p>
</blockquote>
<h2>Using the Inference Engine API in Your Code</h2>
<p>The core <code>libinference_engine.so</code> library implements loading and parsing a model Intermediate Representation (IR), and triggers inference using a specified device. The core library has the following API:</p>
<ul>
<li><code><a class="el" href="classInferenceEngine_1_1Core.html" title="This class represents Inference Engine Core entity. It can throw exceptions safely for the applicatio...">InferenceEngine::Core</a></code></li>
<li><code><a class="el" href="classInferenceEngine_1_1Blob.html" title="This class represents a universal container in the Inference Engine. ">InferenceEngine::Blob</a></code>, <code><a class="el" href="classInferenceEngine_1_1TBlob.html" title="Represents real host memory allocated for a Tensor/Blob per C type. ">InferenceEngine::TBlob</a></code>, <code><a class="el" href="classInferenceEngine_1_1NV12Blob.html" title="Represents a blob that contains two planes (Y and UV) in NV12 color format. ">InferenceEngine::NV12Blob</a></code></li>
<li><code><a class="el" href="namespaceInferenceEngine.html#ad1c63c694d34358a748f591ffa74a9d0" title="This is a convenient type for working with a map containing pairs(string, pointer to a Blob instance)...">InferenceEngine::BlobMap</a></code></li>
<li><code><a class="el" href="namespaceInferenceEngine.html#a08270747275eb79985154365aa782a2a" title="A collection that contains string as key, and InputInfo smart pointer as value. ">InferenceEngine::InputsDataMap</a></code>, <code><a class="el" href="classInferenceEngine_1_1InputInfo.html" title="This class contains information about each input of the network. ">InferenceEngine::InputInfo</a></code>,</li>
<li><code><a class="el" href="namespaceInferenceEngine.html#a76ce999f68455cf962a473718deb500c" title="A collection that contains string as key, and Data smart pointer as value. ">InferenceEngine::OutputsDataMap</a></code></li>
</ul>
<p>C++ Inference Engine API wraps the capabilities of core library:</p>
<ul>
<li><code><a class="el" href="classInferenceEngine_1_1CNNNetReader.html" title="This is a wrapper class used to build and parse a network from the given IR. All the methods here can...">InferenceEngine::CNNNetReader</a></code></li>
<li><code><a class="el" href="classInferenceEngine_1_1CNNNetwork.html" title="This class contains all the information about the Neural Network and the related binary information...">InferenceEngine::CNNNetwork</a></code></li>
<li><code><a class="el" href="classInferenceEngine_1_1ExecutableNetwork.html" title="wrapper over IExecutableNetwork ">InferenceEngine::ExecutableNetwork</a></code></li>
<li><code><a class="el" href="classInferenceEngine_1_1InferRequest.html" title="This class is a wrapper of IInferRequest to provide setters/getters of input/output which operates wi...">InferenceEngine::InferRequest</a></code></li>
</ul>
<h2>Integration Steps</h2>
<p>Integration process includes the following steps: </p><div class="image">
<img src="integration_process.png" alt="integration_process.png"/>
</div>
<p>1) <b>Create Inference Engine Core</b> to manage available devices and their plugins internally. </p><div class="fragment"><div class="line"><a class="code" href="classInferenceEngine_1_1Core.html">InferenceEngine::Core</a> core;</div></div><!-- fragment --><p>2) Create an IR reader by creating an instance of <code><a class="el" href="classInferenceEngine_1_1CNNNetReader.html" title="This is a wrapper class used to build and parse a network from the given IR. All the methods here can...">InferenceEngine::CNNNetReader</a></code> and <b>read a model IR</b> created by the Model Optimizer: </p><div class="fragment"><div class="line">CNNNetReader network_reader;</div><div class="line">network_reader.ReadNetwork(<span class="stringliteral">&quot;Model.xml&quot;</span>);</div><div class="line">network_reader.ReadWeights(<span class="stringliteral">&quot;Model.bin&quot;</span>);</div></div><!-- fragment --><p>3) <b>Configure input and output</b>. Request input and output information using <code><a class="el" href="classInferenceEngine_1_1CNNNetReader.html#a1e3e65597e0426a672ce85d2bcffdb41" title="Gets a copy of built network object. ">InferenceEngine::CNNNetReader::getNetwork()</a></code>, <code><a class="el" href="classInferenceEngine_1_1CNNNetwork.html#add0cc549f1bd88f0f14abe52c897eb54" title="Wraps original method ICNNNetwork::getInputsInfo. ">InferenceEngine::CNNNetwork::getInputsInfo()</a></code>, and <code><a class="el" href="classInferenceEngine_1_1CNNNetwork.html#a3df61f333b129dbaebf96ae3cc18cd06" title="Wraps original method ICNNNetwork::getOutputsInfo. ">InferenceEngine::CNNNetwork::getOutputsInfo()</a></code> methods: </p><div class="fragment"><div class="line"><span class="keyword">auto</span> network = network_reader.getNetwork();<span class="comment"></span></div><div class="line"><span class="comment">/** Taking information about all topology inputs **/</span></div><div class="line"><a class="code" href="namespaceInferenceEngine.html#a08270747275eb79985154365aa782a2a">InferenceEngine::InputsDataMap</a> input_info(network.getInputsInfo());<span class="comment"></span></div><div class="line"><span class="comment">/** Taking information about all topology outputs **/</span></div><div class="line"><a class="code" href="namespaceInferenceEngine.html#a76ce999f68455cf962a473718deb500c">InferenceEngine::OutputsDataMap</a> output_info(network.getOutputsInfo());</div></div><!-- fragment --><p> Optionally, set the number format (precision) and memory layout for inputs and outputs. Refer to the <a class="el" href="_docs_IE_DG_supported_plugins_Supported_Devices.html">Supported configurations</a> chapter to choose the relevant configuration.</p>
<p>You can also allow input of any size. To do this, mark each input as resizable by setting a desired resize algorithm (e.g. <code>BILINEAR</code>) inside of the appropriate input info.</p>
<p>Basic color format conversions are supported as well. By default, the Inference Engine assumes that the input color format is <code>BGR</code> and color format conversions are disabled. The Inference Engine supports the following color format conversions:</p><ul>
<li><code>RGB-&gt;BGR</code></li>
<li><code>RGBX-&gt;BGR</code></li>
<li><code>BGRX-&gt;BGR</code></li>
<li><code>NV12-&gt;BGR</code></li>
</ul>
<p>where <code>X</code> is a channel that will be ignored during inference. To enable the conversions, set a desired color format (for example, <code>RGB</code>) for each input inside of the appropriate input info.</p>
<p>If you want to run inference for multiple images at once, you can use the built-in batch pre-processing functionality.</p>
<blockquote class="doxtable">
<p><b>NOTE</b>: Batch pre-processing is not supported if input color format is set to <code>ColorFormat::NV12</code>. </p>
</blockquote>
<p>You can use the following code snippet to configure input and output: </p><div class="fragment"><div class="line"><span class="comment">/** Iterating over all input info**/</span></div><div class="line"><span class="keywordflow">for</span> (<span class="keyword">auto</span> &amp;item : input_info) {</div><div class="line">    <span class="keyword">auto</span> input_data = item.second;</div><div class="line">    input_data-&gt;setPrecision(Precision::U8);</div><div class="line">    input_data-&gt;setLayout(Layout::NCHW);</div><div class="line">    input_data-&gt;getPreProcess().setResizeAlgorithm(RESIZE_BILINEAR);</div><div class="line">    input_data-&gt;getPreProcess().setColorFormat(<a class="code" href="namespaceInferenceEngine.html#a5ee5ca7708cc67a9a0becc2593d0558aae2262afdcd9754598dbc87e4a4725246">ColorFormat::RGB</a>);</div><div class="line">}<span class="comment"></span></div><div class="line"><span class="comment">/** Iterating over all output info**/</span></div><div class="line"><span class="keywordflow">for</span> (<span class="keyword">auto</span> &amp;item : output_info) {</div><div class="line">    <span class="keyword">auto</span> output_data = item.second;</div><div class="line">    output_data-&gt;setPrecision(Precision::FP32);</div><div class="line">    output_data-&gt;setLayout(Layout::NC);</div><div class="line">}</div></div><!-- fragment --><blockquote class="doxtable">
<p><b>NOTE</b>: NV12 input color format pre-processing differs from other color conversions. In case of NV12, Inference Engine expects two separate image planes (Y and UV). You must use a specific <code><a class="el" href="classInferenceEngine_1_1NV12Blob.html" title="Represents a blob that contains two planes (Y and UV) in NV12 color format. ">InferenceEngine::NV12Blob</a></code> object instead of default blob object and set this blob to the Inference Engine Infer Request using <code><a class="el" href="classInferenceEngine_1_1InferRequest.html#a27fb179e3bae652d76076965fd2a5653" title="Sets input/output data to infer. ">InferenceEngine::InferRequest::SetBlob()</a></code>. Refer to <a class="el" href="_inference_engine_samples_hello_nv12_input_classification_README.html">Hello NV12 Input Classification C++ Sample</a> for more details. </p>
</blockquote>
<p>If you skip this step, the default values are set:</p>
<ul>
<li>no resize algorithm is set for inputs</li>
<li>input color format - <code>ColorFormat::RAW</code> meaning that input does not need color conversions</li>
<li>input and output precision - <code>Precision::FP32</code></li>
<li>input layout - <code>Layout::NCHW</code></li>
<li>output layout depends on number of its dimensions:</li>
</ul>
<table class="doxtable">
<tr>
<th align="left">Number of dimensions </th><th>5 </th><th>4 </th><th>3 </th><th>2 </th><th>1  </th></tr>
<tr>
<td align="left">Layout </td><td>NCDHW </td><td>NCHW </td><td>CHW </td><td>NC </td><td>C </td></tr>
</table>
<p>4) <b>Load the model</b> to the device using <code><a class="el" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff" title="Creates an executable network from a network object. Users can create as many networks as they need a...">InferenceEngine::Core::LoadNetwork()</a></code>: </p><div class="fragment"><div class="line"><span class="keyword">auto</span> executable_network = core.<a class="code" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff">LoadNetwork</a>(network, <span class="stringliteral">&quot;CPU&quot;</span>);</div></div><!-- fragment --><p> It creates an executable network from a network object. The executable network is associated with single hardware device. It is possible to create as many networks as needed and to use them simultaneously (up to the limitation of the hardware resources). Second parameter is a configuration for plugin. It is map of pairs: (parameter name, parameter value). Choose device from <a class="el" href="_docs_IE_DG_supported_plugins_Supported_Devices.html">Supported devices</a> page for more details about supported configuration parameters. </p><div class="fragment"><div class="line"><span class="comment">/** Optional config. E.g. this enables profiling of performance counters. **/</span></div><div class="line">std::map&lt;std::string, std::string&gt; config = {{ <a class="code" href="namespaceInferenceEngine_1_1PluginConfigParams.html#a06e7d1c7f8905f0915d73eba49fa1bed">PluginConfigParams::KEY_PERF_COUNT</a>, PluginConfigParams::YES }};</div><div class="line"><span class="keyword">auto</span> executable_network = core.<a class="code" href="classInferenceEngine_1_1Core.html#afcd1cc386d0ef3d2d33c6bf7d447d5ff">LoadNetwork</a>(network, <span class="stringliteral">&quot;CPU&quot;</span>, config);</div></div><!-- fragment --><p>5) <b>Create an infer request</b>: </p><div class="fragment"><div class="line"><span class="keyword">auto</span> infer_request = executable_network.CreateInferRequest();</div></div><!-- fragment --><p>6) <b>Prepare input</b>. You can use one of the following options to prepare input:</p><ul>
<li><b>Optimal way for a single network.</b> Get blobs allocated by an infer request using <code><a class="el" href="classInferenceEngine_1_1InferRequest.html#a9601a4cda3f309181af34feedf1b914c" title="Wraps original method IInferRequest::GetBlob. ">InferenceEngine::InferRequest::GetBlob()</a></code> and feed an image and the input data to the blobs. In this case, input data must be aligned (resized manually) with a given blob size and have a correct color format. <div class="fragment"><div class="line"><span class="comment">/** Iterating over all input blobs **/</span></div><div class="line"><span class="keywordflow">for</span> (<span class="keyword">auto</span> &amp; item : inputInfo) {</div><div class="line">    <span class="keyword">auto</span> input_name = item-&gt;first;<span class="comment"></span></div><div class="line"><span class="comment">    /** Getting input blob **/</span></div><div class="line">    <span class="keyword">auto</span> input = infer_request.GetBlob(input_name);<span class="comment"></span></div><div class="line"><span class="comment">    /** Fill input tensor with planes. First b channel, then g and r channels **/</span></div><div class="line">    ...</div><div class="line">}</div></div><!-- fragment --></li>
<li><b>Optimal way for a cascade of networks (output of one network is input for another).</b> Get output blob from the first request using <code><a class="el" href="classInferenceEngine_1_1InferRequest.html#a9601a4cda3f309181af34feedf1b914c" title="Wraps original method IInferRequest::GetBlob. ">InferenceEngine::InferRequest::GetBlob()</a></code> and set it as input for the second request using <code><a class="el" href="classInferenceEngine_1_1InferRequest.html#a27fb179e3bae652d76076965fd2a5653" title="Sets input/output data to infer. ">InferenceEngine::InferRequest::SetBlob()</a></code>. <div class="fragment"><div class="line"><span class="keyword">auto</span> output = infer_request1-&gt;GetBlob(output_name);</div><div class="line">infer_request2-&gt;SetBlob(input_name, output);</div></div><!-- fragment --></li>
<li><b>Optimal way for ROI handling (a ROI object located inside of input of one network is input for another).</b> It is possible to re-use shared input by several networks. You do not need to allocate separate input blob for a network if it processes a ROI object located inside of already allocated input of a previous network. For instance, when first network detects objects on a video frame (stored as input blob) and second network accepts detected bounding boxes (ROI inside of the frame) as input. In this case, it is allowed to re-use pre-allocated input blob (used by first network) by second network and just crop ROI without allocation of new memory using <code><a class="el" href="namespaceInferenceEngine.html#ad9bb08c6ea48c086cec356d10151d44e" title="Creates a blob with given precision and dimensions. ">InferenceEngine::make_shared_blob()</a></code> with passing of <code><a class="el" href="classInferenceEngine_1_1Blob.html#abb6c4f89181e2dd6d8a29ada2dfb4060" title="A smart pointer containing Blob object. ">InferenceEngine::Blob::Ptr</a></code> and <code><a class="el" href="structInferenceEngine_1_1ROI.html" title="This structure describes ROI data. ">InferenceEngine::ROI</a></code> as parameters. <div class="fragment"><div class="line"><span class="comment">/** inputBlob points to input of a previous network and</span></div><div class="line"><span class="comment">    cropROI contains coordinates of output bounding box **/</span></div><div class="line"><a class="code" href="classInferenceEngine_1_1Blob.html#abb6c4f89181e2dd6d8a29ada2dfb4060">InferenceEngine::Blob::Ptr</a> inputBlob;</div><div class="line"><a class="code" href="structInferenceEngine_1_1ROI.html">InferenceEngine::ROI</a> cropRoi;</div><div class="line">...</div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">/** roiBlob uses shared memory of inputBlob and describes cropROI</span></div><div class="line"><span class="comment">    according to its coordinates **/</span></div><div class="line">auto roiBlob = <a class="code" href="namespaceInferenceEngine.html#ad9bb08c6ea48c086cec356d10151d44e">InferenceEngine::make_shared_blob</a>(inputBlob, cropRoi);</div><div class="line">infer_request2-&gt;SetBlob(input_name, roiBlob);</div></div><!-- fragment --> Make sure that shared input is kept valid during execution of each network. Otherwise, ROI blob may be corrupted if the original input blob (that ROI is cropped from) has already been rewritten.</li>
<li>Allocate input blobs of the appropriate types and sizes, feed an image and the input data to the blobs, and call <code><a class="el" href="classInferenceEngine_1_1InferRequest.html#a27fb179e3bae652d76076965fd2a5653" title="Sets input/output data to infer. ">InferenceEngine::InferRequest::SetBlob()</a></code> to set these blobs for an infer request: <div class="fragment"><div class="line"><span class="comment">/** Iterating over all input blobs **/</span></div><div class="line"><span class="keywordflow">for</span> (<span class="keyword">auto</span> &amp; item : inputInfo) {</div><div class="line">    <span class="keyword">auto</span> input_data = item-&gt;second;<span class="comment"></span></div><div class="line"><span class="comment">    /** Creating input blob **/</span></div><div class="line">    <a class="code" href="classInferenceEngine_1_1TBlob.html#a6af98afe8c25be14b916348df0712a09">InferenceEngine::TBlob&lt;unsigned char&gt;::Ptr</a> input;</div><div class="line">    <span class="comment">// assuming input precision was asked to be U8 in prev step</span></div><div class="line">    input = InferenceEngine::make_shared_blob&lt;unsigned char, InferenceEngine::SizeVector&gt;(<a class="code" href="classInferenceEngine_1_1Precision.html">InferenceEngine::Precision</a>:U8, input_data-&gt;getDims());</div><div class="line">    input-&gt;<a class="code" href="classInferenceEngine_1_1TBlob.html#a55e95d6fd89bf29e39376429467db0ed">allocate</a>();</div><div class="line">    infer_request-&gt;SetBlob(item.first, input);</div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">    /** Fill input tensor with planes. First b channel, then g and r channels **/</span></div><div class="line">    ...</div><div class="line">}</div></div><!-- fragment --> A blob can be filled before and after <code>SetBlob()</code>.</li>
</ul>
<blockquote class="doxtable">
<p><b>NOTE:</b></p>
<ul>
<li><code>SetBlob()</code> method compares precision and layout of an input blob with ones defined on step 3 and throws an exception if they do not match. It also compares a size of the input blob with input size of the read network. But if input was configured as resizable, you can set an input blob of any size (for example, any ROI blob). Input resize will be invoked automatically using resize algorithm configured on step 3. Similarly to the resize, color format conversions allow the color format of an input blob to differ from the color format of the read network. Color format conversion will be invoked automatically using color format configured on step 3.</li>
<li><code>GetBlob()</code> logic is the same for pre-processable and not pre-processable input. Even if it is called with input configured as resizable or as having specific color format, a blob allocated by an infer request is returned. Its size and color format are already consistent with the corresponding values of the read network. No pre-processing will happen for this blob. If you call <code>GetBlob()</code> after <code>SetBlob()</code>, you will get the blob you set in <code>SetBlob()</code>. </li>
</ul>
</blockquote>
<p>7) <b>Do inference</b> by calling the <code><a class="el" href="classInferenceEngine_1_1InferRequest.html#a405293e8423d82a5b45f642a3bef0d24" title="Start inference of specified input(s) in asynchronous mode. ">InferenceEngine::InferRequest::StartAsync</a></code> and <code><a class="el" href="classInferenceEngine_1_1InferRequest.html#ab5887acbd127429f628b77f4970cc701" title="Wraps original method IInferRequest::Wait. ">InferenceEngine::InferRequest::Wait</a></code> methods for asynchronous request: </p><div class="fragment"><div class="line">infer_request-&gt;StartAsync();</div><div class="line">infer_request.Wait(IInferRequest::WaitMode::RESULT_READY);</div></div><!-- fragment --><p>or by calling the <code><a class="el" href="classInferenceEngine_1_1InferRequest.html#a3391ce30894abde730523e9ca9371ce8" title="Wraps original method IInferRequest::Infer. ">InferenceEngine::InferRequest::Infer</a></code> method for synchronous request: </p><div class="fragment"><div class="line">sync_infer_request-&gt;Infer();</div></div><!-- fragment --><p> <code>StartAsync</code> returns immediately and starts inference without blocking main thread, <code>Infer</code> blocks main thread and returns when inference is completed. Call <code>Wait</code> for waiting result to become available for asynchronous request.</p>
<p>There are three ways to use it:</p><ul>
<li>specify maximum duration in milliseconds to block for. The method is blocked until the specified timeout has elapsed, or the result becomes available, whichever comes first.</li>
<li><code>InferenceEngine::IInferRequest::WaitMode::RESULT_READY</code> - waits until inference result becomes available</li>
<li><code>InferenceEngine::IInferRequest::WaitMode::STATUS_ONLY</code> - immediately returns request status.It does not block or interrupts current thread.</li>
</ul>
<p>Both requests are thread-safe: can be called from different threads without fearing corruption and failures.</p>
<p>Multiple requests for single <code>ExecutableNetwork</code> are executed sequentially one by one in FIFO order.</p>
<p>While request is ongoing, all its methods except <code><a class="el" href="classInferenceEngine_1_1InferRequest.html#ab5887acbd127429f628b77f4970cc701" title="Wraps original method IInferRequest::Wait. ">InferenceEngine::InferRequest::Wait</a></code> would throw an exception.</p>
<p>8) Go over the output blobs and <b>process the results</b>. Note that casting <code>Blob</code> to <code>TBlob</code> via <code>std::dynamic_pointer_cast</code> is not recommended way, better to access data via <code>buffer()</code> and <code><a class="el" href="namespaceInferenceEngine.html#a2221ba6e6a6337f6b40fb6e1d49f0a8c" title="Helper cast function to work with shared Blob objects. ">as()</a></code> methods as follows: </p><div class="fragment"><div class="line"><span class="keywordflow">for</span> (<span class="keyword">auto</span> &amp;item : output_info) {</div><div class="line">    <span class="keyword">auto</span> output_name = item.first;</div><div class="line">    <span class="keyword">auto</span> output = infer_request.GetBlob(output_name);</div><div class="line">    {</div><div class="line">        <span class="keyword">auto</span> <span class="keyword">const</span> memLocker = output-&gt;cbuffer(); <span class="comment">// use const memory locker</span></div><div class="line">        <span class="comment">// output_buffer is valid as long as the lifetime of memLocker</span></div><div class="line">        <span class="keyword">const</span> <span class="keywordtype">float</span> *output_buffer = memLocker.as&lt;<span class="keyword">const</span> <span class="keywordtype">float</span> *&gt;();<span class="comment"></span></div><div class="line"><span class="comment">        /** output_buffer[] - accessing output blob data **/</span></div></div><!-- fragment --><h2>Building Your Application</h2>
<p>For details about building your application, refer to the CMake files for the sample applications. All samples reside in the samples directory in the Inference Engine installation directory.</p>
<h3>Running Your Application</h3>
<p>Before running compiled binary files, make sure your application can find the Inference Engine libraries. On Linux* operating systems, including Ubuntu* and CentOS*, the <code>LD_LIBRARY_PATH</code> environment variable is usually used to specify directories to be looked for libraries. You can update the <code>LD_LIBRARY_PATH</code> with paths to the directories in the Inference Engine installation directory where the libraries reside.</p>
<p>Add a path the directory containing the core and plugin libraries:</p><ul>
<li>For the Inference Engine installed within the OpenVINO&trade; toolkit package: <div class="fragment"><div class="line">$ export LD_LIBRARY_PATH=/opt/intel/computer_vision_sdk_&lt;version&gt;/inference_engine/lib/&lt;linux_version&gt;/intel64:$LD_LIBRARY_PATH</div></div><!-- fragment --></li>
<li>For Intel&reg; Deep Learning Deployment Toolkit installation: <div class="fragment"><div class="line">$ export LD_LIBRARY_PATH=/opt/intel/deep_learning_sdk_&lt;version&gt;/deployment_tools/inference_engine/lib/&lt;linux_version&gt;/intel64:$LD_LIBRARY_PATH</div></div><!-- fragment --></li>
</ul>
<p>Add paths the directories containing the required third-party libraries:</p><ul>
<li>For Inference Engine installed within the OpenVINO&trade; toolkit package: <div class="fragment"><div class="line">$ export LD_LIBRARY_PATH=/opt/intel/computer_vision_sdk_&lt;version&gt;/inference_engine/external/mklml_lnx/lib:$LD_LIBRARY_PATH</div><div class="line">$ export LD_LIBRARY_PATH=/opt/intel/computer_vision_sdk_&lt;version&gt;/inference_engine/external/cldnn/lib:$LD_LIBRARY_PATH</div></div><!-- fragment --></li>
<li>For Intel&reg; Deep Learning Deployment Toolkit installation: <div class="fragment"><div class="line">export LD_LIBRARY_PATH=/opt/intel/deep_learning_sdk_&lt;version&gt;/deployment_tools/external/mklml_lnx/lib:$LD_LIBRARY_PATH</div><div class="line">export LD_LIBRARY_PATH=/opt/intel/deep_learning_sdk_&lt;version&gt;/deployment_tools/external/cldnn/lib:$LD_LIBRARY_PATH</div></div><!-- fragment --></li>
</ul>
<p>Alternatively, you can use the following scripts that reside in the Inference Engine directory of the OpenVINO&trade; toolkit and Intel&reg; Deep Learning Deployment Toolkit installation folders respectively:</p>
<ul>
<li><div class="fragment"><div class="line">/opt/intel/computer_vision_sdk_&lt;version&gt;/bin/setupvars.sh</div></div><!-- fragment --></li>
<li><div class="fragment"><div class="line">/opt/intel/deep_learning_sdk_&lt;version&gt;/deployment_tools/inference_engine/bin/setvars.sh</div></div><!-- fragment --></li>
</ul>
<p>To run compiled applications on Microsoft* Windows* OS, make sure that Microsoft* Visual C++ 2015 Redistributable and Intel® C++ Compiler 2017 Redistributable packages are installed and <code>&lt;INSTALL_DIR&gt;/bin/intel64/Release/*.dll</code> files are placed to the application folder or accessible via <code>PATH%</code> environment variable. </p>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>