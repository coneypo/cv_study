<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Converting TensorFlow*-Slim Image Classification Model Library Models - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Converting TensorFlow*-Slim Image Classification Model Library Models </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a href="https://github.com/tensorflow/models/tree/master/research/slim/README.md">TensorFlow*-Slim Image Classification Model Library</a> is a library to define, train and evaluate classification models in TensorFlow*. The library contains Python scripts defining the classification topologies together with checkpoint files for several pre-trained classification topologies. To convert a TensorFlow*-Slim library model, complete the following steps:</p>
<ol type="1">
<li>Download the TensorFlow*-Slim models <a href="https://github.com/tensorflow/models">git repository</a>.</li>
<li>Download the pre-trained model <a href="https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models">checkpoint</a>.</li>
<li>Export the inference graph.</li>
<li>Convert the model using the Model Optimizer.</li>
</ol>
<p>The <a href="#example_of_an_inception_v1_model_conversion">Example of an Inception V1 Model Conversion</a> section below illustrates the process of converting an Inception V1 Model.</p>
<h2>Example of an Inception V1 Model Conversion <a class="anchor" id="example_of_an_inception_v1_model_conversion"></a></h2>
<p>This example demonstrates how to convert the model on Linux* OSes, but it could be easily adopted for the Windows* OSes.</p>
<p>Step 1. Create a new directory to clone the TensorFlow*-Slim git repository to:</p>
<div class="fragment"><div class="line">mkdir tf_models</div></div><!-- fragment --> <div class="fragment"><div class="line">git clone https://github.com/tensorflow/models.git tf_models</div></div><!-- fragment --><p>Step 2. Download and unpack the <a href="http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz">Inception V1 model checkpoint file</a>:</p>
<div class="fragment"><div class="line">wget http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz</div></div><!-- fragment --> <div class="fragment"><div class="line">tar xzvf inception_v1_2016_08_28.tar.gz</div></div><!-- fragment --><p>Step 3. Export the inference graph &mdash; the protobuf file (<code>.pb</code>) containing the architecture of the topology. Note, this file <em>doesn't</em> contain the neural network weights and cannot be used for inference.</p>
<div class="fragment"><div class="line">python3 tf_models/research/slim/export_inference_graph.py \</div><div class="line">    --model_name inception_v1 \</div><div class="line">    --output_file inception_v1_inference_graph.pb</div></div><!-- fragment --><p>Model Optimizer comes with the summarize graph utility, which identifies graph input and output nodes. Run the utility to determine input/output nodes of the Inception V1 model:</p>
<div class="fragment"><div class="line">python3 &lt;MODEL_OPTIMIZER_INSTALL_DIR&gt;/mo/utils/summarize_graph.py --input_model ./inception_v1_inference_graph.pb</div></div><!-- fragment --><p>The output looks as follows:<br />
 </p><div class="fragment"><div class="line">1 input(s) detected:</div><div class="line">Name: input, type: float32, shape: (-1,224,224,3)</div><div class="line">1 output(s) detected:</div><div class="line">InceptionV1/Logits/Predictions/Reshape_1</div></div><!-- fragment --><p> The tool finds one input node with name <code>input</code>, type <code>float32</code>, fixed image size <code>(224,224,3)</code> and undefined batch size <code>-1</code>. The output node name is <code>InceptionV1/Logits/Predictions/Reshape_1</code>.<br />
</p>
<p>Step 4. Convert the model with the Model Optimizer:</p>
<div class="fragment"><div class="line">&lt;MODEL_OPTIMIZER_INSTALL_DIR&gt;/mo_tf.py --input_model ./inception_v1_inference_graph.pb --input_checkpoint ./inception_v1.ckpt -b 1 --mean_value [127.5,127.5,127.5] --scale 127.5</div></div><!-- fragment --><p>The <code>-b</code> command line parameter is required because the Model Optimizer cannot convert a model with undefined input size.</p>
<p>Refer to the <a href="#tf_slim_mean_scale_values">Mean and Scale Values for TensorFlow*-Slim Models</a> for the information why <code>--mean_values</code> and <code>--scale</code> command line parameters are used.</p>
<h2>Mean and Scale Values for TensorFlow*-Slim Models <a class="anchor" id="tf_slim_mean_scale_values"></a></h2>
<p>The TensorFlow*-Slim Models were trained with normalized input data. There are several different normalization algorithms used in the Slim library. Inference Engine classification sample does not perform image pre-processing except resizing to the input layer size. It is necessary to pass mean and scale values to the Model Optimizer so they are embedded into the generated IR in order to get correct classification results.</p>
<p>The file <a href="https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/preprocessing_factory.py">preprocessing_factory.py</a> contains a dictionary variable <code>preprocessing_fn_map</code> defining mapping between the model type and pre-processing function to be used. The function code should be analyzed to figure out the mean/scale values.</p>
<p>The <a href="https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py">inception_preprocessing.py</a> file defines the pre-processing function for the Inception models. The <code>preprocess_for_eval</code> function contains the following code:</p>
<div class="fragment"><div class="line">...</div><div class="line">if image.dtype != tf.float32:</div><div class="line">  image = tf.image.convert_image_dtype(image, dtype=tf.float32)</div><div class="line">...</div><div class="line">image = tf.subtract(image, 0.5)</div><div class="line">image = tf.multiply(image, 2.0)</div><div class="line">return image</div></div><!-- fragment --><p>Firstly, the <code>image</code> is converted to data type <code>tf.float32</code> and the values in the tensor are scaled to the <code>[0, 1]</code> range using the <a href="https://www.tensorflow.org/api_docs/python/tf/image/convert_image_dtype">tf.image.convert_image_dtype</a> function. Then the <code>0.5</code> is subtracted from the image values and values multiplied by <code>2.0</code>. The final image range of values is <code>[-1, 1]</code>.</p>
<p>Inference Engine classification sample reads an input image as a three-dimensional array of integer values from the range <code>[0, 255]</code>. In order to scale them to <code>[-1, 1]</code> range, the mean value <code>127.5</code> for each image channel should be specified as well as scale factor <code>127.5</code>.</p>
<p>Similarly, the mean/scale values can be determined for other Slim models.</p>
<p>The exact mean/scale values are defined in the table with list of supported TensorFlow*-Slim models at the <a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html">Converting a TensorFlow* Model</a>. </p>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>