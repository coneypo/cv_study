<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Model Optimizer Developer Guide - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Model Optimizer Developer Guide </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Model Optimizer is a cross-platform command-line tool that facilitates the transition between the training and deployment environment, performs static model analysis, and adjusts deep learning models for optimal execution on end-point target devices.</p>
<p>Model Optimizer process assumes you have a network model trained using a supported deep learning framework. The scheme below illustrates the typical workflow for deploying a trained deep learning model:</p>
<div class="image">
<img src="workflow_steps.png" alt="workflow_steps.png"/>
</div>
<p>Model Optimizer produces an Intermediate Representation (IR) of the network, which can be read, loaded, and inferred with the Inference Engine. The Inference Engine API offers a unified API across a number of supported Intel&reg; platforms. The Intermediate Representation is a pair of files describing the model:</p>
<ul>
<li><code>.xml</code> - Describes the network topology</li>
<li><code>.bin</code> - Contains the weights and biases binary data.</li>
</ul>
<h2>What's New in the Model Optimizer in this Release?</h2>
<ul>
<li>ONNX*:<ul>
<li>Added support of the following ONNX* operations: DeformableConvolution, Upsample operation supporting 7th and 9th versions of opset, Gemm operation supporting alpha and beta attributes.</li>
<li>Added support of the following Pytorch* topologies through conversion to ONNX*: ESPNet models from <a href="https://github.com/sacmehta/ESPNet/tree/master/pretrained">https://github.com/sacmehta/ESPNet/tree/master/pretrained</a>.</li>
</ul>
</li>
<li>TensorFlow*:<ul>
<li>Added support of the following TensorFlow* operations: Erf, BatchMatMul, SpaceToDepth, Fill, Select, OneHot, TopK, GatherTree, LogicalAnd, LogicalOr, Equal, NotEqual, Less, LessEqual, Greater, GreaterEqual, Squeeze and ExpandDims (not converted to the Reshape layer anymore).</li>
<li>Updated Model Optimizer to be compatible with TensorFlow 1.14.0.</li>
<li>Added support of the following TensorFlow* topologies:<ul>
<li>TensorFlow Object Detection API version 1.13.X and 1.14.0 topologies</li>
<li>I3D topologies from <a href="https://github.com/deepmind/kinetics-i3d">https://github.com/deepmind/kinetics-i3d</a></li>
<li>GNMT (<a href="https://github.com/tensorflow/nmt">https://github.com/tensorflow/nmt</a>) can be converted using <a class="el" href="_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_GNMT_From_Tensorflow.html">this instruction</a></li>
<li>BERT (<a href="https://github.com/google-research/bert">https://github.com/google-research/bert</a>) can be converted using <a class="el" href="_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_BERT_From_Tensorflow.html">this instruction</a></li>
</ul>
</li>
<li>Added support of the Dynamic sequence lengths in TensorFlow recurrent models</li>
</ul>
</li>
<li>Caffe*:<ul>
<li>Added support of SSH model (Single Stage Headless Face Detector) from <a href="https://github.com/mahyarnajibi/SSH">https://github.com/mahyarnajibi/SSH</a>.</li>
</ul>
</li>
<li>MXNet*:<ul>
<li>Added support of the following MXNet* operations: DeformableConvolution, DeformablePSROIPooling, Where, exp, slice_like, div_scalar, minus_scalar, greater_scalar, elemtwise_sub.</li>
</ul>
</li>
<li>Kaldi*:<ul>
<li>Added support for nnet3 TDNN networks.</li>
</ul>
</li>
<li>Common changes:<ul>
<li>Updated the IR version from 5 to 6.</li>
<li>Extended the <code>--input</code> command line parameters to specify shapes and values for freezing arbitrary nodes (not only model inputs). The command line parameter <code>--freeze_placeholder_with_value</code> is deprecated.</li>
<li>Implemented fusing of a Softmax layer pattern from Pytorch*.</li>
<li>Introduced the new model transformation API for writing better Model Optimizer extensions.</li>
<li>Renamed Intel experimental layer Quantize to FakeQuantize and ONNX Intel experimental operator Quantize to FakeQuantize</li>
</ul>
</li>
</ul>
<p>Notice that certain topology-specific layers (like DetectionOutput used in the SSD*) and several general-purpose layers (like Squeeze and Unsqueeze) are now delivered in the source code. This assumes that the <a class="el" href="_inference_engine_src_extension_README.html">extensions library</a> is compiled/loaded. The extensions are also required for the <a class="el" href="_models_intel_index.html">pre-trained models</a> inference. Please refer to the <a class="el" href="_docs_IE_DG_supported_plugins_Supported_Devices.html">complete list of layers</a> that require the extensions library.</p>
<blockquote class="doxtable">
<p><b>NOTE:</b> <a href="https://software.intel.com/en-us/system-studio">Intel® System Studio</a> is an all-in-one, cross-platform tool suite, purpose-built to simplify system bring-up and improve system and IoT device application performance on Intel® platforms. If you are using the Intel® Distribution of OpenVINO™ with Intel® System Studio, go to <a href="https://software.intel.com/en-us/articles/get-started-with-openvino-and-intel-system-studio-2019">Get Started with Intel® System Studio</a>. </p>
</blockquote>
<h2>Table of Content</h2>
<ul>
<li><a class="el" href="_docs_IE_DG_Introduction.html">Introduction to Intel&reg; Deep Learning Deployment Toolkit</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_Prepare_Trained_Model.html">Preparing and Optimizing your Trained Model with Model Optimizer</a><ul>
<li><a class="el" href="_docs_MO_DG_prepare_model_Config_Model_Optimizer.html">Configuring Model Optimizer</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Converting_Model.html">Converting a Model to Intermediate Representation (IR)</a><ul>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Converting_Model_General.html">Converting a Model Using General Conversion Parameters</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html">Converting Your Caffe* Model</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html">Converting Your TensorFlow* Model</a><ul>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_BERT_From_Tensorflow.html">Converting BERT from TensorFlow</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_GNMT_From_Tensorflow.html">Converting GNMT from TensorFlow</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_YOLO_From_Tensorflow.html">Converting YOLO from DarkNet to Tensorflow and then to IR</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_FaceNet_From_Tensorflow.html">Converting FaceNet from TensorFlow</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_DeepSpeech_From_Tensorflow.html">Converting DeepSpeech from TensorFlow</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_lm_1b_From_Tensorflow.html">Converting Language Model on One Billion Word Benchmark from TensorFlow</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_NCF_From_Tensorflow.html">Converting Neural Collaborative Filtering Model from TensorFlow*</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Object_Detection_API_Models.html">Converting TensorFlow* Object Detection API Models</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Slim_Library_Models.html">Converting TensorFlow*-Slim Image Classification Model Library Models</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_CRNN_From_Tensorflow.html">Converting CRNN Model from TensorFlow*</a></li>
</ul>
</li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_MxNet.html">Converting Your MXNet* Model</a><ul>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_mxnet_specific_Convert_Style_Transfer_From_MXNet.html">Converting a Style Transfer Model from MXNet</a></li>
</ul>
</li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Kaldi.html">Converting Your Kaldi* Model</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html">Converting Your ONNX* Model</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_Model_Optimization_Techniques.html">Model Optimizations Techniques</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Cutting_Model.html">Cutting parts of the model</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Subgraph_Replacement_Model_Optimizer.html">Sub-graph Replacement in Model Optimizer</a><ul>
<li><a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_TensorFlow_SSD_ObjectDetection_API.html">(Deprecated) Case-Study: Converting SSD models created with the TensorFlow* Object Detection API</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_TensorFlow_Faster_RCNN_ObjectDetection_API.html">(Deprecated) Case-Study: Converting Faster R-CNN models created with the TensorFlow* Object Detection API</a></li>
</ul>
</li>
<li><a class="el" href="_docs_MO_DG_prepare_model_Supported_Frameworks_Layers.html">Supported Framework Layers</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_IRLayersCatalogSpec.html">IR Notation Reference</a></li>
</ul>
</li>
<li><a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Customize_Model_Optimizer.html">Custom Layers in Model Optimizer</a><ul>
<li><a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Extending_Model_Optimizer_with_New_Primitives.html">Extending Model Optimizer with New Primitives</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Extension_generation.html">Using Automatic Generation for Creating Model Optimizer and Inference Engine Extension</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Legacy_Mode_for_Caffe_Custom_Layers.html">Legacy Mode for Caffe* Custom Layers</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Offloading_Sub_Graph_Inference.html">Offloading Sub-Graph Inference</a></li>
</ul>
</li>
<li><a class="el" href="_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html">Model Optimizer Frequently Asked Questions</a></li>
</ul>
</li>
<li><a class="el" href="_docs_MO_DG_Known_Issues_Limitations.html">Known Issues</a></li>
</ul>
<p><b>Typical Next Step:</b> <a class="el" href="_docs_IE_DG_Introduction.html">Introduction to Intel® Deep Learning Deployment Toolkit</a> </p>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>