<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Inference Engine Demos - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Inference Engine Demos </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The Open Model Zoo demo applications are console applications that demonstrate how you can use the Inference Engine in your applications to solve specific use-cases.</p>
<p>The Open Model Zoo includes the following demos:</p>
<ul>
<li><a class="el" href="_demos_python_demos_action_recognition_README.html">Action Recognition Python* Demo</a> - Demo application for Action Recognition algorithm, which classifies actions that are being performed on input video.</li>
<li><a class="el" href="_demos_crossroad_camera_demo_README.html">Crossroad Camera C++ Demo</a> - Person Detection followed by the Person Attributes Recognition and Person Reidentification Retail, supports images/video and camera inputs.</li>
<li><a class="el" href="_demos_gaze_estimation_demo_README.html">Gaze Estimation C++ Demo</a> - Face detection followed by gaze estimation, head pose estimation and facial landmarks regression.</li>
<li><a class="el" href="_demos_human_pose_estimation_demo_README.html">Human Pose Estimation C++ Demo</a> - Human pose estimation demo.</li>
<li><a class="el" href="_demos_python_demos_image_retrieval_demo_README.html">Image Retrieval Python* Demo</a> - The demo demonstrates how to run Image Retrieval models using OpenVINO&trade;.</li>
<li><a class="el" href="_demos_segmentation_demo_README.html">Image Segmentation C++ Demo</a> - Inference of image segmentation networks like FCN8 (the demo supports only images as inputs).</li>
<li><a class="el" href="_demos_python_demos_instance_segmentation_demo_README.html">Instance Segmentation Python* Demo</a> - Inference of instance segmentation networks trained in <code>Detectron</code> or <code>maskrcnn-benchmark</code>.</li>
<li><a class="el" href="_demos_interactive_face_detection_demo_README.html">Interactive Face Detection C++ Demo</a> - Face Detection coupled with Age/Gender, Head-Pose, Emotion, and Facial Landmarks detectors. Supports video and camera inputs.</li>
<li><a class="el" href="_demos_python_demos_face_recognition_demo_README.html">Interactive Face Recognition Python* Demo</a> - Face Detection coupled with Head-Pose, Facial Landmarks and Face Recognition detectors. Supports video and camera inputs.</li>
<li><a class="el" href="_demos_mask_rcnn_demo_README.html">Mask R-CNN C++ Demo for TensorFlow* Object Detection API</a> - Inference of instance segmentation networks created with TensorFlow* Object Detection API.</li>
<li><a class="el" href="_demos_python_demos_multi_camera_multi_person_tracking_README.html">Multi-Camera Multi-Person Tracking Python* Demo</a> Demo application for multiple persons tracking on multiple cameras.</li>
<li><a class="el" href="_demos_multichannel_demo_README.html">Multi-Channel Face Detection C++ Demo</a> - Simultaneous Multi Camera Face Detection demo.</li>
<li><a class="el" href="_demos_object_detection_demo_faster_rcnn_README.html">Object Detection for Faster R-CNN C++ Demo</a> - Inference of object detection networks like Faster R-CNN (the demo supports only images as inputs).</li>
<li><a class="el" href="_demos_object_detection_demo_ssd_async_README.html">Object Detection for SSD C++ Demo</a> - Demo application for SSD-based Object Detection networks, new Async API performance showcase, and simple OpenCV interoperability (supports video and camera inputs).</li>
<li><a class="el" href="_demos_object_detection_demo_yolov3_async_README.html">Object Detection for YOLO V3 C++ Demo</a> - Demo application for YOLOV3-based Object Detection networks, new Async API performance showcase, and simple OpenCV interoperability (supports video and camera inputs).</li>
<li><a class="el" href="_demos_pedestrian_tracker_demo_README.html">Pedestrian Tracker C++ Demo</a> - Demo application for pedestrian tracking scenario.</li>
<li><a class="el" href="_demos_security_barrier_camera_demo_README.html">Security Barrier Camera C++ Demo</a> - Vehicle Detection followed by the Vehicle Attributes and License-Plate Recognition, supports images/video and camera inputs.</li>
<li><a class="el" href="_demos_smart_classroom_demo_README.html">Smart Classroom C++ Demo</a> - Face recognition and action detection demo for classroom environment.</li>
<li><a class="el" href="_demos_super_resolution_demo_README.html">Super Resolution C++ Demo</a> - Super Resolution demo (the demo supports only images as inputs). It enhances the resolution of the input image.</li>
<li><a class="el" href="_demos_text_detection_demo_README.html">Text Detection C++ Demo</a> - Text Detection demo. It detects and recognizes multi-oriented scene text on an input image and puts a bounding box around detected area.</li>
<li>Several C++ demos referenced above have simplified implementation in Python*, located in the <code>python_demos</code> directory.</li>
</ul>
<h2>Media Files Available for Demos</h2>
<p>To run the demo applications, you can use images and videos from the media files collection available at <a href="https://github.com/intel-iot-devkit/sample-videos">https://github.com/intel-iot-devkit/sample-videos</a>.</p>
<h2>Demos that Support Pre-Trained Models</h2>
<blockquote class="doxtable">
<p><b>NOTE:</b> Inference Engine HDDL and FPGA plugins are available in <a href="https://software.intel.com/en-us/openvino-toolkit">proprietary</a> distribution only. </p>
</blockquote>
<p>You can download the .<a class="el" href="_models_intel_index.html">pre-trained models</a> using the OpenVINO <a class="el" href="_tools_downloader_README.html">Model Downloader</a> or from <a href="https://download.01.org/opencv/">https://download.01.org/opencv/</a>. The table below shows the correlation between models, demos, and supported plugins. The plugins names are exactly as they are passed to the demos with <code>-d</code> option. The correlation between the plugins and supported devices see in the <a href="https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_Supported_Devices.html">Supported Devices</a> section.</p>
<blockquote class="doxtable">
<p><b>NOTE:</b> <b>MYRIAD</b> below stands for Intel® Movidius™ Neural Compute Stick, Intel® Neural Compute Stick 2, and Intel® Vision Accelerator Design with Intel® Movidius™ Vision Processing Units. </p>
</blockquote>
<table class="doxtable">
<tr>
<th>Model </th><th>Demos supported on the model </th><th>CPU </th><th>GPU </th><th>MYRIAD/HDDL </th><th>HETERO:FPGA,CPU  </th></tr>
<tr>
<td>action-recognition-0001-decoder </td><td><a class="el" href="_demos_python_demos_action_recognition_README.html">Action Recognition Demo</a> </td><td>Supported </td><td>Supported </td><td></td><td></td></tr>
<tr>
<td>action-recognition-0001-encoder </td><td><a class="el" href="_demos_python_demos_action_recognition_README.html">Action Recognition Demo</a> </td><td>Supported </td><td>Supported </td><td></td><td></td></tr>
<tr>
<td>driver-action-recognition-adas-0002-decoder </td><td><a class="el" href="_demos_python_demos_action_recognition_README.html">Action Recognition Demo</a> </td><td>Supported </td><td>Supported </td><td></td><td></td></tr>
<tr>
<td>driver-action-recognition-adas-0002-encoder </td><td><a class="el" href="_demos_python_demos_action_recognition_README.html">Action Recognition Demo</a> </td><td>Supported </td><td>Supported </td><td></td><td>Supported </td></tr>
<tr>
<td>person-attributes-recognition-crossroad-0230 </td><td><a class="el" href="_demos_crossroad_camera_demo_README.html">Crossroad Camera Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>person-reidentification-retail-0031 </td><td><a class="el" href="_demos_crossroad_camera_demo_README.html">Crossroad Camera Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>person-reidentification-retail-0076 </td><td><a class="el" href="_demos_crossroad_camera_demo_README.html">Crossroad Camera Demo</a><br />
<a class="el" href="_demos_python_demos_multi_camera_multi_person_tracking_README.html">Multi-Camera Multi-Person Tracking Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>person-reidentification-retail-0079 </td><td><a class="el" href="_demos_crossroad_camera_demo_README.html">Crossroad Camera Demo</a><br />
<a class="el" href="_demos_python_demos_multi_camera_multi_person_tracking_README.html">Multi-Camera Multi-Person Tracking Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>person-vehicle-bike-detection-crossroad-0078 </td><td><a class="el" href="_demos_crossroad_camera_demo_README.html">Crossroad Camera Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>human-pose-estimation-0001 </td><td><a class="el" href="_demos_human_pose_estimation_demo_README.html">Human Pose Estimation Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>image-retrieval-0001 </td><td><a class="el" href="_demos_python_demos_image_retrieval_demo_README.html">Image Retrieval Python* Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>semantic-segmentation-adas-0001 </td><td><a class="el" href="_demos_segmentation_demo_README.html">Image Segmentation Demo</a> </td><td>Supported </td><td>Supported </td><td></td><td>Supported </td></tr>
<tr>
<td>instance-segmentation-security-0010 </td><td><a class="el" href="_demos_python_demos_instance_segmentation_demo_README.html">Instance Segmentation Demo</a> </td><td>Supported </td><td></td><td></td><td>Supported </td></tr>
<tr>
<td>instance-segmentation-security-0050 </td><td><a class="el" href="_demos_python_demos_instance_segmentation_demo_README.html">Instance Segmentation Demo</a> </td><td>Supported </td><td></td><td></td><td>Supported </td></tr>
<tr>
<td>instance-segmentation-security-0083 </td><td><a class="el" href="_demos_python_demos_instance_segmentation_demo_README.html">Instance Segmentation Demo</a> </td><td>Supported </td><td></td><td></td><td>Supported </td></tr>
<tr>
<td>age-gender-recognition-retail-0013 </td><td><a class="el" href="_demos_interactive_face_detection_demo_README.html">Interactive Face Detection Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>emotions-recognition-retail-0003 </td><td><a class="el" href="_demos_interactive_face_detection_demo_README.html">Interactive Face Detection Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>face-detection-adas-0001 </td><td><a class="el" href="_demos_interactive_face_detection_demo_README.html">Interactive Face Detection Demo</a><br />
<a class="el" href="_demos_python_demos_face_recognition_demo_README.html">Interactive Face Recognition Python* Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>face-detection-adas-binary-0001 </td><td><a class="el" href="_demos_interactive_face_detection_demo_README.html">Interactive Face Detection Demo</a> </td><td>Supported </td><td>Supported </td><td></td><td></td></tr>
<tr>
<td>face-detection-retail-0004 </td><td><a class="el" href="_demos_interactive_face_detection_demo_README.html">Interactive Face Detection Demo</a><br />
<a class="el" href="_demos_python_demos_face_recognition_demo_README.html">Interactive Face Recognition Python* Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>facial-landmarks-35-adas-0002 </td><td><a class="el" href="_demos_interactive_face_detection_demo_README.html">Interactive Face Detection Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>head-pose-estimation-adas-0001 </td><td><a class="el" href="_demos_interactive_face_detection_demo_README.html">Interactive Face Detection Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>license-plate-recognition-barrier-0001 </td><td><a class="el" href="_demos_security_barrier_camera_demo_README.html">Security Barrier Camera Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>vehicle-attributes-recognition-barrier-0039 </td><td><a class="el" href="_demos_security_barrier_camera_demo_README.html">Security Barrier Camera Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>vehicle-license-plate-detection-barrier-0106 </td><td><a class="el" href="_demos_security_barrier_camera_demo_README.html">Security Barrier Camera Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>face-reidentification-retail-0095 </td><td><a class="el" href="_demos_smart_classroom_demo_README.html">Smart Classroom Demo</a><br />
<a class="el" href="_demos_python_demos_face_recognition_demo_README.html">Interactive Face Recognition Python* Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>landmarks-regression-retail-0009 </td><td><a class="el" href="_demos_smart_classroom_demo_README.html">Smart Classroom Demo</a><br />
<a class="el" href="_demos_python_demos_face_recognition_demo_README.html">Interactive Face Recognition Python* Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>person-detection-action-recognition-0005 </td><td><a class="el" href="_demos_smart_classroom_demo_README.html">Smart Classroom Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>person-detection-action-recognition-teacher-0002 </td><td><a class="el" href="_demos_smart_classroom_demo_README.html">Smart Classroom Demo</a> </td><td>Supported </td><td>Supported </td><td></td><td>Supported </td></tr>
<tr>
<td>single-image-super-resolution-1032 </td><td><a class="el" href="_demos_super_resolution_demo_README.html">Super Resolution Demo</a> </td><td>Supported </td><td>Supported </td><td></td><td>Supported </td></tr>
<tr>
<td>single-image-super-resolution-1033 </td><td><a class="el" href="_demos_super_resolution_demo_README.html">Super Resolution Demo</a> </td><td>Supported </td><td>Supported </td><td></td><td>Supported </td></tr>
<tr>
<td>text-detection-0003 </td><td><a class="el" href="_demos_text_detection_demo_README.html">Text Detection Demo</a> </td><td>Supported </td><td>Supported </td><td></td><td>Supported </td></tr>
<tr>
<td>text-detection-0004 </td><td><a class="el" href="_demos_text_detection_demo_README.html">Text Detection Demo</a> </td><td>Supported </td><td>Supported </td><td></td><td>Supported </td></tr>
<tr>
<td>text-recognition-0012 </td><td><a class="el" href="_demos_text_detection_demo_README.html">Text Detection Demo</a> </td><td>Supported </td><td>Supported </td><td></td><td></td></tr>
<tr>
<td>gaze-estimation-adas-0002 </td><td><a class="el" href="_demos_gaze_estimation_demo_README.html">Gaze Estimation Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>head-pose-estimation-adas-0001 </td><td><a class="el" href="_demos_gaze_estimation_demo_README.html">Gaze Estimation Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>facial-landmarks-35-adas-0002 </td><td><a class="el" href="_demos_gaze_estimation_demo_README.html">Gaze Estimation Demo</a> </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>pedestrian-and-vehicle-detector-adas-0001 </td><td>any demo that supports SSD*-based models, above </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>pedestrian-detection-adas-0002 </td><td>any demo that supports SSD*-based models, above </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>pedestrian-detection-adas-binary-0001 </td><td>any demo that supports SSD*-based models, above </td><td>Supported </td><td>Supported </td><td></td><td></td></tr>
<tr>
<td>person-detection-retail-0002 </td><td>any demo that supports SSD*-based models, above </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>person-detection-retail-0013 </td><td>any demo that supports SSD*-based models, above </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>road-segmentation-adas-0001 </td><td>any demo that supports SSD*-based models, above </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
<tr>
<td>vehicle-detection-adas-binary-0001 </td><td>any demo that supports SSD*-based models, above </td><td>Supported </td><td>Supported </td><td></td><td></td></tr>
<tr>
<td>vehicle-detection-adas-0002 </td><td>any demo that supports SSD*-based models, above </td><td>Supported </td><td>Supported </td><td>Supported </td><td>Supported </td></tr>
</table>
<p>Notice that the FPGA support comes through a <a href="https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_HETERO.html">heterogeneous execution</a>, for example, when the post-processing is happening on the CPU.</p>
<h2>Build the Demo Applications</h2>
<p>To be able to build demos you need to source <em><a class="el" href="namespaceInferenceEngine.html" title="Inference Engine API. ">InferenceEngine</a></em> and <em>OpenCV</em> environment from a binary package which is available as <a href="https://software.intel.com/en-us/openvino-toolkit">proprietary</a> distribution. Please run the following command before the demos build (assuming that the binary package was installed to <code>&lt;INSTALL_DIR&gt;</code>): </p><div class="fragment"><div class="line">source &lt;INSTALL_DIR&gt;/deployment_tools/bin/setupvars.sh</div></div><!-- fragment --><p> You can also build demos manually using Inference Engine binaries from the <a href="https://github.com/opencv/dldt/tree/master">dldt</a> repo. In this case please set <code>InferenceEngine_DIR</code> to a CMake folder you built the dldt project from, for example <code>&lt;dldt_repo&gt;/inference-engine/build</code>. Please also set the <code>OpenCV_DIR</code> variable pointing to the required OpenCV package. The same OpenCV version should be used both for the inference engine and demos build. Please refer to the Inference Engine <a href="https://github.com/opencv/dldt/tree/master/inference-engine/README.md">build instructions</a> for details. Please also add path to built Inference Engine libraries to <code>LD_LIBRARY_PATH</code> (Linux*) or <code>PATH</code> (Windows*) variable before building the demos.</p>
<h3><a class="anchor" id="build_demos_linux"></a>Build the Demo Applications on Linux*</h3>
<p>The officially supported Linux* build environment is the following:</p>
<ul>
<li>Ubuntu* 16.04 LTS 64-bit or CentOS* 7.4 64-bit</li>
<li>GCC* 5.4.0 (for Ubuntu* 16.04) or GCC* 4.8.5 (for CentOS* 7.4)</li>
<li>CMake* version 2.8 or higher.</li>
</ul>
<p>To build the demo applications for Linux, go to the directory with the <code>build_demos.sh</code> script and run it: </p><div class="fragment"><div class="line">build_demos.sh</div></div><!-- fragment --><p>You can also build the demo applications manually:</p>
<ol type="1">
<li>Navigate to a directory that you have write access to and create a demos build directory. This example uses a directory named <code>build</code>: <div class="fragment"><div class="line">mkdir build</div></div><!-- fragment --></li>
<li>Go to the created directory: <div class="fragment"><div class="line">cd build</div></div><!-- fragment --></li>
<li>Run CMake to generate the Make files for release or debug configuration:<ul>
<li>For release configuration: <div class="fragment"><div class="line">cmake -DCMAKE_BUILD_TYPE=Release &lt;open_model_zoo&gt;/demos</div></div><!-- fragment --></li>
<li>For debug configuration: <div class="fragment"><div class="line">cmake -DCMAKE_BUILD_TYPE=Debug &lt;open_model_zoo&gt;/demos</div></div><!-- fragment --></li>
</ul>
</li>
<li>Run <code>make</code> to build the demos: <div class="fragment"><div class="line">make</div></div><!-- fragment --></li>
</ol>
<p>For the release configuration, the demo application binaries are in <code>&lt;path_to_build_directory&gt;/intel64/Release/</code>; for the debug configuration — in <code>&lt;path_to_build_directory&gt;/intel64/Debug/</code>.</p>
<h3><a class="anchor" id="build_demos_windows"></a>Build the Demos Applications on Microsoft Windows* OS</h3>
<p>The recommended Windows* build environment is the following:</p><ul>
<li>Microsoft Windows* 10</li>
<li>Microsoft Visual Studio* 2015, 2017, or 2019</li>
<li>CMake* version 2.8 or higher</li>
</ul>
<blockquote class="doxtable">
<p><b>NOTE</b>: If you want to use Microsoft Visual Studio 2019, you are required to install CMake 3.14. </p>
</blockquote>
<p>To build the demo applications for Windows, go to the directory with the <code>build_demos_msvc.bat</code> batch file and run it: </p><div class="fragment"><div class="line">build_demos_msvc.bat</div></div><!-- fragment --><p>By default, the script automatically detects the highest Microsoft Visual Studio version installed on the machine and uses it to create and build a solution for a demo code. Optionally, you can also specify the preffered Microsoft Visual Studio version to be used by the script. Supported versions are: <code>VS2015</code>, <code>VS2017</code>, <code>VS2019</code>. For example, to build the demos using the Microsoft Visual Studio 2017, use the following command: </p><div class="fragment"><div class="line">build_demos_msvc.bat VS2017</div></div><!-- fragment --><p>The demo applications binaries are in the <code>C:\Users\&lt;username&gt;\Documents\Intel\OpenVINO\omz_demos_build_build\intel64\Release</code> directory.</p>
<p>You can also build a generated solution by yourself, for example, if you want to build binaries in Debug configuration. Run the appropriate version of the Microsoft Visual Studio and open the generated solution file from the <code>C:\Users\&lt;username&gt;\Documents\Intel\OpenVINO\omz_demos_build\Demos.sln</code> directory.</p>
<h2>Get Ready for Running the Demo Applications</h2>
<h3>Get Ready for Running the Demo Applications on Linux*</h3>
<p>Before running compiled binary files, make sure your application can find the Inference Engine and OpenCV libraries. If you use a <a href="https://software.intel.com/en-us/openvino-toolkit">proprietary</a> distribution to build demos, run the <code>setupvars</code> script to set all necessary environment variables: </p><div class="fragment"><div class="line">source &lt;INSTALL_DIR&gt;/bin/setupvars.sh</div></div><!-- fragment --><p> If you use your own Inference Engine and OpenCV binaries to build the demos please make sure you have added them to the <code>LD_LIBRARY_PATH</code> environment variable.</p>
<p>**(Optional)**: The OpenVINO environment variables are removed when you close the shell. As an option, you can permanently set the environment variables as follows:</p>
<ol type="1">
<li>Open the <code>.bashrc</code> file in <code>&lt;user_home_directory&gt;</code>: <div class="fragment"><div class="line">vi &lt;user_home_directory&gt;/.bashrc</div></div><!-- fragment --></li>
<li>Add this line to the end of the file: <div class="fragment"><div class="line">source &lt;INSTALL_DIR&gt;/bin/setupvars.sh</div></div><!-- fragment --></li>
<li>Save and close the file: press the <b>Esc</b> key, type <code>:wq</code> and press the <b>Enter</b> key.</li>
<li>To test your change, open a new terminal. You will see <code>[setupvars.sh] OpenVINO environment initialized</code>.</li>
</ol>
<p>You are ready to run the demo applications. To learn about how to run a particular demo, read the demo documentation by clicking the demo name in the demo list above.</p>
<h3>Get Ready for Running the Demo Applications on Windows*</h3>
<p>Before running compiled binary files, make sure your application can find the Inference Engine and OpenCV libraries. If you use a <a href="https://software.intel.com/en-us/openvino-toolkit">proprietary</a> distribution to build demos, run the <code>setupvars</code> script to set all necessary environment variables: </p><div class="fragment"><div class="line">&lt;INSTALL_DIR&gt;\bin\setupvars.bat</div></div><!-- fragment --><p> If you use your own Inference Engine and OpenCV binaries to build the demos please make sure you have added to the <code>PATH</code> environment variable.</p>
<p>To debug or run the demos on Windows in Microsoft Visual Studio, make sure you have properly configured <b>Debugging</b> environment settings for the <b>Debug</b> and <b>Release</b> configurations. Set correct paths to the OpenCV libraries, and debug and release versions of the Inference Engine libraries. For example, for the <b>Debug</b> configuration, go to the project's <b>Configuration Properties</b> to the <b>Debugging</b> category and set the <code>PATH</code> variable in the <b>Environment</b> field to the following:</p>
<div class="fragment"><div class="line">PATH=&lt;INSTALL_DIR&gt;\deployment_tools\inference_engine\bin\intel64\Debug;&lt;INSTALL_DIR&gt;\opencv\bin;%PATH%</div></div><!-- fragment --><p> where <code>&lt;INSTALL_DIR&gt;</code> is the directory in which the OpenVINO toolkit is installed.</p>
<p>You are ready to run the demo applications. To learn about how to run a particular demo, read the demo documentation by clicking the demo name in the demos list above.</p>
<h2>See Also</h2>
<ul>
<li><a href="https://docs.openvinotoolkit.org/latest/_docs_IE_DG_Introduction.html">Introduction to Intel's Deep Learning Inference Engine</a> </li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>