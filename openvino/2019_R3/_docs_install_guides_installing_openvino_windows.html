<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Install Intel® Distribution of OpenVINO™ toolkit for Windows* 10 - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Install Intel® Distribution of OpenVINO™ toolkit for Windows* 10 </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><blockquote class="doxtable">
<p><b>NOTES</b>:</p><ul>
<li><a href="https://software.intel.com/en-us/system-studio">Intel® System Studio</a> is an all-in-one, cross-platform tool suite, purpose-built to simplify system bring-up and improve system and IoT device application performance on Intel® platforms. If you are using the Intel® Distribution of OpenVINO™ with Intel® System Studio, go to <a href="https://software.intel.com/en-us/articles/get-started-with-openvino-and-intel-system-studio-2019">Get Started with Intel® System Studio</a>.</li>
<li>Intel® Distribution of OpenVINO™ toolkit was formerly known as the Intel® Computer Vision SDK.</li>
<li>This guide applies to Microsoft Windows* 10 64-bit. For Linux* OS information and instructions, see the <a class="el" href="_docs_install_guides_installing_openvino_linux.html">Installation Guide for Linux</a>. </li>
</ul>
</blockquote>
<h2>Introduction</h2>
<blockquote class="doxtable">
<p><b>IMPORTANT</b>:</p><ul>
<li>All steps in this guide are required, unless otherwise stated.<br />
</li>
<li>In addition to the download package, you must install dependencies and complete configuration steps. </li>
</ul>
</blockquote>
<p>Your installation is complete when these are all completed:</p>
<ol type="1">
<li>Install the <a href="#Install-Core-Components">Intel® Distribution of OpenVINO™ toolkit core components</a></li>
<li>Install the dependencies:<ul>
<li><a href="http://visualstudio.microsoft.com/downloads/">Microsoft Visual Studio* with C++ <b>2019, 2017, or 2015</b> with MSBuild</a></li>
<li><a href="https://cmake.org/download/">CMake <b>3.4 or higher</b> 64-bit</a> <blockquote class="doxtable">
<p><b>NOTE</b>: If you want to use Microsoft Visual Studio 2019, you are required to install CMake 3.14. </p>
</blockquote>
</li>
<li><a href="https://www.python.org/downloads/release/python-365/">Python <b>3.6.5</b> 64-bit</a> <blockquote class="doxtable">
<p><b>IMPORTANT</b>: As part of this installation, make sure you click the option to add the application to your <code>PATH</code> environment variable. </p>
</blockquote>
</li>
</ul>
</li>
<li><a href="#set-the-environment-variables">Set Environment Variables</a></li>
<li><a href="#Configure_MO">Configure the Model Optimizer</a></li>
<li>Run two <a href="#Using-Demo-Scripts">Verification Scripts to Verify Installation</a></li>
<li>Optional: <ul>
<li><a href="#Install-GPU">Install the Intel® Graphics Driver for Windows*</a></li>
<li><a href="#hddl-myriad">Install the drivers and software for the Intel® Vision Accelerator Design with Intel® Movidius™ VPUs</a></li>
<li><a href="#Update-Path">Update Windows* environment variables</a></li>
</ul>
</li>
</ol>
<h3>About the Intel® Distribution of OpenVINO™ toolkit</h3>
<p>The Intel® Distribution of OpenVINO™ toolkit speeds the deployment of applications and solutions that emulate human vision. Based on Convolutional Neural Networks (CNN), the toolkit extends computer vision (CV) workloads across Intel® hardware to maximize performance.</p>
<p>The Intel® Distribution of OpenVINO™ toolkit includes the Intel® Deep Learning Deployment Toolkit (Intel® DLDT). For more information, see the online <a href="https://software.intel.com/en-us/OpenVINO-toolkit">Intel® Distribution of OpenVINO™ toolkit Overview</a> page.</p>
<p>The Intel® Distribution of OpenVINO™ toolkit for Windows* 10 OS:</p>
<ul>
<li>Enables CNN-based deep learning inference on the edge</li>
<li>Supports heterogeneous execution across Intel® CPU, Intel® Processor Graphics (GPU), Intel® Movidius™ Neural Compute Stick, Intel® Neural Compute Stick 2, and Intel® Vision Accelerator Design with Intel® Movidius™ VPUs</li>
<li>Speeds time-to-market through an easy-to-use library of computer vision functions and pre-optimized kernels</li>
<li>Includes optimized calls for computer vision standards including OpenCV*, OpenCL™, and OpenVX*</li>
</ul>
<h4><a class="anchor" id="InstallPackageContents"></a>Included in the Installation Package</h4>
<p>The following components are installed by default:</p>
<table class="doxtable">
<tr>
<th align="left">Component </th><th align="left">Description  </th></tr>
<tr>
<td align="left"><a class="el" href="_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html">Model Optimizer</a> </td><td align="left">This tool imports, converts, and optimizes models that were trained in popular frameworks to a format usable by Intel tools, especially the Inference Engine.<br />
<b>NOTE</b>: Popular frameworks include such frameworks as Caffe*, TensorFlow*, MXNet*, and ONNX*. </td></tr>
<tr>
<td align="left"><a class="el" href="_docs_IE_DG_inference_engine_intro.html">Inference Engine</a> </td><td align="left">This is the engine that runs the deep learning model. It includes a set of libraries for an easy inference integration into your applications. </td></tr>
<tr>
<td align="left"><a href="https://docs.opencv.org/master/">OpenCV*</a> </td><td align="left">OpenCV* community version compiled for Intel® hardware </td></tr>
<tr>
<td align="left">OpenVX* </td><td align="left">Intel's implementation of OpenVX* optimized for running on an Intel CPU, GPU, or IPU (Image processing unit). </td></tr>
<tr>
<td align="left"><a class="el" href="_docs_IE_DG_Samples_Overview.html">Inference Engine Samples</a> </td><td align="left">A set of simple console applications demonstrating how to use Intel's Deep Learning Inference Engine in your applications. </td></tr>
<tr>
<td align="left"><a class="el" href="_demos_README.html">Demos</a> </td><td align="left">A set of console applications that demonstrate how you can use the Inference Engine in your applications to solve specific use-cases </td></tr>
<tr>
<td align="left"><a class="el" href="_docs_IE_DG_Tools_Overview.html">Additional Tools</a> </td><td align="left">A set of tools to work with your models </td></tr>
<tr>
<td align="left"><a class="el" href="_models_intel_index.html">Documentation for Pre-Trained Models </a> </td><td align="left">Documentation for the pre-trained models available in the <a href="https://github.com/opencv/open_model_zoo">Open Model Zoo repo</a> </td></tr>
</table>
<h3>System Requirements</h3>
<p>Only the Intel® CPU, Intel® Processor Graphics, Intel® Movidius™ Neural Compute Stick, Intel® Neural Compute Stick 2 and Intel® Vision Accelerator Design with Intel® Movidius™ VPUs options are supported for the Windows* installation. Linux* is required to use the FPGA.</p>
<p><b>Hardware</b></p>
<ul>
<li>6th-10th Generation Intel® Core™ processors</li>
<li>Intel® Xeon® v5 family</li>
<li>Intel® Xeon® v6 family</li>
<li>Intel® Movidius™ Neural Compute Stick</li>
<li>Intel® Neural Compute Stick 2</li>
<li>Intel® Vision Accelerator Design with Intel® Movidius™ VPUs</li>
</ul>
<p><b>Processor Notes:</b></p>
<ul>
<li>Processor graphics are not included in all processors. See <a href="https://ark.intel.com/#@Processors">Processors specifications</a> for information about your processor.</li>
<li>A chipset that supports processor graphics is required if you're using an Intel Xeon processor. See <a href="https://ark.intel.com/#@Chipsets">Chipset specifications</a> for information about your chipset.</li>
</ul>
<p><b>Operating System</b></p>
<ul>
<li>Microsoft Windows* 10 64-bit</li>
</ul>
<p><b>Software</b></p><ul>
<li><a href="http://visualstudio.microsoft.com/downloads/">Microsoft Visual Studio* with C++ <b>2019, 2017, or 2015</b> with MSBuild</a></li>
<li><a href="https://cmake.org/download/">CMake <b>3.4 or higher</b> 64-bit</a> <blockquote class="doxtable">
<p><b>NOTE</b>: If you want to use Microsoft Visual Studio 2019, you are required to install CMake 3.14. </p>
</blockquote>
</li>
<li><a href="https://www.python.org/downloads/release/python-365/">Python <b>3.6.5</b> 64-bit</a></li>
</ul>
<h2>Installation Steps</h2>
<h3><a class="anchor" id="Install-Core-Components"></a>Install the Intel® Distribution of OpenVINO™ toolkit Core Components</h3>
<ol type="1">
<li>If you have not downloaded the Intel® Distribution of OpenVINO™ toolkit, <a href="http://software.intel.com/en-us/openvino-toolkit/choose-download/free-download-windows">download the latest version</a>. By default, the file is saved to the <code>Downloads</code> directory as <code>w_openvino_toolkit_p_&lt;version&gt;.exe</code>.</li>
<li>Go to the <code>Downloads</code> folder.</li>
<li><p class="startli">Double-click <code>w_openvino_toolkit_p_&lt;version&gt;.exe</code>. A window opens to let you choose your installation directory and components. The default installation directory is <code>C:\Program Files (x86)\IntelSWTools\openvino_&lt;version&gt;</code>, for simplicity, a shortcut to the latest installation is also created: <code>C:\Program Files (x86)\IntelSWTools\openvino</code>. If you choose a different installation directory, the installer will create the directory for you:</p>
<div class="image">
<img src="openvino-install-windows-01.png" alt="openvino-install-windows-01.png"/>
</div>
</li>
<li>Click <b>Next</b>.</li>
<li>You are asked if you want to provide consent to gather information. Choose the option of your choice. Click <b>Next</b>.</li>
<li><p class="startli">If you are missing external dependencies, you will see a warning screen. Write down the dependencies you are missing. <b>You need to take no other action at this time</b>. After installing the Intel® Distribution of OpenVINO™ toolkit core components, install the missing dependencies. The screen example below indicates you are missing two dependencies:</p>
<div class="image">
<img src="openvino-install-windows-02.png" alt="openvino-install-windows-02.png"/>
</div>
</li>
<li>Click <b>Next</b>.</li>
<li><p class="startli">When the first part of installation is complete, the final screen informs you that the core components have been installed and additional steps still required:</p>
<div class="image">
<img src="openvino-install-windows-03.png" alt="openvino-install-windows-03.png"/>
</div>
</li>
<li>Click <b>Finish</b> to close the installation wizard. A new browser window opens to the next section of the installation guide to set the environment variables. You are in the same document. The new window opens in case you ran the installation without first opening this installation guide.</li>
<li>If the installation indicated you must install dependencies, install them first. If there are no missing dependencies, you can go ahead and <a href="#set-the-environment-variables">set the environment variables</a>.</li>
</ol>
<h3>Set the Environment Variables <a class="anchor" id="set-the-environment-variables"></a></h3>
<blockquote class="doxtable">
<p><b>NOTE</b>: If you installed the Intel® Distribution of OpenVINO™ to the non-default install directory, replace <code>C:\Program Files (x86)\IntelSWTools</code> with the directory in which you installed the software. </p>
</blockquote>
<p>You must update several environment variables before you can compile and run OpenVINO™ applications. Open the Command Prompt, and run the <code>setupvars.bat</code> batch file to temporarily set your environment variables: </p><div class="fragment"><div class="line">cd C:\Program Files (x86)\IntelSWTools\openvino\bin\</div></div><!-- fragment --><div class="fragment"><div class="line">setupvars.bat</div></div><!-- fragment --><p><b>(Optional)</b>: OpenVINO toolkit environment variables are removed when you close the Command Prompt window. As an option, you can permanently set the environment variables manually.</p>
<p>The environment variables are set. Continue to the next section to configure the Model Optimizer.</p>
<h2>Configure the Model Optimizer <a class="anchor" id="Configure_MO"></a></h2>
<blockquote class="doxtable">
<p><b>IMPORTANT</b>: These steps are required. You must configure the Model Optimizer for at least one framework. The Model Optimizer will fail if you do not complete the steps in this section. </p>
</blockquote>
<blockquote class="doxtable">
<p><b>NOTE</b>: If you see an error indicating Python is not installed when you know you installed it, your computer might not be able to find the program. For the instructions to add Python to your system environment variables, see <a href="#Update-Path">Update Your Windows Environment Variables</a>. </p>
</blockquote>
<p>The Model Optimizer is a key component of the Intel® Distribution of OpenVINO™ toolkit. You cannot do inference on your trained model without running the model through the Model Optimizer. When you run a pre-trained model through the Model Optimizer, your output is an Intermediate Representation (IR) of the network. The IR is a pair of files that describe the whole model:</p>
<ul>
<li><code>.xml</code>: Describes the network topology</li>
<li><code>.bin</code>: Contains the weights and biases binary data</li>
</ul>
<p>The Inference Engine reads, loads, and infers the IR files, using a common API across the CPU, GPU, or VPU hardware.</p>
<p>The Model Optimizer is a Python*-based command line tool (<code>mo.py</code>), which is located in <code>C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer</code>. Use this tool on models trained with popular deep learning frameworks such as Caffe*, TensorFlow*, MXNet*, and ONNX* to convert them to an optimized IR format that the Inference Engine can use.</p>
<p>This section explains how to use scripts to configure the Model Optimizer either for all of the supported frameworks at the same time or for individual frameworks. If you want to manually configure the Model Optimizer instead of using scripts, see the <b>Using Manual Configuration Process</b> section on the <a class="el" href="_docs_MO_DG_prepare_model_Config_Model_Optimizer.html">Configuring the Model Optimizer</a> page.</p>
<p>For more information about the Model Optimizer, see the <a class="el" href="_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html">Model Optimizer Developer Guide</a>.</p>
<h3>Model Optimizer Configuration Steps</h3>
<p>You can configure the Model Optimizer either for all supported frameworks at once or for one framework at a time. Choose the option that best suits your needs. If you see error messages, make sure you installed all dependencies.</p>
<blockquote class="doxtable">
<p><b>IMPORTANT</b>: The Internet access is required to execute the following steps successfully. If you have access to the Internet through the proxy server only, please make sure that it is configured in your environment. </p>
</blockquote>
<blockquote class="doxtable">
<p><b>NOTE</b>: In the steps below:</p><ul>
<li>If you you want to use the Model Optimizer from another installed versions of Intel® Distribution of OpenVINO™ toolkit installed, replace <code>openvino</code> with <code>openvino_&lt;version&gt;</code>.</li>
<li>If you installed the Intel® Distribution of OpenVINO™ toolkit to the non-default installation directory, replace <code>C:\Program Files (x86)\IntelSWTools</code> with the directory where you installed the software. </li>
</ul>
</blockquote>
<p>These steps use a command prompt to make sure you see error messages.</p>
<h4>Option 1: Configure the Model Optimizer for all supported frameworks at the same time:</h4>
<ol type="1">
<li><p class="startli">Open a command prompt. To do so, type <code>cmd</code> in your <b>Search Windows</b> box and then press <b>Enter</b>. Type commands in the opened window:</p>
<div class="image">
<img src="command_prompt.PNG" alt="command_prompt.PNG"/>
</div>
</li>
<li>Go to the Model Optimizer prerequisites directory.<br />
 <div class="fragment"><div class="line">cd C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer\install_prerequisites</div></div><!-- fragment --></li>
<li>Run the following batch file to configure the Model Optimizer for Caffe*, TensorFlow*, MXNet*, Kaldi*, and ONNX*:<br />
 <div class="fragment"><div class="line">install_prerequisites.bat</div></div><!-- fragment --></li>
</ol>
<h4>Option 2: Configure the Model Optimizer for each framework separately:</h4>
<ol type="1">
<li>Go to the Model Optimizer prerequisites directory:<br />
 <div class="fragment"><div class="line">cd C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer\install_prerequisites</div></div><!-- fragment --></li>
<li>Run the batch file for the framework you will use with the Model Optimizer. You can use more than one:<ul>
<li>For <b>Caffe</b>:<br />
 <div class="fragment"><div class="line">install_prerequisites_caffe.bat</div></div><!-- fragment --></li>
<li>For <b>TensorFlow</b>:<br />
 <div class="fragment"><div class="line">install_prerequisites_tf.bat</div></div><!-- fragment --></li>
<li>For <b>MXNet</b>:<br />
 <div class="fragment"><div class="line">install_prerequisites_mxnet.bat</div></div><!-- fragment --></li>
<li>For <b>ONNX</b>: <div class="fragment"><div class="line">install_prerequisites_onnx.bat</div></div><!-- fragment --></li>
<li>For <b>Kaldi</b>: <div class="fragment"><div class="line">install_prerequisites_kaldi.bat</div></div><!-- fragment --></li>
</ul>
</li>
</ol>
<p>The Model Optimizer is configured for one or more frameworks. Success is indicated by a screen similar to this:</p>
<div class="image">
<img src="Configure-MO.PNG" alt="Configure-MO.PNG"/>
</div>
<p>You are ready to use two short demos to see the results of running the Intel Distribution of OpenVINO toolkit and to verify your installation was successful. The demo scripts are required since they perform additional configuration steps. Continue to the next section.</p>
<p>If you want to use a GPU or VPU, or update your Windows* environment variables, read through the <a href="#optional-steps">Optional Steps</a> section.</p>
<h2><a class="anchor" id="Using-Demo-Scripts"></a>Use Verification Scripts to Verify Your Installation</h2>
<blockquote class="doxtable">
<p><b>IMPORTANT</b>: This section is required. In addition to confirming your installation was successful, demo scripts perform other steps, such as setting up your computer to use the Inference Engine samples. </p>
</blockquote>
<blockquote class="doxtable">
<p><b>NOTE</b>: The paths in this section assume you used the default installation directory. If you used a directory other than <code>C:\Program Files (x86)\IntelSWTools</code>, update the directory with the location where you installed the software. </p>
</blockquote>
<p>To verify the installation and compile two samples, run the verification applications provided with the product on the CPU:</p>
<ol type="1">
<li>Open a command prompt window.</li>
<li>Go to the Inference Engine demo directory:<br />
 <div class="fragment"><div class="line">cd C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\</div></div><!-- fragment --></li>
<li>Run the verification scripts by following the instructions in the next section.</li>
</ol>
<h3><a class="anchor" id="run-the-image-classification-verification-script"></a>Run the Image Classification Verification Script</h3>
<p>To run the script, start the <code>demo_squeezenet_download_convert_run.bat</code> file: </p><div class="fragment"><div class="line">demo_squeezenet_download_convert_run.bat</div></div><!-- fragment --><p>This script downloads a SqueezeNet model, uses the Model Optimizer to convert the model to the <code>.&zwj;bin</code> and <code>.&zwj;xml</code> Intermediate Representation (IR) files. The Inference Engine requires this model conversion so it can use the IR as input and achieve optimum performance on Intel hardware.<br />
 This verification script builds the <a class="el" href="_inference_engine_samples_classification_sample_async_README.html">Image Classification Sample Async</a> application and run it with the <code>car.png</code> image in the demo directory. For a brief description of the Intermediate Representation, see <a href="#Configure_MO">Configuring the Model Optimizer</a>.</p>
<p>When the verification script completes, you will have the label and confidence for the top-10 categories: </p><div class="image">
<img src="image_classification_script_output_win.png" alt="image_classification_script_output_win.png"/>
</div>
<p>This demo is complete. Leave the console open and continue to the next section to run the Inference Pipeline demo.</p>
<h3>Run the Inference Pipeline Verification Script</h3>
<p>To run the script, start the <code>demo_security_barrier_camera.bat</code> file while still in the console: </p><div class="fragment"><div class="line">demo_security_barrier_camera.bat</div></div><!-- fragment --><p>This script downloads three pre-trained model IRs, builds the <a class="el" href="_demos_security_barrier_camera_demo_README.html">Security Barrier Camera Demo</a> application, and runs it with the downloaded models and the <code>car_1.bmp</code> image from the <code>demo</code> directory to show an inference pipeline. The verification script uses vehicle recognition in which vehicle attributes build on each other to narrow in on a specific attribute.</p>
<p>First, an object is identified as a vehicle. This identification is used as input to the next model, which identifies specific vehicle attributes, including the license plate. Finally, the attributes identified as the license plate are used as input to the third model, which recognizes specific characters in the license plate.</p>
<p>When the demo completes, you have two windows open:</p>
<ul>
<li>A console window that displays information about the tasks performed by the demo</li>
<li>An image viewer window that displays a resulting frame with detections rendered as bounding boxes, similar to the following:</li>
</ul>
<div class="image">
<img src="inference_pipeline_script_win.png" alt="inference_pipeline_script_win.png"/>
</div>
<p>Close the image viewer window to end the demo.</p>
<p>To learn more about the verification scripts, see <code>README.txt</code> in <code>C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo</code>.</p>
<p>For detailed description of the OpenVINO™ pre-trained object detection and object recognition models, see the <a class="el" href="_models_intel_index.html">Overview of OpenVINO™ toolkit Pre-Trained Models</a> page.</p>
<p>In this section, you saw a preview of the Intel® Distribution of OpenVINO™ toolkit capabilities.</p>
<p>Congratulations. You have completed all the required installation, configuration, and build steps to work with your trained models using CPU.</p>
<p>If you want to use Intel® Processor graphics (GPU), Intel® Movidius™ Neural Compute Stick, Intel® Neural Compute Stick 2 or Intel® Vision Accelerator Design with Intel® Movidius™ (VPU), or add CMake* and Python* to your Windows* environment variables, read through the next section for additional steps.</p>
<p>If you want to continue and run the Image Classification Sample Application on one of the supported hardware device, see the <a href="#run-the-image-classification-sample-application">Run the Image Classification Sample Application</a> section.</p>
<h2><a class="anchor" id="optional-steps"></a>Optional Steps</h2>
<p>Use the optional steps below if you want to:</p><ul>
<li>Infer models on <a href="#Install-GPU">Intel® Processor Graphics</a></li>
<li>Infer models on <a href="#usb-myriad">Intel® Vision Accelerator Design with Intel® Movidius™ VPUs</a></li>
<li><a href="#Update-Path">Add CMake* or Python* to your Windows* environment variables</a>.</li>
</ul>
<h3><a class="anchor" id="Install-GPU"></a>Optional: Additional Installation Steps for Intel® Processor Graphics (GPU)</h3>
<blockquote class="doxtable">
<p><b>NOTE</b>: These steps are required only if you want to use a GPU. </p>
</blockquote>
<p>If your applications offload computation to Intel® Integrated Graphics, you must have the Intel Graphics Driver for Windows version 15.65 or higher. To see if you have this driver installed:</p>
<ol type="1">
<li>Type <b>device manager</b> in your <b>Search Windows</b> box. The <b>Device Manager</b> opens.</li>
<li><p class="startli">Click the drop-down arrow to view the <b>Display adapters</b>. You see the adapter that is installed in your computer:</p>
<div class="image">
<img src="DeviceManager.PNG" alt="DeviceManager.PNG"/>
</div>
</li>
<li>Right-click the adapter name and select <b>Properties</b>.</li>
<li><p class="startli">Click the <b>Driver</b> tab to see the driver version. Make sure the version number is 15.65 or higher.</p>
<div class="image">
<img src="DeviceDriverVersion.PNG" alt="DeviceDriverVersion.PNG"/>
</div>
</li>
<li>If your device driver version is lower than 15.65, <a href="http://downloadcenter.intel.com/product/80939/Graphics-Drivers">download and install a higher version</a>.</li>
</ol>
<p>You are done updating your device driver and are ready to use your GPU.</p>
<h3><a class="anchor" id="hddl-myriad"></a> Optional: Additional Installation Steps for the Intel® Vision Accelerator Design with Intel® Movidius™ VPUs</h3>
<blockquote class="doxtable">
<p><b>NOTE</b>: These steps are required only if you want to use Intel® Vision Accelerator Design with Intel® Movidius™ VPUs. </p>
</blockquote>
<p>To perform inference on Intel® Vision Accelerator Design with Intel® Movidius™ VPUs, the following additional installation steps are required:</p>
<ol type="1">
<li>Install the Movidius™ VSC driver:<ol type="a">
<li>Go to the <code>&lt;INSTALL_DIR&gt;\deployment_tools\inference-engine\external\MovidiusDriver</code> directory, where <code>&lt;INSTALL_DIR&gt;</code> is the directory in which the Intel Distribution of OpenVINO toolkit is installed.</li>
<li>Right click on the <code>Movidius_VSC_Device.inf</code> file and choose <b>Install</b> from the pop up menu.</li>
</ol>
</li>
<li>If your Intel® Vision Accelerator Design with Intel® Movidius™ VPUs card requires SMBUS connection to PCIe slot (Raw video data card with HW version Fab-B and before), install the SMBUS driver:<ol type="a">
<li>Go to the <code>&lt;INSTALL_DIR&gt;\deployment_tools\inference-engine\external\hddl\SMBusDriver</code> directory, where <code>&lt;INSTALL_DIR&gt;</code> is the directory in which the Intel Distribution of OpenVINO toolkit is installed.</li>
<li>Right click on the <code>hddlsmbus.inf</code> file and choose <b>Install</b> from the pop up menu.</li>
</ol>
</li>
<li>Download and install <a href="https://www.microsoft.com/en-us/download/details.aspx?id=48145">Visual C++ Redistributable for Visual Studio 2015</a></li>
</ol>
<p>You are done installing your device driver and are ready to use your Intel® Vision Accelerator Design with Intel® Movidius™ VPUs.</p>
<p>See also:</p>
<ul>
<li>For advanced configuration steps for your IEI Mustang-V100-MX8 accelerator, see <a class="el" href="_docs_install_guides_movidius_setup_guide.html">Intel® Movidius™ VPUs Setup Guide for Use with Intel® Distribution of OpenVINO™ toolkit</a>.</li>
<li>After you've configurated your Intel® Vision Accelerator Design with Intel® Movidius™ VPUs, see <a class="el" href="_docs_install_guides_movidius_programming_guide.html">Intel® Movidius™ VPUs Programming Guide for Use with Intel® Distribution of OpenVINO™ toolkit</a> to learn how to distribute a model across all 8 VPUs to maximize performance.</li>
</ul>
<p>After configuration is done, you are ready to run the verification scripts with the HDDL Plugin for your Intel® Vision Accelerator Design with Intel® Movidius™ VPUs.</p>
<ol type="1">
<li>Open a command prompt window.</li>
<li>Go to the Inference Engine demo directory: <div class="fragment"><div class="line">cd C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\</div></div><!-- fragment --></li>
<li>Run the Image Classification verification script. If you have access to the Internet through the proxy server only, please make sure that it is configured in your environment. <div class="fragment"><div class="line">demo_squeezenet_download_convert_run.bat -d HDDL</div></div><!-- fragment --></li>
<li>Run the Inference Pipeline verification script: <div class="fragment"><div class="line">demo_security_barrier_camera.bat -d HDDL</div></div><!-- fragment --></li>
</ol>
<h3><a class="anchor" id="Update-Path"></a>Optional: Update Your Windows Environment Variables</h3>
<blockquote class="doxtable">
<p><b>NOTE</b>: These steps are only required under special circumstances, such as if you forgot to check the box during the CMake* or Python* installation to add the application to your Windows <code>PATH</code> environment variable. </p>
</blockquote>
<p>Use these steps to update your Windows <code>PATH</code> if a command you execute returns an error message stating that an application cannot be found. This might happen if you do not add CMake or Python to your <code>PATH</code> environment variable during the installation.</p>
<ol type="1">
<li>In your <b>Search Windows</b> box, type <b>Edit the system environment variables</b> and press <b>Enter</b>. A window similar to the following displays: <div class="image">
<img src="System_Properties.PNG" alt="System_Properties.PNG"/>
</div>
</li>
<li>At the bottom of the screen, click <b>Environment Variables</b>.</li>
<li>Under <b>System variables</b>, click <b>Path</b> and then <b>Edit</b>: <div class="image">
<img src="Environment_Variables-select_Path.PNG" alt="Environment_Variables-select_Path.PNG"/>
</div>
</li>
<li>In the opened window, click <b>Browse</b>. A browse window opens: <div class="image">
<img src="Add_Environment_Variable.PNG" alt="Add_Environment_Variable.PNG"/>
</div>
</li>
<li>If you need to add CMake to the <code>PATH</code>, browse to the directory in which you installed CMake. The default directory is <code>C:\Program Files\CMake</code>.</li>
<li>If you need to add Python to the <code>PATH</code>, browse to the directory in which you installed Python. The default directory is <code>C:\Users\&lt;USER_ID&gt;\AppData\Local\Programs\Python\Python36\Python</code>.</li>
<li>Click <b>OK</b> repeatedly to close each screen.</li>
</ol>
<p>Your <code>PATH</code> environment variable is updated.</p>
<h2><a class="anchor" id="run-the-image-classification-sample-application"></a>Run the Image Classification Sample Application</h2>
<p>In this section you will run the Image Classification Sample Application with a Squeezenet1.1 Caffe* model on three types of Intel® hardware: CPU, GPU and VPU.</p>
<blockquote class="doxtable">
<p><b>IMPORTANT</b>: This section requires that you have <a href="#run-the-image-classification-verification-script">Run the Image Classification Verification Script</a>. This script builds the Image Classification sample application and downloads the required Caffe* Squeezenet model. </p>
</blockquote>
<p>Setting up a neural network is the first step in running the sample.</p>
<h3><a class="anchor" id="set-up-a-neural-network-model"></a>Set Up a Neural Network Model</h3>
<p>If you are running inference on hardware other than VPU-based devices, you already have the required FP32 neural network model converted to an optimized Intermediate Representation (IR). Follow the steps in the <a href="#run-the-sample-application">Run the Sample Application</a> section to run the sample.</p>
<p>If you want to run inference on a VPU device (Intel® Movidius™ Neural Compute Stick, Intel® Neural Compute Stick 2 or Intel® Vision Accelerator Design with Intel® Movidius™ VPUs), you'll need an FP16 version of the model, which you will set up in this paragraph.</p>
<p>To convert the FP32 model to a FP16 IR suitable for VPU-based hardware accelerators, follow the steps below:</p>
<ol type="1">
<li>Create a directory for the FP16 SqueezeNet Model, for example, <code>C:\Users\&lt;username&gt;\Documents\squeezenet1.1_FP16</code></li>
<li>Open the Command Prompt and run the Model Optimizer to convert the FP32 Squeezenet Caffe* model delivered with the installation into an optimized FP16 Intermediate Representation (IR): <div class="fragment"><div class="line">python3 &quot;C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer\mo.py&quot; --input_model &quot;C:\Users&lt;username&gt;\Documents\Intel\OpenVINO\openvino_models\models\FP32\classification\squeezenet\1.1\caffe\squeezenet1.1.caffemodel&quot; --data_type FP16 --output_dir &quot;C:\Users&lt;username&gt;\Documents\squeezenet1.1_FP16&quot;</div></div><!-- fragment --></li>
<li>The <code>squeezenet1.1.labels</code> file contains the classes that ImageNet uses. This file is included so that the inference results show text instead of classification numbers. Copy <code>squeezenet1.1.labels</code> to your optimized model location: <div class="fragment"><div class="line">copy &quot;C:\Users&lt;username&gt;\Documents\Intel\OpenVINO\openvino_models\ir\FP32\classification\squeezenet\1.1\caffe\squeezenet1.1.labels&quot; &quot;C:\Users&lt;username&gt;\Documents\squeezenet1.1_FP16&quot;</div></div><!-- fragment --></li>
</ol>
<p>Now your neural network setup is complete and you're ready to run the sample application.</p>
<h3><a class="anchor" id="run-the-sample-application"></a>Run the Sample Application</h3>
<p>In this paragraph you will run the Image Classification sample application, which was automatically built when you <a href="#run-the-image-classification-verification-script">Ran the Image Classification Verification Script</a>. To run the sample application:</p>
<ol type="1">
<li>Go to the samples build directory: <div class="fragment"><div class="line">cd C:\Users&lt;username&gt;\Documents\Intel\OpenVINO\inference_engine_samples_build\intel64\Release</div></div><!-- fragment --></li>
<li>Run the sample executable with specifying the <code>car.png</code> file from the <code>demo</code> directory as an input image, the IR of your FP16 model and a plugin for a hardware device to perform inference on. <blockquote class="doxtable">
<p><b>NOTE</b>: Running the sample application on hardware other than CPU requires performing <a href="#optional-steps">additional hardware configuration steps</a>. </p>
</blockquote>
<ul>
<li>For CPU: <div class="fragment"><div class="line">classification_sample_async.exe -i &quot;C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\car.png&quot; -m &quot;C:\Users&lt;username&gt;\Documents\squeezenet1.1_FP16\squeezenet1.1.xml&quot; -d CPU</div></div><!-- fragment --></li>
<li>For GPU: <div class="fragment"><div class="line">classification_sample_async.exe -i &quot;C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\car.png&quot; -m &quot;C:\Users&lt;username&gt;\Documents\squeezenet1.1_FP16\squeezenet1.1.xml&quot; -d GPU</div></div><!-- fragment --></li>
<li>For VPU (Intel® Movidius™ Neural Compute Stick and Intel® Neural Compute Stick 2): <div class="fragment"><div class="line">classification_sample_async.exe -i &quot;C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\car.png&quot; -m &quot;C:\Users&lt;username&gt;\Documents\squeezenet1.1_FP16\squeezenet1.1.xml&quot; -d MYRIAD</div></div><!-- fragment --></li>
<li>For VPU (Intel® Vision Accelerator Design with Intel® Movidius™ VPUs): <div class="fragment"><div class="line">classification_sample_async.exe -i &quot;C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\demo\car.png&quot; -m &quot;C:\Users&lt;username&gt;\Documents\squeezenet1.1_FP16\squeezenet1.1.xml&quot; -d HDDL</div></div><!-- fragment --></li>
</ul>
</li>
</ol>
<p>For information on Sample Applications, see the <a href="https://docs.openvinotoolkit.org/latest/_docs_IE_DG_Samples_Overview.html">Inference Engine Samples Overview</a>.</p>
<p>Congratulations, you have finished the installation of the Intel® Distribution of OpenVINO™ toolkit for Windows*. To learn more about how the Intel® Distribution of OpenVINO™ toolkit works, the Hello World tutorial and other resources are provided below.</p>
<h2><a class="anchor" id="Summary"></a>Summary</h2>
<p>In this document, you installed the Intel® Distribution of OpenVINO™ toolkit and its dependencies. You also configured the Model Optimizer for one or more frameworks. After the software was installed and configured, you ran two verification scripts. You might have also installed drivers that will let you use a GPU or VPU to infer your models and run the Image Classification Sample application.</p>
<p>You are now ready to learn more about converting models trained with popular deep learning frameworks to the Inference Engine format, following the links below, or you can move on to running the <a class="el" href="_docs_IE_DG_Samples_Overview.html">sample applications</a>.</p>
<p>To learn more about converting deep learning models, go to:</p>
<ul>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html">Convert Your Caffe* Model</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html">Convert Your TensorFlow* Model</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_MxNet.html">Convert Your MXNet* Model</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html">Convert Your ONNX* Model</a></li>
</ul>
<h2>Additional Resources</h2>
<ul>
<li><a href="https://software.intel.com/en-us/openvino-toolkit">Intel Distribution of OpenVINO Toolkit home page</a></li>
<li><a href="https://software.intel.com/en-us/openvino-toolkit/documentation/featured">Intel Distribution of OpenVINO Toolkit documentation</a></li>
<li><a href="https://software.intel.com/en-us/articles/OpenVINO-RelNotes">OpenVINO™ Release Notes</a></li>
<li><a class="el" href="_docs_IE_DG_Introduction.html">Introduction to Intel® Deep Learning Deployment Toolkit</a></li>
<li><a class="el" href="_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html">Inference Engine Developer Guide</a></li>
<li><a class="el" href="_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html">Model Optimizer Developer Guide</a></li>
<li><a class="el" href="_docs_IE_DG_Samples_Overview.html">Inference Engine Samples Overview</a></li>
<li><a class="el" href="_models_intel_index.html">Overview of OpenVINO™ Toolkit Pre-Trained Models</a></li>
<li>Intel Distribution of OpenVINO Toolkit Hello World Activities, see the <a href="https://github.com/intel-iot-devkit/inference-tutorials-generic/tree/openvino_toolkit_r3_0">Inference Tutorials for Face Detection and Car Detection Exercises</a></li>
<li><a href="https://software.intel.com/en-us/neural-compute-stick/get-started">Intel® Neural Compute Stick 2 Get Started</a> </li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>