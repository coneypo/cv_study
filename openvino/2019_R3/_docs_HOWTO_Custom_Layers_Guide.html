<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Custom Layers Guide - OpenVINO Toolkit</title>
<script type="text/javascript" src="jquery.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="dynsections.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../assets/versions_raw.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="../assets/openvino-versions.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
<script type="text/javascript" src="openvino-layout.js?v=061b82a77ebb139b894ce81faa5ba1f6"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname"><a href="<domain_placeholder>" class="homelink-id">OpenVINO Toolkit</a></div>
  </div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js?v=e23bd1c8e92f867d90ca8af9d83669a3"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Custom Layers Guide </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The Intel® Distribution of OpenVINO™ toolkit supports neural network model layers in multiple frameworks including TensorFlow*, Caffe*, MXNet*, Kaldi* and ONYX*. The list of known layers is different for each of the supported frameworks. To see the layers supported by your framework, refer to <a class="el" href="_docs_MO_DG_prepare_model_Supported_Frameworks_Layers.html">supported frameworks</a>.</p>
<p>Custom layers are layers that are not included in the list of known layers. If your topology contains any layers that are not in the list of known layers, the Model Optimizer classifies them as custom.</p>
<p>This guide illustrates the workflow for running inference on topologies featuring custom layers, allowing you to plug in your own implementation for existing or completely new layers. For a step-by-step example of creating and executing a custom layer, see the <a href="https://github.com/david-drew/OpenVINO-Custom-Layers/tree/master/2019.r2.0">Custom Layer Implementation Tutorials for Linux and Windows.</a></p>
<h2>Terms used in this guide</h2>
<ul>
<li><em>Layer</em> — The abstract concept of a math function that is selected for a specific purpose (relu, sigmoid, tanh, convolutional). This is one of a sequential series of building blocks within the neural network.</li>
<li><em>Kernel</em> — The implementation of a layer function, in this case, the math programmed (in C++ and Python) to perform the layer operation for target hardware (CPU or GPU).</li>
<li><em>Intermediate Representation (IR)</em> — Neural Network used only by the Inference Engine in OpenVINO abstracting the different frameworks and describing topology, layer parameters and weights. The original format will be a supported framework such as TensorFlow, Caffe, or MXNet.</li>
<li><em>Model Extension Generator</em> — Generates template source code files for each of the extensions needed by the Model Optimizer and the Inference Engine.</li>
<li><em>Inference Engine Extension</em> — Device-specific module implementing custom layers (a set of kernels).</li>
</ul>
<h2>Custom Layer Overview</h2>
<p>The <a href="https://docs.openvinotoolkit.org/2019_R1.1/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html">Model Optimizer</a> searches the list of known layers for each layer contained in the input model topology before building the model's internal representation, optimizing the model, and producing the Intermediate Representation files.</p>
<p>The <a href="https://docs.openvinotoolkit.org/2019_R1.1/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html">Inference Engine</a> loads the layers from the input model IR files into the specified device plugin, which will search a list of known layer implementations for the device. If your topology contains layers that are not in the list of known layers for the device, the Inference Engine considers the layer to be unsupported and reports an error. To see the layers that are supported by each device plugin for the Inference Engine, refer to the <a href="https://docs.openvinotoolkit.org/2019_R1.1/_docs_IE_DG_supported_plugins_Supported_Devices.html">Supported Devices</a> documentation. <br />
 <b>Note:</b> If a device doesn't support a particular layer, an alternative to creating a new custom layer is to target an additional device using the HETERO plugin. The <a href="https://docs.openvinotoolkit.org/2019_R1.1/_docs_IE_DG_supported_plugins_HETERO.html">Heterogeneous Plugin</a> may be used to run an inference model on multiple devices allowing the unsupported layers on one device to "fallback" to run on another device (e.g., CPU) that does support those layers.</p>
<h2>Custom Layer Implementation Workflow</h2>
<p>When implementing a custom layer for your pre-trained model in the Intel® Distribution of OpenVINO™ toolkit, you will need to add extensions to both the Model Optimizer and the Inference Engine.</p>
<h2>Custom Layer Extensions for the Model Optimizer</h2>
<p>The following figure shows the basic processing steps for the Model Optimizer highlighting the two necessary custom layer extensions, the Custom Layer Extractor and the Custom Layer Operation.</p>
<div class="image">
<img src="MO_extensions_flow.png" alt="MO_extensions_flow.png"/>
</div>
<p>The Model Optimizer first extracts information from the input model which includes the topology of the model layers along with parameters, input and output format, etc., for each layer. The model is then optimized from the various known characteristics of the layers, interconnects, and data flow which partly comes from the layer operation providing details including the shape of the output for each layer. Finally, the optimized model is output to the model IR files needed by the Inference Engine to run the model.</p>
<p>The Model Optimizer starts with a library of known extractors and operations for each <a href="https://docs.openvinotoolkit.org/2019_R1.1/_docs_MO_DG_prepare_model_Supported_Frameworks_Layers.html">supported model framework</a> which must be extended to use each unknown custom layer. The custom layer extensions needed by the Model Optimizer are:</p>
<ul>
<li>Custom Layer Extractor<ul>
<li>Responsible for identifying the custom layer operation and extracting the parameters for each instance of the custom layer. The layer parameters are stored per instance and used by the layer operation before finally appearing in the output IR. Typically the input layer parameters are unchanged, which is the case covered by this tutorial.</li>
</ul>
</li>
<li>Custom Layer Operation<ul>
<li>Responsible for specifying the attributes that are supported by the custom layer and computing the output shape for each instance of the custom layer from its parameters. <br />
 The <code>--mo-op</code> command-line argument shown in the examples below generates a custom layer operation for the Model Optimizer.</li>
</ul>
</li>
</ul>
<h2>Custom Layer Extensions for the Inference Engine</h2>
<p>The following figure shows the basic flow for the Inference Engine highlighting two custom layer extensions for the CPU and GPU Plugins, the Custom Layer CPU extension and the Custom Layer GPU Extension.</p>
<div class="image">
<img src="IE_extensions_flow.png" alt="IE_extensions_flow.png"/>
</div>
<p>Each device plugin includes a library of optimized implementations to execute known layer operations which must be extended to execute a custom layer. The custom layer extension is implemented according to the target device:</p>
<ul>
<li>Custom Layer CPU Extension<ul>
<li>A compiled shared library (.so or .dll binary) needed by the CPU Plugin for executing the custom layer on the CPU.</li>
</ul>
</li>
<li>Custom Layer GPU Extension<ul>
<li>OpenCL source code (.cl) for the custom layer kernel that will be compiled to execute on the GPU along with a layer description file (.xml) needed by the GPU Plugin for the custom layer kernel.</li>
</ul>
</li>
</ul>
<h2>Model Extension Generator</h2>
<p>Using answers to interactive questions or a *.json* configuration file, the Model Extension Generator tool generates template source code files for each of the extensions needed by the Model Optimizer and the Inference Engine. To complete the implementation of each extension, the template functions may need to be edited to fill-in details specific to the custom layer or the actual custom layer functionality itself.</p>
<h3>Command-line</h3>
<p>The Model Extension Generator is included in the Intel® Distribution of OpenVINO™ toolkit installation and is run using the command (here with the "--help" option):</p>
<div class="fragment"><div class="line">python3 /opt/intel/openvino/deployment_tools/tools/extension_generator/extgen.py new --help</div></div><!-- fragment --><p>where the output will appear similar to:</p>
<div class="fragment"><div class="line">usage: You can use any combination of the following arguments:</div><div class="line"></div><div class="line">Arguments to configure extension generation in the interactive mode:</div><div class="line"></div><div class="line">optional arguments:</div><div class="line">  -h, --help            show this help message and exit</div><div class="line">  --mo-caffe-ext        generate a Model Optimizer Caffe* extractor</div><div class="line">  --mo-mxnet-ext        generate a Model Optimizer MXNet* extractor</div><div class="line">  --mo-tf-ext           generate a Model Optimizer TensorFlow* extractor</div><div class="line">  --mo-op               generate a Model Optimizer operation</div><div class="line">  --ie-cpu-ext          generate an Inference Engine CPU extension</div><div class="line">  --ie-gpu-ext          generate an Inference Engine GPU extension</div><div class="line">  --output_dir OUTPUT_DIR</div><div class="line">                        set an output directory. If not specified, the current</div><div class="line">                        directory is used by default.</div></div><!-- fragment --><p>The available command-line arguments are used to specify which extension(s) to generate templates for the Model Optimizer or Inference Engine. The generated extension files for each argument will appear starting from the top of the output directory as follows:</p>
<table class="doxtable">
<tr>
<th>Command-line Argument </th><th>Output Directory Location  </th></tr>
<tr>
<td><code>--mo-caffe-ext</code> </td><td>user_mo_extensions/front/caffe </td></tr>
<tr>
<td><code>--mo-mxnet-ext</code> </td><td>user_mo_extensions/front/mxnet </td></tr>
<tr>
<td><code>--mo-tf-ext</code> </td><td>user_mo_extensions/front/tf </td></tr>
<tr>
<td><code>--mo-op</code> </td><td>user_mo_extensions/ops </td></tr>
<tr>
<td><code>--ie-cpu-ext</code> </td><td>user_ie_extensions/cpu </td></tr>
<tr>
<td><code>--ie-gpu-ext</code> </td><td>user_ie_extensions/gpu </td></tr>
</table>
<h3>Extension Workflow</h3>
<p>The workflow for each generated extension follows the same basic steps:</p>
<div class="image">
<img src="MEG_generic_flow.png" alt="MEG_generic_flow.png"/>
</div>
<p><b>Step 1: Generate:</b> Use the Model Extension Generator to generate the Custom Layer Template Files.</p>
<p><b>Step 2: Edit:</b> Edit the Custom Layer Template Files as necessary to create the specialized Custom Layer Extension Source Code.</p>
<p><b>Step 3: Specify:</b> Specify the custom layer extension locations to be used by the Model Optimizer or Inference Engine.</p>
<h2>Caffe* Models with Custom Layers <a class="anchor" id="caffe-models-with-custom-layers"></a></h2>
<p>If your Caffe* model has custom layers:</p>
<p><b>Register the custom layers as extensions to the Model Optimizer</b>. For instructions, see <a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Extending_Model_Optimizer_with_New_Primitives.html">Extending Model Optimizer with New Primitives</a>. When your custom layers are registered as extensions, the Model Optimizer generates a valid and optimized Intermediate Representation. You will need a bit of Python* code that lets the Model Optimizer;</p>
<ul>
<li>Generate a valid Intermediate Representation according to the rules you specified.</li>
<li>Be independent from the availability of Caffe on your computer.</li>
</ul>
<p>If your model contains Custom Layers, it is important to understand the internal workflow of the Model Optimizer. Consider the following example.</p>
<p><b>Example</b>:</p>
<p>The network has:</p>
<ul>
<li>One input layer (#1)</li>
<li>One output Layer (#5)</li>
<li>Three internal layers (#2, 3, 4)</li>
</ul>
<p>The custom and standard layer types are:</p>
<ul>
<li>Layers #2 and #5 are implemented as Model Optimizer extensions.</li>
<li>Layers #1 and #4 are supported in Model Optimizer out-of-the box.</li>
<li>Layer #3 is neither in the list of supported layers nor in extensions, but is specified in CustomLayersMapping.xml.</li>
</ul>
<blockquote class="doxtable">
<p><b>NOTE</b>: If any of the layers are not in one of three categories described above, the Model Optimizer fails with an appropriate message and a link to the corresponding question in <a class="el" href="_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html">Model Optimizer FAQ</a>. </p>
</blockquote>
<p>The general process is as shown:</p>
<div class="image">
<img src="mo_caffe_priorities.png" alt="mo_caffe_priorities.png"/>
<div class="caption">
Example custom layer network</div></div>
<p><br />
</p>
<p><b>Step 1:</b> The example model is fed to the Model Optimizer that <b>loads the model</b> with the special parser built on top of the <code>caffe.proto</code> file. In case of failure, the Model Optimizer asks you to prepare the parser that can read the model. For more information, refer to the Model Optimizer, <a href="MO_FAQ.html#FAQ1">FAQ #1</a>.</p>
<p><b>Step 2:</b> The Model Optimizer <b>extracts the attributes of all layers</b> by going through the list of layers and attempting to find the appropriate extractor. In order of priority, the Model Optimizer checks if the layer is:</p>
<ul>
<li>A. Registered as a Model Optimizer extension</li>
<li>B. Registered as a standard Model Optimizer layer</li>
</ul>
<p>When the Model Optimizer finds a satisfying condition from the list above, it extracts the attributes according to the following rules:</p>
<ul>
<li>For A. - takes only the parameters specified in the extension</li>
<li>For B. - takes only the parameters specified in the standard extractor <br />
</li>
</ul>
<p><b>Step 3:</b> The Model Optimizer <b>calculates the output shape of all layers</b>. The logic is the same as it is for the priorities. <b>Important:</b> the Model Optimizer always takes the first available option.</p>
<p><b>Step 4:</b> The Model Optimizer <b>optimizes the original model and produces the two Intermediate Representation (IR) files in .xml and .bin</b>. <br />
</p>
<h2>TensorFlow* Models with Custom Layers <a class="anchor" id="Tensorflow-models-with-custom-layers"></a></h2>
<p>You have three options for TensorFlow* models with custom layers: <br />
</p>
<ul>
<li><b>Register those layers as extensions to the Model Optimizer.</b> In this case, the Model Optimizer generates a valid and optimized Intermediate Representation.</li>
<li><b>If you have sub-graphs that should not be expressed with the analogous sub-graph in the Intermediate Representation, but another sub-graph should appear in the model, the Model Optimizer provides such an option.</b> This feature is helpful for many TensorFlow models. To read more, see <a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Subgraph_Replacement_Model_Optimizer.html">Sub-graph Replacement in the Model Optimizer</a>.</li>
<li><p class="startli"><b>Experimental feature of registering definite sub-graphs of the model as those that should be offloaded to TensorFlow during inference.</b> In this case, the Model Optimizer produces an Intermediate Representation that:</p><ul>
<li>Can be inferred only on CPU</li>
<li>Reflects each sub-graph as a single custom layer in the Intermediate Representation</li>
</ul>
<p class="startli">For more information, see <a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Offloading_Sub_Graph_Inference.html">Offloading Computations to TensorFlow*</a>. This feature is for development only. It is expected to be used, when you have the model that has complex structure and it is not an easy task to write extensions for internal subgraphs. In this case, you offload these complex subgraphs to TensorFlow to make sure that Model Optimizer and Inference Engine can successfully execute your model, however, for each such subgraph, TensorFlow library is called that is not optimized for inference. Then, you start replacing each subgraph with extension and remove its offloading to TensorFlow during inference until all the models are converted by Model Optimizer and inferred by Inference Engine only with the maximum performance.</p>
</li>
</ul>
<h2>MXNet* Models with Custom Layers <a class="anchor" id="mxnet-models-with-custom-layers"></a></h2>
<p>There are two options to convert your MXNet* model that contains custom layers:</p>
<ol type="1">
<li>Register the custom layers as extensions to the Model Optimizer. For instructions, see <a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Extending_MXNet_Model_Optimizer_with_New_Primitives.html">Extending MXNet Model Optimizer with New Primitives</a>. When your custom layers are registered as extensions, the Model Optimizer generates a valid and optimized Intermediate Representation. You can create Model Optimizer extensions for both MXNet layers with op <code>Custom</code> and layers which are not standard MXNet layers.</li>
<li>If you have sub-graphs that should not be expressed with the analogous sub-graph in the Intermediate Representation, but another sub-graph should appear in the model, the Model Optimizer provides such an option. In MXNet the function is actively used for ssd models provides an opportunity to for the necessary subgraph sequences and replace them. To read more, see <a class="el" href="_docs_MO_DG_prepare_model_customize_model_optimizer_Subgraph_Replacement_Model_Optimizer.html">Sub-graph Replacement in the Model Optimizer</a>.</li>
</ol>
<h2>Kaldi* Models with Custom Layers <a class="anchor" id="Kaldi-models-with-custom-layers"></a></h2>
<p>For information on converting your Kaldi* model containing custom layers see <a href="https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Kaldi.html">Converting a Kaldi Model in the Model Optimizer Developer Guide</a>.</p>
<h2>ONNX* Models with Custom Layers <a class="anchor" id="ONNX-models-with-custom-layers"></a></h2>
<p>For information on converting your ONNX* model containing custom layers see <a href="https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html">Converting an ONNX Model in the Model Optimizer Developer Guide</a>.</p>
<h2>Step-by-Step Custom Layers Tutorial</h2>
<p>For a step-by-step walk-through creating and executing a custom layer, see <a href="https://github.com/david-drew/OpenVINO-Custom-Layers/tree/master/2019.r2.0">Custom Layer Implementation Tutorial for Linux and Windows.</a></p>
<h2>Additional Resources</h2>
<ul>
<li>Intel® Distribution of OpenVINO™ toolkit home page: <a href="https://software.intel.com/en-us/openvino-toolkit">https://software.intel.com/en-us/openvino-toolkit</a></li>
<li>OpenVINO™ toolkit online documentation: <a href="https://docs.openvinotoolkit.org">https://docs.openvinotoolkit.org</a></li>
<li><a href="https://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html">Model Optimizer Developer Guide</a></li>
<li><a href="https://docs.openvinotoolkit.org/latest/_docs_IE_DG_Integrate_your_kernels_into_IE.html">Kernel Extensivility in the Inference Engine Developer Guide</a></li>
<li><a href="https://docs.openvinotoolkit.org/latest/_docs_IE_DG_Samples_Overview.html">Inference Engine Samples Overview</a></li>
<li><a href="https://docs.openvinotoolkit.org/latest/_intel_models_index.html">Overview of OpenVINO™ Toolkit Pre-Trained Models</a></li>
<li><a href="https://github.com/intel-iot-devkit/inference-tutorials-generic">Inference Engine Tutorials</a></li>
<li>For IoT Libraries and Code Samples see the <a href="https://github.com/intel-iot-devkit">Intel® IoT Developer Kit</a>.</li>
</ul>
<h2>Converting Models:</h2>
<ul>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html">Convert Your Caffe* Model</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html">Convert Your TensorFlow* Model</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_MxNet.html">Convert Your MXNet* Model</a></li>
<li><a class="el" href="_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html">Convert Your ONNX* Model</a> </li>
</ul>
</div></div><!-- contents -->
  <div class="footer">
  <div id="nav-path" class="navpath"></div>
  <div class="footer-content">
    <div class="footer-column">
      <h4>Support</h4>
      <ul>
        <li>
          <a href="https://software.intel.com/en-us/forums/computer-vision">Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit</a></li>
      </ul>
    </div>
  </div>
  <div class="copyright">
    <p>Documentation for OpenVINO Toolkit.<br/>
    For more complete information about compiler optimizations, see our <a href="/<version_placeholder>/_docs_IE_DG_Optimization_notice.html">Optimization Notice</a></p>
  </div>
</div>
</body>
</html>